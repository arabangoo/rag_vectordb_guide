# LangChainì„ í™œìš©í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ

> í•œêµ­ì–´ ê°œë°œìë¥¼ ìœ„í•œ LangChain ê¸°ë°˜ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ ì™„ë²½ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ì†Œê°œ](#-í”„ë¡œì íŠ¸-ì†Œê°œ)
- [ì™œ LangChainì¸ê°€?](#-ì™œ-langchainì¸ê°€)
- [ì£¼ìš” ì‚¬ì–‘](#-ì£¼ìš”-ì‚¬ì–‘-specs)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [í™˜ê²½ êµ¬ì¶•](#-í™˜ê²½-êµ¬ì¶•)
- [ë¹ ë¥¸ ì‹œì‘](#-ë¹ ë¥¸-ì‹œì‘)
- [ìƒì„¸ ê°€ì´ë“œ](#-ìƒì„¸-ê°€ì´ë“œ)
- [ë²¡í„° ìŠ¤í† ì–´ í†µí•©](#-ë²¡í„°-ìŠ¤í† ì–´-í†µí•©)
- [ê³ ê¸‰ RAG íŒ¨í„´](#-ê³ ê¸‰-rag-íŒ¨í„´)
- [ì„±ëŠ¥ ìµœì í™”](#-ì„±ëŠ¥-ìµœì í™”)
- [í”„ë¡œë•ì…˜ ë°°í¬](#-í”„ë¡œë•ì…˜-ë°°í¬)
- [ì‹¤ë¬´ ê°€ì´ë“œ](#-ì‹¤ë¬´-ê°€ì´ë“œ)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
- [FAQ](#-faq)
- [ì°¸ê³  ìë£Œ](#-ì°¸ê³ -ìë£Œ)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ì†Œê°œ

ì´ í”„ë¡œì íŠ¸ëŠ” **LangChain í”„ë ˆì„ì›Œí¬**ë¥¼ í™œìš©í•˜ì—¬ í”„ë¡œë•ì…˜ê¸‰ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.

### LangChainì´ë€?

**LangChain**ì€ LLM(Large Language Model) ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

- **ê°œë°œì‚¬**: LangChain Inc. (Harrison Chase, CEO)
- **ì¶œì‹œ**: 2022ë…„ 10ì›”
- **GitHub Stars**: 100,000+ (2024ë…„ 12ì›” ê¸°ì¤€)
- **ì§€ì› ì–¸ì–´**: Python, JavaScript/TypeScript
- **ë¼ì´ì„ ìŠ¤**: MIT License

### ì£¼ìš” íŠ¹ì§•

- âœ… **í†µí•© í”„ë ˆì„ì›Œí¬**: 100+ LLM, ë²¡í„°DB, ë¬¸ì„œë¡œë”ë¥¼ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©
- âœ… **ì²´ì¸ êµ¬ì„±**: ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬í˜„
- âœ… **ëª¨ë“ˆí™”**: Document Loaders, Text Splitters, Embeddings, Vector Stores, Retrievers, Chains ë“±
- âœ… **LCEL (LangChain Expression Language)**: ì„ ì–¸ì  ì²´ì¸ êµ¬ì„± ë° ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
- âœ… **LangSmith**: ë””ë²„ê¹…, í‰ê°€, ëª¨ë‹ˆí„°ë§ ë„êµ¬ ë‚´ì¥
- âœ… **LangGraph**: ë³µì¡í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬í˜„
- âœ… **ìƒíƒœê³„**: í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°ì™€ í’ë¶€í•œ ë¬¸ì„œí™”

### í•™ìŠµ ëª©í‘œ

ì´ ê°€ì´ë“œë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. LangChain í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´í•´ ë° í™œìš©
2. ë‹¤ì–‘í•œ ë²¡í„° ìŠ¤í† ì–´(Pinecone, Chroma, FAISS, MongoDB, Supabase)ì™€ RAG í†µí•©
3. Document Loadersë¡œ ë‹¤ì–‘í•œ í˜•ì‹(PDF, DOCX, CSV, HTML ë“±) ë¬¸ì„œ ì²˜ë¦¬
4. Text Splittersë¡œ ìµœì í™”ëœ ì²­í‚¹ ì „ëµ êµ¬í˜„
5. Retrieval Chains, Conversational Chains, RetrievalQA êµ¬í˜„
6. Self-Query, Parent-Document, Multi-Query ë“± ê³ ê¸‰ RAG íŒ¨í„´ êµ¬í˜„
7. LangSmithë¡œ RAG íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹… ë° ìµœì í™”
8. í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

### ì‹¤ì œ í™œìš© ì‚¬ë¡€

LangChain ê¸°ë°˜ RAGëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤:

- **ğŸ“š ê¸°ì—… ì§€ì‹ ê´€ë¦¬**: ì‚¬ë‚´ ë¬¸ì„œ, ì •ì±…, FAQë¥¼ í†µí•©í•œ AI ì±—ë´‡
- **ğŸ¢ ê³ ê° ì§€ì›**: ì œí’ˆ ë§¤ë‰´ì–¼, í‹°ì¼“ íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ìë™ ì‘ë‹µ
- **ğŸ“Š ê¸ˆìœµ ë¶„ì„**: ì¬ë¬´ ë³´ê³ ì„œ, ë‰´ìŠ¤, ê·œì • í†µí•© ë¶„ì„
- **ğŸ” ë²•ë¥  ë¦¬ì„œì¹˜**: íŒë¡€, ë²•ë ¹, ê³„ì•½ì„œ ê²€ìƒ‰ ë° ìš”ì•½
- **ğŸ“ êµìœ¡ í”Œë«í¼**: ê°•ì˜ ìë£Œ, êµì¬ ê¸°ë°˜ ê°œì¸í™” í•™ìŠµ ë„ìš°ë¯¸
- **ğŸ’» ì½”ë“œ ê²€ìƒ‰**: ì‚¬ë‚´ ì½”ë“œë² ì´ìŠ¤ ê²€ìƒ‰ ë° ë¬¸ì„œ ìë™ ìƒì„±
- **ğŸŒ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ, ì°¨íŠ¸ í†µí•© ê²€ìƒ‰

---

## ğŸ¤” ì™œ LangChainì¸ê°€?

### ê¸°ì¡´ ë°©ì‹ vs LangChain

| ê¸°ëŠ¥ | ì§ì ‘ êµ¬í˜„ (Raw API) | LangChain |
|------|---------------------|-----------|
| **ê°œë°œ ì†ë„** | ëŠë¦¼ (ëª¨ë“  ê²ƒ ì§ì ‘ êµ¬í˜„) | **ë¹ ë¦„ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)** |
| **LLM ì „í™˜** | ì½”ë“œ ì „ë©´ ìˆ˜ì • | **ì„¤ì •ë§Œ ë³€ê²½** |
| **ë²¡í„°DB ë³€ê²½** | ì¸í„°í˜ì´ìŠ¤ ì¬êµ¬í˜„ | **í´ë˜ìŠ¤ë§Œ êµì²´** |
| **ë¬¸ì„œ ë¡œë”©** | ê° í˜•ì‹ë§ˆë‹¤ íŒŒì„œ êµ¬í˜„ | **100+ ë¡œë” ê¸°ë³¸ ì œê³µ** |
| **ì²´ì¸ êµ¬ì„±** | ë³µì¡í•œ ë¡œì§ í•„ìš” | **ì„ ì–¸ì  LCEL ì‚¬ìš©** |
| **ë””ë²„ê¹…** | print ë¬¸ ë˜ëŠ” ë¡œê¹… | **LangSmith ì‹œê°í™”** |
| **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬** | ì½”ë“œì— í•˜ë“œì½”ë”© | **í…œí”Œë¦¿ ë° Hub í™œìš©** |
| **ìŠ¤íŠ¸ë¦¬ë°** | ì§ì ‘ êµ¬í˜„ í•„ìš” | **ê¸°ë³¸ ì§€ì›** |
| **ë©”ëª¨ë¦¬ ê´€ë¦¬** | ìˆ˜ë™ êµ¬í˜„ | **ConversationBufferMemory ë“±** |

### RAGì— ìµœì ì¸ ì´ìœ 

1. **ì¶”ìƒí™”ì™€ ìœ ì—°ì„±**: ë²¡í„°DB, LLM ì œê³µì‚¬ë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥
2. **í’ë¶€í•œ í†µí•©**: 100+ ë¬¸ì„œ ë¡œë”, 50+ ë²¡í„° ìŠ¤í† ì–´ ê¸°ë³¸ ì§€ì›
3. **ê³ ê¸‰ RAG íŒ¨í„´**: Self-Query, Parent-Document, Multi-Query ë“± ë‚´ì¥
4. **í”„ë¡œë•ì…˜ ì§€ì›**: ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ í•¸ë“¤ë§, ì¬ì‹œë„ ë¡œì§ ê¸°ë³¸ ì œê³µ
5. **ëª¨ë‹ˆí„°ë§**: LangSmithë¡œ ì²´ì¸ ì‹¤í–‰ ì¶”ì  ë° ì„±ëŠ¥ ë¶„ì„
6. **ì»¤ë®¤ë‹ˆí‹°**: í™œë°œí•œ ìƒíƒœê³„ì™€ ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸
7. **í‘œì¤€í™”**: RAG êµ¬í˜„ì˜ ì‚¬ì‹¤ìƒ í‘œì¤€ í”„ë ˆì„ì›Œí¬

---

## ğŸ“‹ ì£¼ìš” ì‚¬ì–‘ (Specs)

### ì§€ì›í•˜ëŠ” LLM

| ì œê³µì‚¬ | ëª¨ë¸ ì˜ˆì‹œ | LangChain í´ë˜ìŠ¤ |
|--------|----------|------------------|
| **OpenAI** | GPT-4o, GPT-4o-mini, GPT-3.5-turbo | `ChatOpenAI` |
| **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus/Haiku | `ChatAnthropic` |
| **Google** | Gemini 2.5 Flash, Gemini 2.5 Pro | `ChatGoogleGenerativeAI` |
| **AWS Bedrock** | Claude, Llama 3.1, Titan | `BedrockChat` |
| **Azure OpenAI** | GPT-4o (Azure) | `AzureChatOpenAI` |
| **Ollama** | Llama 3.1, Mistral, Qwen | `ChatOllama` |
| **Cohere** | Command R+, Command Light | `ChatCohere` |
| **Hugging Face** | ëª¨ë“  text-generation ëª¨ë¸ | `HuggingFaceHub` |

### ì§€ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸

| ì œê³µì‚¬ | ëª¨ë¸ | ì°¨ì› | LangChain í´ë˜ìŠ¤ |
|--------|------|------|------------------|
| **OpenAI** | text-embedding-3-small | 1536 | `OpenAIEmbeddings` |
| **OpenAI** | text-embedding-3-large | 3072 | `OpenAIEmbeddings` |
| **Cohere** | embed-multilingual-v3 | 1024 | `CohereEmbeddings` |
| **Google** | text-embedding-004 | 768 | `GoogleGenerativeAIEmbeddings` |
| **AWS Bedrock** | amazon.titan-embed-text-v1 | 1024 | `BedrockEmbeddings` |
| **Hugging Face** | BAAI/bge-large-en-v1.5 | 1024 | `HuggingFaceEmbeddings` |
| **Ollama** | nomic-embed-text | 768 | `OllamaEmbeddings` |
| **Sentence Transformers** | all-MiniLM-L6-v2 | 384 | `HuggingFaceEmbeddings` |

### ì§€ì›í•˜ëŠ” ë²¡í„° ìŠ¤í† ì–´

| ë²¡í„° ìŠ¤í† ì–´ | íƒ€ì… | íŠ¹ì§• | LangChain í´ë˜ìŠ¤ |
|------------|------|------|------------------|
| **FAISS** | ë¡œì»¬ | ë¹ ë¥¸ ê²€ìƒ‰, ë©”ëª¨ë¦¬ ê¸°ë°˜ | `FAISS` |
| **Chroma** | ë¡œì»¬/í´ë¼ìš°ë“œ | ê²½ëŸ‰, ì„ë² ë”© ê´€ë¦¬ | `Chroma` |
| **Pinecone** | í´ë¼ìš°ë“œ | ì™„ì „ ê´€ë¦¬í˜•, ê³ ì„±ëŠ¥ | `Pinecone` |
| **Weaviate** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | GraphQL, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ | `Weaviate` |
| **MongoDB Atlas** | í´ë¼ìš°ë“œ | í†µí•© DB, ë©”íƒ€ë°ì´í„° í•„í„° | `MongoDBAtlasVectorSearch` |
| **Supabase** | í´ë¼ìš°ë“œ | PostgreSQL + pgvector | `SupabaseVectorStore` |
| **Qdrant** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | Rust ê¸°ë°˜, ê³ ì„±ëŠ¥ | `Qdrant` |
| **Milvus** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | ì—”í„°í”„ë¼ì´ì¦ˆê¸‰, í™•ì¥ì„± | `Milvus` |
| **Elasticsearch** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ë¶„ì„ | `ElasticsearchStore` |
| **Redis** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | ì¸ë©”ëª¨ë¦¬, ì´ˆê³ ì† | `Redis` |

### ì§€ì›í•˜ëŠ” ë¬¸ì„œ ë¡œë”

| ë¬¸ì„œ íƒ€ì… | LangChain Loader | ìš©ë„ |
|----------|------------------|------|
| **PDF** | `PyPDFLoader`, `PDFPlumberLoader`, `UnstructuredPDFLoader` | PDF ë¬¸ì„œ ë¡œë”© |
| **Word** | `Docx2txtLoader`, `UnstructuredWordDocumentLoader` | DOCX, DOC íŒŒì¼ |
| **CSV** | `CSVLoader` | ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„° |
| **HTML** | `UnstructuredHTMLLoader`, `BSHTMLLoader` | ì›¹ í˜ì´ì§€ |
| **Markdown** | `UnstructuredMarkdownLoader` | ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ |
| **JSON** | `JSONLoader` | JSON ë°ì´í„° |
| **í…ìŠ¤íŠ¸** | `TextLoader` | ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ |
| **Excel** | `UnstructuredExcelLoader` | XLSX, XLS íŒŒì¼ |
| **PowerPoint** | `UnstructuredPowerPointLoader` | PPT, PPTX íŒŒì¼ |
| **ì´ë©”ì¼** | `UnstructuredEmailLoader` | EML, MSG íŒŒì¼ |
| **ì½”ë“œ** | `GenericLoader` + `LanguageParser` | Python, JS, Java ë“± |
| **ì›¹** | `WebBaseLoader`, `SeleniumURLLoader` | URL í¬ë¡¤ë§ |
| **YouTube** | `YoutubeLoader` | YouTube ìë§‰ |
| **Notion** | `NotionDirectoryLoader` | Notion í˜ì´ì§€ |
| **Google Drive** | `GoogleDriveLoader` | Google Docs, Sheets |
| **S3** | `S3FileLoader`, `S3DirectoryLoader` | AWS S3 íŒŒì¼ |

### ì§€ì›í•˜ëŠ” Text Splitters

| Splitter | ì„¤ëª… | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|----------|------|--------------|
| **RecursiveCharacterTextSplitter** | ê³„ì¸µì  êµ¬ë¶„ìë¡œ ë¶„í•  (\n\n â†’ \n â†’ ê³µë°±) | **ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ (ê¸°ë³¸ ê¶Œì¥)** |
| **CharacterTextSplitter** | ë‹¨ì¼ êµ¬ë¶„ìë¡œ ë¶„í•  | ë‹¨ìˆœ í…ìŠ¤íŠ¸ |
| **TokenTextSplitter** | í† í° ìˆ˜ ê¸°ì¤€ ë¶„í•  | LLM í† í° ì œí•œ ì¤€ìˆ˜ |
| **MarkdownHeaderTextSplitter** | ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ì¤€ ë¶„í•  | ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ |
| **HTMLHeaderTextSplitter** | HTML íƒœê·¸ ê¸°ì¤€ ë¶„í•  | HTML ë¬¸ì„œ |
| **LatexTextSplitter** | LaTeX êµ¬ì¡° ê¸°ì¤€ ë¶„í•  | í•™ìˆ  ë…¼ë¬¸ |
| **PythonCodeTextSplitter** | Python êµ¬ë¬¸ ê¸°ì¤€ ë¶„í•  | Python ì½”ë“œ |
| **RecursiveJsonSplitter** | JSON êµ¬ì¡° ê¸°ì¤€ ë¶„í•  | JSON ë°ì´í„° |
| **SentenceTransformersTokenTextSplitter** | ì„ë² ë”© ëª¨ë¸ í† í° ê¸°ì¤€ | ì„ë² ë”© ê¸¸ì´ ì œí•œ |

### ì˜ˆìƒ ë¹„ìš© (2025ë…„ ê¸°ì¤€)

#### ì†Œê·œëª¨ í”„ë¡œì íŠ¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸)
```
âœ… LangChain í”„ë ˆì„ì›Œí¬
- LangChain Core: ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
- LangSmith: ë¬´ë£Œ í‹°ì–´ (5,000 traces/ì›”)

âœ… ë²¡í„° ìŠ¤í† ì–´
- Chroma (ë¡œì»¬): $0/ì›”
- FAISS (ë¡œì»¬): $0/ì›”
- Pinecone Starter: $0/ì›” (100,000 ë²¡í„°)

âœ… LLM API
- OpenAI GPT-4o-mini: ~$5-10/ì›” (ì›” 1,000 ì¿¼ë¦¬)
- Ollama (ë¡œì»¬): $0/ì›”

âœ… ì„ë² ë”© API
- OpenAI text-embedding-3-small: ~$0.20/ì›”
- Ollama (ë¡œì»¬): $0/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $0-10/ì›”
```

#### ì¤‘ê·œëª¨ í”„ë¡œì íŠ¸ (í”„ë¡œë•ì…˜)
```
âœ… LangChain í”„ë ˆì„ì›Œí¬
- LangSmith Pro: $39/ì›” (100,000 traces)

âœ… ë²¡í„° ìŠ¤í† ì–´
- Pinecone Standard: $70/ì›” (1M ë²¡í„°, 1 pod)
- MongoDB Atlas M10: $57/ì›”
- Supabase Pro: $25/ì›”

âœ… LLM API
- OpenAI GPT-4o: ~$100-200/ì›” (ì›” 10,000 ì¿¼ë¦¬)
- Anthropic Claude 3.5 Sonnet: ~$150/ì›”

âœ… ì„ë² ë”© API
- OpenAI embeddings: ~$2-5/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $200-400/ì›”
```

#### ëŒ€ê·œëª¨ ì—”í„°í”„ë¼ì´ì¦ˆ
```
âœ… LangChain í”„ë ˆì„ì›Œí¬
- LangSmith Enterprise: Custom pricing (1M+ traces)

âœ… ë²¡í„° ìŠ¤í† ì–´
- Pinecone Enterprise: $500-2,000/ì›”
- MongoDB Atlas M30+: $440+/ì›”
- Weaviate Enterprise: Custom

âœ… LLM API
- OpenAI GPT-4o: $1,000-5,000/ì›” (ëŒ€ëŸ‰ íŠ¸ë˜í”½)
- Azure OpenAI: Custom pricing

âœ… ì¸í”„ë¼
- AWS/GCP/Azure: $500-2,000/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $2,000-10,000+/ì›”
```

**ë¹„ìš© ìµœì í™” íŒ:**
- **ë¡œì»¬ ëª¨ë¸ í™œìš©**: Ollamaë¡œ ì„ë² ë”©/LLM ë¹„ìš© ì ˆê°
- **ìºì‹±**: ë™ì¼ ì¿¼ë¦¬ ì¬ì‚¬ìš©ìœ¼ë¡œ API í˜¸ì¶œ ê°ì†Œ
- **ë°°ì¹˜ ì²˜ë¦¬**: ë²¡í„° ì„ë² ë”© ì¼ê´„ ìƒì„±
- **ëª¨ë¸ ì„ íƒ**: GPT-4o-mini, Claude 3 Haiku ë“± ì €ë¹„ìš© ëª¨ë¸ í™œìš©
- **LangSmith ë¬´ë£Œ í‹°ì–´**: ê°œë°œ í™˜ê²½ì—ì„œ í™œìš©

---

## ğŸ— ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### LangChain RAG ì›Œí¬í”Œë¡œìš°

```mermaid
graph TD
    User[ì‚¬ìš©ì ì§ˆë¬¸] --> App[ì• í”Œë¦¬ì¼€ì´ì…˜]

    App -->|LCEL Chain| RAGChain{RAG Chain}

    RAGChain -->|1. ì„ë² ë”©| Embeddings[Embeddings<br/>OpenAI/Cohere/Ollama]
    Embeddings -->|ì¿¼ë¦¬ ë²¡í„°| RAGChain

    RAGChain -->|2. ê²€ìƒ‰| VectorStore[(Vector Store<br/>Pinecone/Chroma/FAISS)]
    VectorStore -->|ê´€ë ¨ ë¬¸ì„œ| RAGChain

    RAGChain -->|3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±| PromptTemplate[Prompt Template<br/>Context + Question]
    PromptTemplate -->|ìµœì¢… í”„ë¡¬í”„íŠ¸| RAGChain

    RAGChain -->|4. LLM í˜¸ì¶œ| LLM[LLM<br/>GPT-4o/Claude/Gemini]
    LLM -->|ë‹µë³€| RAGChain

    RAGChain -->|ê²°ê³¼| App
    App --> User

    style RAGChain fill:#4A90E2,stroke:#357ABD,stroke-width:3px,color:#fff
    style VectorStore fill:#50C878,stroke:#3A9B5C,stroke-width:2px
```

### ë¬¸ì„œ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    Docs[ì›ë³¸ ë¬¸ì„œ<br/>PDF/DOCX/HTML] -->|1. Load| Loader[Document Loaders<br/>PyPDFLoader, etc.]

    Loader -->|Document ê°ì²´| Splitter[Text Splitters<br/>RecursiveCharacterTextSplitter]

    Splitter -->|ì²­í¬ ë¶„í• | Chunks[Document Chunks<br/>1000ìì”©]

    Chunks -->|2. Embed| Embeddings[Embeddings Model<br/>OpenAI/Cohere]

    Embeddings -->|ë²¡í„° ìƒì„±| VectorStore[(3. Store<br/>Vector Store<br/>Pinecone/Chroma)]

    VectorStore -->|ì¸ë±ì‹± ì™„ë£Œ| Ready[RAG ì¤€ë¹„ ì™„ë£Œ]
```

### LangChain í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì•„í‚¤í…ì²˜

```
                    LangChain RAG ì‹œìŠ¤í…œ
                            |
        +-------------------+-------------------+
        |                   |                   |
    Models            Retrievers            Chains
        |                   |                   |
   +----+----+         +----+----+         +----+----+
   |         |         |         |         |         |
  LLMs  Embeddings  Vector    Index     LCEL   Runnable
                     Store
        |                   |                   |
        +-------------------+-------------------+
                            |
              LangChain Core Framework
                            |
        +-------------------+-------------------+
        |                   |                   |
   Document              Memory            Callbacks
   Loaders               Buffer            LangSmith
```

### ìƒì„¸ RAG ì²´ì¸ êµ¬ì¡°

```python
# LCEL (LangChain Expression Language) ì²´ì¸ êµ¬ì¡°

RAG_CHAIN = (
    # 1. ì…ë ¥: ì‚¬ìš©ì ì§ˆë¬¸
    {"context": retriever, "question": RunnablePassthrough()}

    # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
    | prompt_template

    # 3. LLM í˜¸ì¶œ
    | llm

    # 4. ì¶œë ¥ íŒŒì‹±
    | StrOutputParser()
)
```

### í”„ë¡œë•ì…˜ ì•„í‚¤í…ì²˜

```
[í´ë¼ì´ì–¸íŠ¸ ê³„ì¸µ]
ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ (React/Vue) | ëª¨ë°”ì¼ ì•± | Slack Bot
                    |
              HTTPS/WebSocket
                    |
                    v
[API ê³„ì¸µ]
FastAPI/Flask/Express Server
â”œâ”€ ì¸ì¦/ì¸ê°€ (JWT)
â”œâ”€ Rate Limiting (Redis)
â”œâ”€ Request Validation
â””â”€ LangChain ì²´ì¸ ì‹¤í–‰
                    |
                    v
[LangChain ê³„ì¸µ]
â”œâ”€ RAG Chain (LCEL)
â”œâ”€ Conversational Chain (Memory)
â”œâ”€ Agent Executor (Tools)
â””â”€ LangSmith Tracing
                    |
        +-----------+-----------+
        |           |           |
        v           v           v
   [Vector DB]   [LLM API]  [Embeddings]
   - Pinecone    - OpenAI    - OpenAI
   - Chroma      - Anthropic - Cohere
   - MongoDB     - Bedrock   - Ollama
   - Supabase
                    |
                    v
[ëª¨ë‹ˆí„°ë§ ê³„ì¸µ]
â”œâ”€ LangSmith (ì²´ì¸ ì¶”ì , ì„±ëŠ¥ ë¶„ì„)
â”œâ”€ Prometheus/Grafana (ë©”íŠ¸ë¦­)
â”œâ”€ ELK Stack (ë¡œê·¸)
â””â”€ Sentry (ì—ëŸ¬ ì¶”ì )
```

---

## ğŸš€ í™˜ê²½ êµ¬ì¶•

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **Python**: 3.8 ì´ìƒ (3.10+ ê¶Œì¥)
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 4GB RAM (8GB+ ê¶Œì¥)
- **ìš´ì˜ì²´ì œ**: Windows 10+, macOS 11+, Ubuntu 20.04+
- **íŒ¨í‚¤ì§€ ê´€ë¦¬ì**: pip ë˜ëŠ” poetry

### 1. LangChain ì„¤ì¹˜

#### í•µì‹¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ LangChain ì„¤ì¹˜
pip install langchain langchain-community

# ì£¼ìš” í†µí•© íŒ¨í‚¤ì§€
pip install langchain-openai      # OpenAI í†µí•©
pip install langchain-anthropic   # Anthropic (Claude) í†µí•©
pip install langchain-google-genai # Google (Gemini) í†µí•©
pip install langchain-cohere      # Cohere í†µí•©
```

#### ë²¡í„° ìŠ¤í† ì–´ë³„ ì„¤ì¹˜

```bash
# FAISS (ë¡œì»¬, ë¹ ë¥¸ ê²€ìƒ‰)
pip install faiss-cpu  # CPU ë²„ì „
# pip install faiss-gpu  # GPU ë²„ì „ (CUDA í•„ìš”)

# Chroma (ë¡œì»¬, ê²½ëŸ‰)
pip install chromadb

# Pinecone (í´ë¼ìš°ë“œ)
pip install pinecone-client

# MongoDB Atlas
pip install pymongo

# Supabase
pip install supabase

# Weaviate
pip install weaviate-client

# Qdrant
pip install qdrant-client
```

#### ë¬¸ì„œ ë¡œë” ë° ìœ í‹¸ë¦¬í‹°

```bash
# ë¬¸ì„œ ì²˜ë¦¬
pip install pypdf          # PDF ë¡œë”©
pip install python-docx    # Word ë¬¸ì„œ
pip install beautifulsoup4 # HTML íŒŒì‹±
pip install lxml           # HTML/XML íŒŒì‹±

# Unstructured (ë‹¤ì–‘í•œ í¬ë§· ì§€ì›)
pip install unstructured
pip install "unstructured[pdf]"

# í…ìŠ¤íŠ¸ ë¶„í• 
pip install tiktoken       # OpenAI í† í° ì¹´ìš´í„°

# í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
pip install python-dotenv
```

#### ì „ì²´ requirements.txt

```txt
# LangChain í•µì‹¬
langchain==0.1.0
langchain-community==0.0.13
langchain-core==0.1.10

# LLM í†µí•©
langchain-openai==0.0.2
langchain-anthropic==0.0.1
langchain-google-genai==0.0.5
langchain-cohere==0.0.3

# ë²¡í„° ìŠ¤í† ì–´
faiss-cpu==1.7.4
chromadb==0.4.22
pinecone-client==3.0.0
pymongo==4.6.1
supabase==2.3.0

# ë¬¸ì„œ ì²˜ë¦¬
pypdf==3.17.4
python-docx==1.1.0
beautifulsoup4==4.12.2
lxml==5.1.0
unstructured==0.11.8
tiktoken==0.5.2

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.0
requests==2.31.0

# ëª¨ë‹ˆí„°ë§ (ì„ íƒ)
langsmith==0.0.77
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ ìƒì„±:

```env
# LLM API í‚¤
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
GOOGLE_API_KEY=your-google-api-key
COHERE_API_KEY=your-cohere-key

# ë²¡í„° ìŠ¤í† ì–´
PINECONE_API_KEY=your-pinecone-key
PINECONE_ENVIRONMENT=us-east-1-aws  # ë˜ëŠ” ë‹¤ë¥¸ í™˜ê²½

# MongoDB Atlas
MONGODB_URI=mongodb+srv://user:password@cluster.mongodb.net/

# Supabase
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-supabase-anon-key

# LangSmith (ì„ íƒ, ë””ë²„ê¹…ìš©)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-key
LANGCHAIN_PROJECT=my-rag-project

# Ollama (ë¡œì»¬ LLM, ì„ íƒ)
OLLAMA_BASE_URL=http://localhost:11434
```

### 3. ì„¤ì¹˜ í™•ì¸

```python
# test_setup.py
import os
from dotenv import load_dotenv

load_dotenv()

def test_imports():
    """íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸"""
    print("1ï¸âƒ£ íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸...\n")

    try:
        import langchain
        print(f"âœ… LangChain ë²„ì „: {langchain.__version__}")

        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        print("âœ… LangChain OpenAI í†µí•©")

        from langchain_community.vectorstores import FAISS, Chroma
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ (FAISS, Chroma)")

        from langchain.text_splitter import RecursiveCharacterTextSplitter
        print("âœ… Text Splitter")

        from langchain_community.document_loaders import PyPDFLoader
        print("âœ… Document Loaders")

        from langchain.chains import RetrievalQA
        print("âœ… Chains")

        return True
    except ImportError as e:
        print(f"âŒ Import ì‹¤íŒ¨: {e}")
        return False

def test_openai_connection():
    """OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\n2ï¸âƒ£ OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸...\n")

    try:
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings

        # LLM í…ŒìŠ¤íŠ¸
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0
        )
        response = llm.invoke("ì•ˆë…•í•˜ì„¸ìš”!")
        print(f"âœ… LLM ì‘ë‹µ: {response.content[:50]}...")

        # ì„ë² ë”© í…ŒìŠ¤íŠ¸
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vector = embeddings.embed_query("í…ŒìŠ¤íŠ¸")
        print(f"âœ… ì„ë² ë”© ì°¨ì›: {len(vector)}")

        return True
    except Exception as e:
        print(f"âŒ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def test_vector_store():
    """ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸"""
    print("\n3ï¸âƒ£ ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸...\n")

    try:
        from langchain_openai import OpenAIEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain.schema import Document

        # ìƒ˜í”Œ ë¬¸ì„œ
        docs = [
            Document(page_content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
            Document(page_content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤."),
        ]

        # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = FAISS.from_documents(docs, embeddings)

        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        results = vectorstore.similarity_search("LangChainì´ë€?", k=1)
        print(f"âœ… ê²€ìƒ‰ ê²°ê³¼: {results[0].page_content}")

        return True
    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    print("=" * 60)
    print("  LangChain RAG í™˜ê²½ ì„¤ì • í™•ì¸")
    print("=" * 60 + "\n")

    imports_ok = test_imports()
    openai_ok = test_openai_connection()
    vectorstore_ok = test_vector_store()

    print("\n" + "=" * 60)
    if imports_ok and openai_ok and vectorstore_ok:
        print("ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("   ì´ì œ LangChain RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸  ì¼ë¶€ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ìœ„ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.")
    print("=" * 60)

if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
python test_setup.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
============================================================
  LangChain RAG í™˜ê²½ ì„¤ì • í™•ì¸
============================================================

1ï¸âƒ£ íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸...

âœ… LangChain ë²„ì „: 0.1.0
âœ… LangChain OpenAI í†µí•©
âœ… ë²¡í„° ìŠ¤í† ì–´ (FAISS, Chroma)
âœ… Text Splitter
âœ… Document Loaders
âœ… Chains

2ï¸âƒ£ OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸...

âœ… LLM ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?...
âœ… ì„ë² ë”© ì°¨ì›: 1536

3ï¸âƒ£ ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸...

âœ… ê²€ìƒ‰ ê²°ê³¼: LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

============================================================
ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
   ì´ì œ LangChain RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.
============================================================
```

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ RAG ì‹œìŠ¤í…œ (5ë¶„ ë§Œì— êµ¬í˜„)

```python
# quick_start.py
import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.schema import Document

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 1. ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„
documents = [
    Document(
        page_content="LangChainì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. "
                     "2022ë…„ 10ì›” Harrison Chaseê°€ ê°œë°œí–ˆìœ¼ë©°, Pythonê³¼ JavaScriptë¥¼ ì§€ì›í•©ë‹ˆë‹¤.",
        metadata={"source": "langchain_intro.txt"}
    ),
    Document(
        page_content="RAG(Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ë‹µë³€ì„ ê°•í™”í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. "
                     "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤.",
        metadata={"source": "rag_intro.txt"}
    ),
    Document(
        page_content="LangChainì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ëŠ” Models, Prompts, Chains, Memory, Agentsì…ë‹ˆë‹¤. "
                     "LCEL(LangChain Expression Language)ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        metadata={"source": "langchain_components.txt"}
    ),
]

# 2. í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í‚¹)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
splits = text_splitter.split_documents(documents)
print(f"ğŸ“„ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(splits)}\n")

# 3. ì„ë² ë”© ìƒì„± ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = FAISS.from_documents(splits, embeddings)
print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n")

# 4. Retriever ìƒì„±
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 2}  # ìƒìœ„ 2ê°œ ë¬¸ì„œ ê²€ìƒ‰
)

# 5. LLM ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)

# 6. RAG Chain êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ëª¨ë“  ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì „ë‹¬
    retriever=retriever,
    return_source_documents=True
)

# 7. ì§ˆë¬¸í•˜ê¸°
def ask_question(question: str):
    """RAG ì‹œìŠ¤í…œì— ì§ˆë¬¸"""
    print(f"â“ ì§ˆë¬¸: {question}\n")

    result = qa_chain.invoke({"query": question})

    print(f"ğŸ’¡ ë‹µë³€: {result['result']}\n")
    print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ ({len(result['source_documents'])}ê°œ):")
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"  {i}. {doc.metadata.get('source', 'Unknown')}")
        print(f"     {doc.page_content[:100]}...\n")

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("=" * 70)
    print("  LangChain RAG ë¹ ë¥¸ ì‹œì‘")
    print("=" * 70 + "\n")

    # ì§ˆë¬¸ 1
    ask_question("LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")

    print("-" * 70 + "\n")

    # ì§ˆë¬¸ 2
    ask_question("RAGì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
```

**ì‹¤í–‰:**
```bash
python quick_start.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
======================================================================
  LangChain RAG ë¹ ë¥¸ ì‹œì‘
======================================================================

ğŸ“„ ë¬¸ì„œ ì²­í¬ ìˆ˜: 3

âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ

â“ ì§ˆë¬¸: LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸ’¡ ë‹µë³€: LangChainì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤
í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 2022ë…„ 10ì›” Harrison Chaseê°€ ê°œë°œí–ˆìœ¼ë©°, Pythonê³¼
JavaScriptë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë¡œëŠ” Models, Prompts, Chains,
Memory, Agentsê°€ ìˆìœ¼ë©°, LCEL(LangChain Expression Language)ì„ ì‚¬ìš©í•˜ì—¬
ì²´ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³  ë¬¸ì„œ (2ê°œ):
  1. langchain_intro.txt
     LangChainì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ...

  2. langchain_components.txt
     LangChainì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ëŠ” Models, Prompts, Chains, Memory, Agentsì…ë‹ˆë‹¤. ...

----------------------------------------------------------------------

â“ ì§ˆë¬¸: RAGì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

ğŸ’¡ ë‹µë³€: RAG(Retrieval-Augmented Generation)ì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ëŠ” ì™¸ë¶€ ì§€ì‹ì„
ê²€ìƒ‰í•˜ëŠ” ê²€ìƒ‰ ì‹œìŠ¤í…œê³¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨
ë¬¸ì„œë¥¼ ì°¾ì•„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•˜ì—¬ LLMì˜ ë‹µë³€ì„ ê°•í™”í•©ë‹ˆë‹¤.

ğŸ“š ì°¸ê³  ë¬¸ì„œ (2ê°œ):
  1. rag_intro.txt
     RAG(Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ë‹µë³€ì„ ê°•í™”í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ...

  2. langchain_components.txt
     LangChainì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ëŠ” Models, Prompts, Chains, Memory, Agentsì…ë‹ˆë‹¤. ...
```

### LCEL ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„

```python
# quick_start_lcel.py
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ë¬¸ì„œ ì¤€ë¹„
documents = [
    Document(page_content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
    Document(page_content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤."),
]

# ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = text_splitter.split_documents(documents)
vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())
retriever = vectorstore.as_retriever()

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

prompt = ChatPromptTemplate.from_template(template)

# LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# LCEL Chain êµ¬ì„±
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ì‹¤í–‰
if __name__ == "__main__":
    response = rag_chain.invoke("LangChainì´ë€?")
    print(f"ë‹µë³€: {response}")
```

---

## ğŸ“š ìƒì„¸ ê°€ì´ë“œ

### 1. Document Loaders: ë‹¤ì–‘í•œ ë¬¸ì„œ ë¡œë”©

#### 1.1 PDF ë¬¸ì„œ ë¡œë”©

```python
from langchain_community.document_loaders import PyPDFLoader, PDFPlumberLoader

# ë°©ë²• 1: PyPDFLoader (ê¸°ë³¸, ë¹ ë¦„)
loader = PyPDFLoader("documents/company_handbook.pdf")
pages = loader.load()
print(f"ì´ í˜ì´ì§€: {len(pages)}")
print(f"ì²« í˜ì´ì§€ ë‚´ìš©: {pages[0].page_content[:200]}...")

# ë°©ë²• 2: PDFPlumberLoader (í‘œ ì¶”ì¶œ ìš°ìˆ˜)
loader = PDFPlumberLoader("documents/financial_report.pdf")
pages = loader.load()

# ê° í˜ì´ì§€ëŠ” Document ê°ì²´
for i, page in enumerate(pages[:3]):
    print(f"\ní˜ì´ì§€ {i+1} ë©”íƒ€ë°ì´í„°: {page.metadata}")
    print(f"ë‚´ìš©: {page.page_content[:100]}...")
```

**ë©”íƒ€ë°ì´í„°:**
```python
{
    'source': 'documents/company_handbook.pdf',
    'page': 0  # í˜ì´ì§€ ë²ˆí˜¸
}
```

#### 1.2 Word ë¬¸ì„œ ë¡œë”©

```python
from langchain_community.document_loaders import Docx2txtLoader

loader = Docx2txtLoader("documents/policy.docx")
docs = loader.load()

print(f"ë¬¸ì„œ ìˆ˜: {len(docs)}")
print(f"ë‚´ìš©: {docs[0].page_content}")
```

#### 1.3 CSV ë°ì´í„° ë¡œë”©

```python
from langchain_community.document_loaders import CSVLoader

# ê¸°ë³¸ ë¡œë”©
loader = CSVLoader(
    file_path="data/products.csv",
    encoding="utf-8"
)
docs = loader.load()

# íŠ¹ì • ì»¬ëŸ¼ë§Œ ì‚¬ìš©
loader = CSVLoader(
    file_path="data/products.csv",
    csv_args={
        'delimiter': ',',
        'quotechar': '"',
    },
    source_column="product_name"  # ë©”íƒ€ë°ì´í„°ì— í¬í•¨ë  ì»¬ëŸ¼
)
docs = loader.load()

# ê° í–‰ì´ í•˜ë‚˜ì˜ Document
for doc in docs[:3]:
    print(f"ë‚´ìš©: {doc.page_content}")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}\n")
```

#### 1.4 ì›¹ í˜ì´ì§€ ë¡œë”©

```python
from langchain_community.document_loaders import WebBaseLoader

# ë‹¨ì¼ URL
loader = WebBaseLoader("https://python.langchain.com/docs/introduction/")
docs = loader.load()

# ì—¬ëŸ¬ URL
urls = [
    "https://python.langchain.com/docs/introduction/",
    "https://python.langchain.com/docs/get_started/quickstart/",
]
loader = WebBaseLoader(urls)
docs = loader.load()

print(f"ë¡œë“œëœ ë¬¸ì„œ: {len(docs)}ê°œ")
```

#### 1.5 ë””ë ‰í† ë¦¬ ì¼ê´„ ë¡œë”©

```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader

# íŠ¹ì • í™•ì¥ì íŒŒì¼ë§Œ ë¡œë”©
loader = DirectoryLoader(
    "documents/",
    glob="**/*.txt",  # ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì˜ .txt íŒŒì¼
    loader_cls=TextLoader,
    loader_kwargs={'encoding': 'utf-8'}
)
docs = loader.load()

print(f"ë¡œë“œëœ ë¬¸ì„œ: {len(docs)}ê°œ")

# PDF íŒŒì¼ ì¼ê´„ ë¡œë”©
from langchain_community.document_loaders import PyPDFLoader

loader = DirectoryLoader(
    "documents/",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader
)
docs = loader.load()
```

#### 1.6 ì½”ë“œ íŒŒì¼ ë¡œë”©

```python
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import LanguageParser
from langchain_text_splitters import Language

# Python ì½”ë“œ ë¡œë”©
loader = GenericLoader.from_filesystem(
    "src/",
    glob="**/*.py",
    suffixes=[".py"],
    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)
)
docs = loader.load()

# JavaScript ì½”ë“œ ë¡œë”©
loader = GenericLoader.from_filesystem(
    "frontend/",
    glob="**/*.js",
    suffixes=[".js"],
    parser=LanguageParser(language=Language.JS)
)
docs = loader.load()
```

### 2. Text Splitters: íš¨ê³¼ì ì¸ ì²­í‚¹ ì „ëµ

#### 2.1 RecursiveCharacterTextSplitter (ê°€ì¥ ì¼ë°˜ì )

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ê¸°ë³¸ ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # ì²­í¬ í¬ê¸°
    chunk_overlap=200,      # ì²­í¬ ê°„ ì¤‘ë³µ
    length_function=len,    # ê¸¸ì´ ì¸¡ì • í•¨ìˆ˜
    is_separator_regex=False,
)

# í…ìŠ¤íŠ¸ ë¶„í• 
text = """
LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ëª¨ë¸ í†µí•©
2. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
3. ì²´ì¸ êµ¬ì„±
"""

chunks = text_splitter.split_text(text)
print(f"ì²­í¬ ìˆ˜: {len(chunks)}")
for i, chunk in enumerate(chunks):
    print(f"\nì²­í¬ {i+1}:\n{chunk}")

# Document ê°ì²´ ë¶„í• 
from langchain.schema import Document

docs = [Document(page_content=text, metadata={"source": "intro.txt"})]
split_docs = text_splitter.split_documents(docs)

for doc in split_docs:
    print(f"ë‚´ìš©: {doc.page_content[:100]}...")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}\n")
```

**ë¶„í•  ì „ëµ:**
```python
# ê³„ì¸µì  êµ¬ë¶„ì ìˆœì„œ:
# 1. "\n\n" (ë¬¸ë‹¨)
# 2. "\n"   (ì¤„ë°”ê¿ˆ)
# 3. " "    (ê³µë°±)
# 4. ""     (ë¬¸ì ë‹¨ìœ„)
```

#### 2.2 TokenTextSplitter (í† í° ê¸°ë°˜)

```python
from langchain.text_splitter import TokenTextSplitter

# OpenAI í† í° ê¸°ì¤€
text_splitter = TokenTextSplitter(
    encoding_name="cl100k_base",  # GPT-4 í† í° ì¸ì½”ë”©
    chunk_size=500,                # 500 í† í°
    chunk_overlap=50
)

chunks = text_splitter.split_text(text)

# tiktokenìœ¼ë¡œ í† í° ìˆ˜ í™•ì¸
import tiktoken
encoding = tiktoken.get_encoding("cl100k_base")
for i, chunk in enumerate(chunks):
    token_count = len(encoding.encode(chunk))
    print(f"ì²­í¬ {i+1}: {token_count} í† í°")
```

#### 2.3 MarkdownHeaderTextSplitter

```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

markdown_text = """
# LangChain ê°€ì´ë“œ

## ì†Œê°œ

LangChainì€ LLM í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

### ëª¨ë¸ í†µí•©

ë‹¤ì–‘í•œ LLMì„ ì§€ì›í•©ë‹ˆë‹¤.

### ì²´ì¸ êµ¬ì„±

LCELì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

# í—¤ë” ê¸°ì¤€ ë¶„í• 
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)
md_header_splits = markdown_splitter.split_text(markdown_text)

for doc in md_header_splits:
    print(f"ë‚´ìš©: {doc.page_content}")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}\n")
```

**ì¶œë ¥:**
```python
{
    'Header 1': 'LangChain ê°€ì´ë“œ',
    'Header 2': 'ì†Œê°œ'
}
{
    'Header 1': 'LangChain ê°€ì´ë“œ',
    'Header 2': 'ì£¼ìš” ê¸°ëŠ¥',
    'Header 3': 'ëª¨ë¸ í†µí•©'
}
```

#### 2.4 PythonCodeTextSplitter

```python
from langchain.text_splitter import PythonCodeTextSplitter

python_code = """
def hello_world():
    print("Hello, World!")

class MyClass:
    def __init__(self):
        self.value = 0

    def increment(self):
        self.value += 1
"""

python_splitter = PythonCodeTextSplitter(
    chunk_size=100,
    chunk_overlap=0
)

chunks = python_splitter.split_text(python_code)
for i, chunk in enumerate(chunks):
    print(f"\nì²­í¬ {i+1}:\n{chunk}")
```

#### 2.5 ìµœì  ì²­í‚¹ ì „ëµ

```python
# í”„ë¡œë•ì…˜ ê¶Œì¥ ì„¤ì •
from langchain.text_splitter import RecursiveCharacterTextSplitter

def create_optimized_splitter(chunk_size=1000, chunk_overlap=200):
    """ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±"""
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=[
            "\n\n",  # ë¬¸ë‹¨
            "\n",    # ì¤„ë°”ê¿ˆ
            ". ",    # ë¬¸ì¥
            "! ",
            "? ",
            ", ",    # êµ¬ë¬¸
            " ",     # ë‹¨ì–´
            "",      # ë¬¸ì
        ]
    )

# ë¬¸ì„œ íƒ€ì…ë³„ ê¶Œì¥ í¬ê¸°
CHUNK_SIZES = {
    "general": {"size": 1000, "overlap": 200},      # ì¼ë°˜ ë¬¸ì„œ
    "code": {"size": 500, "overlap": 50},           # ì½”ë“œ
    "chat": {"size": 500, "overlap": 100},          # ëŒ€í™”í˜•
    "academic": {"size": 1500, "overlap": 300},     # í•™ìˆ  ë…¼ë¬¸
}

# ì‚¬ìš© ì˜ˆ
splitter = create_optimized_splitter(**CHUNK_SIZES["general"])
chunks = splitter.split_documents(documents)
```

### 3. Embeddings: ë²¡í„° ì„ë² ë”© ìƒì„±

#### 3.1 OpenAI Embeddings

```python
from langchain_openai import OpenAIEmbeddings

# ê¸°ë³¸ ì„¤ì •
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",  # ë˜ëŠ” text-embedding-3-large
    # api_key="sk-..."  # í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ì‚¬ìš©
)

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
text = "LangChainì€ LLM í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
vector = embeddings.embed_query(text)
print(f"ë²¡í„° ì°¨ì›: {len(vector)}")
print(f"ì²« 5ê°œ ê°’: {vector[:5]}")

# ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì¼ê´„ ì„ë² ë”©
texts = [
    "LangChainì€ LLM í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
    "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤.",
    "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì„ë² ë”©ì„ ì €ì¥í•©ë‹ˆë‹¤."
]
vectors = embeddings.embed_documents(texts)
print(f"ì„ë² ë”© ìˆ˜: {len(vectors)}")
```

**ëª¨ë¸ ë¹„êµ:**
```python
# text-embedding-3-small
# - ì°¨ì›: 1536
# - ë¹„ìš©: $0.00002/1K í† í°
# - ì†ë„: ë¹ ë¦„
# - ìš©ë„: ì¼ë°˜ì ì¸ RAG

# text-embedding-3-large
# - ì°¨ì›: 3072
# - ë¹„ìš©: $0.00013/1K í† í°
# - ì •í™•ë„: ë†’ìŒ
# - ìš©ë„: ê³ ì •ë°€ ê²€ìƒ‰
```

#### 3.2 Cohere Embeddings

```python
from langchain_cohere import CohereEmbeddings

embeddings = CohereEmbeddings(
    model="embed-multilingual-v3.0",
    # api_key="your-cohere-key"
)

# í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜
vector = embeddings.embed_query("í•œêµ­ì–´ í…ìŠ¤íŠ¸ë„ ì˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
print(f"ë²¡í„° ì°¨ì›: {len(vector)}")
```

#### 3.3 HuggingFace Embeddings (ë¡œì»¬)

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

# ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (API ë¹„ìš© ì—†ìŒ)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},  # 'cuda' for GPU
    encode_kwargs={'normalize_embeddings': True}
)

vector = embeddings.embed_query("ë¡œì»¬ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
print(f"ë²¡í„° ì°¨ì›: {len(vector)}")  # 384
```

**ì¶”ì²œ ëª¨ë¸:**
```python
# ì˜ì–´
"sentence-transformers/all-MiniLM-L6-v2"  # 384ì°¨ì›, ë¹ ë¦„
"sentence-transformers/all-mpnet-base-v2"  # 768ì°¨ì›, ì •í™•

# ë‹¤êµ­ì–´ (í•œêµ­ì–´ í¬í•¨)
"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
"BAAI/bge-m3"  # ì¤‘êµ­ì–´ ì—°êµ¬ì§„, í•œêµ­ì–´ ìš°ìˆ˜
```

#### 3.4 Ollama Embeddings (ë¡œì»¬ LLM)

```python
from langchain_community.embeddings import OllamaEmbeddings

# Ollama ì„œë²„ í•„ìš”: ollama pull nomic-embed-text
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

vector = embeddings.embed_query("ë¡œì»¬ Ollamaë¡œ ì„ë² ë”©")
print(f"ë²¡í„° ì°¨ì›: {len(vector)}")  # 768
```

#### 3.5 ì„ë² ë”© ë¹„ìš© ìµœì í™”

```python
from functools import lru_cache
from langchain_openai import OpenAIEmbeddings

class CachedEmbeddings:
    """ì„ë² ë”© ê²°ê³¼ ìºì‹±"""

    def __init__(self, embeddings_model):
        self.embeddings = embeddings_model
        self._cache = {}

    def embed_query(self, text: str):
        """ì¿¼ë¦¬ ì„ë² ë”© (ìºì‹±)"""
        if text not in self._cache:
            self._cache[text] = self.embeddings.embed_query(text)
        return self._cache[text]

    def embed_documents(self, texts: list[str]):
        """ë¬¸ì„œ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)"""
        return self.embeddings.embed_documents(texts)

# ì‚¬ìš©
base_embeddings = OpenAIEmbeddings()
cached_embeddings = CachedEmbeddings(base_embeddings)

# ë™ì¼ í…ìŠ¤íŠ¸ ì¬ì‚¬ìš© ì‹œ API í˜¸ì¶œ ì—†ìŒ
v1 = cached_embeddings.embed_query("ê°™ì€ ì§ˆë¬¸")
v2 = cached_embeddings.embed_query("ê°™ì€ ì§ˆë¬¸")  # ìºì‹œì—ì„œ ë°˜í™˜
```

### 4. Vector Stores: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í†µí•©

#### 4.1 FAISS (ë¡œì»¬, ë¹ ë¦„)

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document

# ë¬¸ì„œ ì¤€ë¹„
docs = [
    Document(page_content="LangChainì€ LLM í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.", metadata={"id": 1}),
    Document(page_content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤.", metadata={"id": 2}),
]

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(docs, embeddings)

# ê²€ìƒ‰
results = vectorstore.similarity_search("LLM í”„ë ˆì„ì›Œí¬", k=1)
print(f"ê²°ê³¼: {results[0].page_content}")

# ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰
results_with_scores = vectorstore.similarity_search_with_score("LLM", k=2)
for doc, score in results_with_scores:
    print(f"ì ìˆ˜: {score:.4f} | ë‚´ìš©: {doc.page_content}")

# ì €ì¥ ë° ë¡œë“œ
vectorstore.save_local("faiss_index")
loaded_vectorstore = FAISS.load_local("faiss_index", embeddings)
```

#### 4.2 Chroma (ë¡œì»¬, ê²½ëŸ‰)

```python
from langchain_community.vectorstores import Chroma

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings,
    persist_directory="./chroma_db"  # ì˜êµ¬ ì €ì¥
)

# ê²€ìƒ‰
results = vectorstore.similarity_search("RAG", k=2)

# ë©”íƒ€ë°ì´í„° í•„í„°ë§
results = vectorstore.similarity_search(
    "í”„ë ˆì„ì›Œí¬",
    k=2,
    filter={"id": 1}  # id=1ì¸ ë¬¸ì„œë§Œ
)

# ì»¬ë ‰ì…˜ ì‚­ì œ
vectorstore.delete_collection()
```

#### 4.3 Pinecone (í´ë¼ìš°ë“œ, í”„ë¡œë•ì…˜)

```python
from langchain_community.vectorstores import Pinecone
from pinecone import Pinecone as PineconeClient, ServerlessSpec

# Pinecone ì´ˆê¸°í™”
pc = PineconeClient(api_key="your-api-key")

# ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒ í•œ ë²ˆë§Œ)
index_name = "langchain-rag"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # OpenAI embedding ì°¨ì›
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = Pinecone.from_documents(
    documents=docs,
    embedding=embeddings,
    index_name=index_name
)

# ê²€ìƒ‰
results = vectorstore.similarity_search("LangChain", k=3)
```

#### 4.4 MongoDB Atlas Vector Search

```python
from langchain_community.vectorstores import MongoDBAtlasVectorSearch
from pymongo import MongoClient

# MongoDB ì—°ê²°
client = MongoClient("mongodb+srv://user:password@cluster.mongodb.net/")
collection = client["rag_database"]["documents"]

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = MongoDBAtlasVectorSearch.from_documents(
    documents=docs,
    embedding=embeddings,
    collection=collection,
    index_name="vector_index"
)

# ë©”íƒ€ë°ì´í„° í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰
results = vectorstore.similarity_search(
    "LLM",
    k=3,
    pre_filter={"metadata.category": "AI"}
)
```

#### 4.5 Supabase Vector Store

```python
from langchain_community.vectorstores import SupabaseVectorStore
from supabase import create_client

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase_url = "https://your-project.supabase.co"
supabase_key = "your-anon-key"
supabase_client = create_client(supabase_url, supabase_key)

# ë²¡í„° ìŠ¤í† ì–´
vectorstore = SupabaseVectorStore.from_documents(
    documents=docs,
    embedding=embeddings,
    client=supabase_client,
    table_name="documents",
    query_name="match_documents"  # Supabaseì—ì„œ ìƒì„±í•œ í•¨ìˆ˜ ì´ë¦„
)

# ê²€ìƒ‰
results = vectorstore.similarity_search("RAG", k=2)
```

### 5. Retrievers: ê³ ê¸‰ ê²€ìƒ‰ ì „ëµ

#### 5.1 ê¸°ë³¸ Retriever

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())

# Similarity Search (ê¸°ë³¸)
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

results = retriever.invoke("LangChain")
print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")

# MMR (Maximum Marginal Relevance) - ë‹¤ì–‘ì„± ì¦ê°€
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,    # ì´ˆê¸° í›„ë³´ ìˆ˜
        "lambda_mult": 0.5  # 0=ë‹¤ì–‘ì„±, 1=ìœ ì‚¬ì„±
    }
)

# ì„ê³„ê°’ ê¸°ë°˜ ê²€ìƒ‰
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,  # 0.7 ì´ìƒë§Œ ë°˜í™˜
        "k": 5
    }
)
```

#### 5.2 MultiQueryRetriever (ì§ˆë¬¸ ë³€í˜•)

```python
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
base_retriever = vectorstore.as_retriever()

# ì§ˆë¬¸ì„ ì—¬ëŸ¬ í˜•íƒœë¡œ ë³€í˜•í•˜ì—¬ ê²€ìƒ‰
multi_retriever = MultiQueryRetriever.from_llm(
    retriever=base_retriever,
    llm=llm
)

# ì§ˆë¬¸ í•˜ë‚˜ë¡œ ì—¬ëŸ¬ ë³€í˜•ëœ ì¿¼ë¦¬ ìë™ ìƒì„±
results = multi_retriever.invoke("LangChainì˜ ì¥ì ì€?")

# ë‚´ë¶€ì ìœ¼ë¡œ ìƒì„±ëœ ì§ˆë¬¸ë“¤:
# - "LangChainì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
# - "LangChainì„ ì‚¬ìš©í•˜ë©´ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ì ì€?"
# - "LangChainì´ ì œê³µí•˜ëŠ” í˜œíƒì€?"
```

#### 5.3 ContextualCompressionRetriever (ì••ì¶•)

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
base_retriever = vectorstore.as_retriever()

# ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# ì§ˆë¬¸ì— ì§ì ‘ ê´€ë ¨ëœ ë¶€ë¶„ë§Œ ë°˜í™˜
results = compression_retriever.invoke("LangChainì˜ ì£¼ìš” ê¸°ëŠ¥ì€?")
for doc in results:
    print(f"ì••ì¶•ëœ ë‚´ìš©: {doc.page_content}")
```

#### 5.4 EnsembleRetriever (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)

```python
from langchain.retrievers import EnsembleRetriever, BM25Retriever

# BM25 (í‚¤ì›Œë“œ ê²€ìƒ‰)
bm25_retriever = BM25Retriever.from_documents(docs)
bm25_retriever.k = 3

# ë²¡í„° ê²€ìƒ‰
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ì•™ìƒë¸” (í•˜ì´ë¸Œë¦¬ë“œ)
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.4, 0.6]  # BM25: 40%, Vector: 60%
)

results = ensemble_retriever.invoke("LangChain RAG")
```

#### 5.5 SelfQueryRetriever (ë©”íƒ€ë°ì´í„° ìë™ í•„í„°)

```python
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

# ë©”íƒ€ë°ì´í„° ì •ì˜
metadata_field_info = [
    AttributeInfo(
        name="source",
        description="ë¬¸ì„œ ì¶œì²˜ (íŒŒì¼ëª…)",
        type="string",
    ),
    AttributeInfo(
        name="page",
        description="í˜ì´ì§€ ë²ˆí˜¸",
        type="integer",
    ),
    AttributeInfo(
        name="category",
        description="ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ (AI, Tech, Business ë“±)",
        type="string",
    ),
]

document_content_description = "LangChain ê¸°ìˆ  ë¬¸ì„œ"

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Self-Query Retriever
retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_contents=document_content_description,
    metadata_field_info=metadata_field_info,
    verbose=True
)

# ìì—°ì–´ë¡œ ì¿¼ë¦¬ (ìë™ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° í•„í„° ìƒì„±)
results = retriever.invoke("AI ì¹´í…Œê³ ë¦¬ì˜ LangChain ë¬¸ì„œë¥¼ ì°¾ì•„ì¤˜")
# ë‚´ë¶€ì ìœ¼ë¡œ filter={"category": "AI"} ìë™ ìƒì„±
```

---

## ğŸ”— ë²¡í„° ìŠ¤í† ì–´ í†µí•©

### FAISSì™€ RAG êµ¬ì¶•

```python
# faiss_rag.py
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

# 1. ë¬¸ì„œ ë¡œë”©
loader = PyPDFLoader("documents/guide.pdf")
pages = loader.load()

# 2. ì²­í‚¹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(pages)

# 3. FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = FAISS.from_documents(splits, embeddings)

# 4. ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)
vectorstore.save_local("faiss_index")

# 5. ë¡œë“œ
loaded_vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True  # ë¡œì»¬ íŒŒì¼ ì‹ ë¢°
)

# 6. RAG Chain
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=loaded_vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# 7. ì§ˆë¬¸
result = qa_chain.invoke({"query": "ê°€ì´ë“œì˜ ì£¼ìš” ë‚´ìš©ì€?"})
print(result['result'])
```

### Chromaì™€ RAG êµ¬ì¶•

```python
# chroma_rag.py
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ë¬¸ì„œ ì¤€ë¹„
docs = [
    Document(
        page_content="LangChainì€ 2022ë…„ 10ì›”ì— ì¶œì‹œëœ LLM í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
        metadata={"source": "intro.txt", "category": "Framework"}
    ),
    Document(
        page_content="RAGëŠ” ê²€ìƒ‰ì„ í†µí•´ LLMì˜ ë‹µë³€ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
        metadata={"source": "rag.txt", "category": "Technique"}
    ),
]

# Chroma ë²¡í„° ìŠ¤í† ì–´ (ì˜êµ¬ ì €ì¥)
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# ê¸°ì¡´ DB ë¡œë“œ
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# Retriever
retriever = vectorstore.as_retriever(
    search_type="mmr",  # MMRë¡œ ë‹¤ì–‘ì„± ì¦ê°€
    search_kwargs={"k": 3, "fetch_k": 10}
)

# LCEL Chain
template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

prompt = ChatPromptTemplate.from_template(template)
llm = ChatOpenAI(model="gpt-4o-mini")

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
for chunk in rag_chain.stream("LangChainì´ë€?"):
    print(chunk, end="", flush=True)
```

### Pineconeê³¼ RAG êµ¬ì¶• (í”„ë¡œë•ì…˜)

```python
# pinecone_rag.py
import os
from pinecone import Pinecone as PineconeClient, ServerlessSpec
from langchain_community.vectorstores import Pinecone
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# Pinecone ì´ˆê¸°í™”
pc = PineconeClient(api_key=os.getenv("PINECONE_API_KEY"))
index_name = "production-rag"

# ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒ í•œ ë²ˆ)
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

# ë¬¸ì„œ ë¡œë”©
loader = DirectoryLoader(
    "knowledge_base/",
    glob="**/*.txt",
    loader_cls=TextLoader
)
docs = loader.load()

# ì²­í‚¹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(docs)

# Pineconeì— ì—…ë¡œë“œ
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Pinecone.from_documents(
    documents=splits,
    embedding=embeddings,
    index_name=index_name
)

# ëŒ€í™”í˜• RAG Chain
llm = ChatOpenAI(model="gpt-4o", temperature=0)
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    memory=memory,
    return_source_documents=True
)

# ëŒ€í™”
while True:
    question = input("\nì§ˆë¬¸: ")
    if question.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
        break

    result = qa_chain.invoke({"question": question})
    print(f"\në‹µë³€: {result['answer']}")
    print(f"ì¶œì²˜: {len(result['source_documents'])}ê°œ ë¬¸ì„œ")
```

---

## ğŸš€ ê³ ê¸‰ RAG íŒ¨í„´

### 1. Conversational RAG (ëŒ€í™” ê¸°ë¡ ìœ ì§€)

```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„
vectorstore = FAISS.from_documents(documents, OpenAIEmbeddings())

# ë©”ëª¨ë¦¬ ì„¤ì •
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# ëŒ€í™”í˜• ì²´ì¸
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory,
    return_source_documents=True
)

# ëŒ€í™”
print(qa_chain.invoke({"question": "LangChainì´ë€?"})['answer'])
print(qa_chain.invoke({"question": "ê·¸ê²ƒì˜ ì£¼ìš” ê¸°ëŠ¥ì€?"})['answer'])  # "ê·¸ê²ƒ" = LangChain (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
```

### 2. Parent Document Retriever (ê³„ì¸µì  ê²€ìƒ‰)

```python
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# ë¶€ëª¨ ë¬¸ì„œ ì €ì¥ì†Œ
parent_store = InMemoryStore()

# ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰, í° ì²­í¬ ë°˜í™˜
child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)

vectorstore = Chroma(
    collection_name="split_parents",
    embedding_function=OpenAIEmbeddings()
)

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=parent_store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

# ë¬¸ì„œ ì¶”ê°€
retriever.add_documents(documents)

# ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰í•˜ì§€ë§Œ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
results = retriever.invoke("LangChain")
```

### 3. Reranking (ì¬ìˆœìœ„í™”)

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain_community.vectorstores import FAISS

# ê¸°ë³¸ retriever
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

# Cohere Reranker
compressor = CohereRerank(
    model="rerank-english-v2.0",
    top_n=5  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# ê²€ìƒ‰ í›„ ì¬ìˆœìœ„í™”
results = compression_retriever.invoke("ê°€ì¥ ê´€ë ¨ìˆëŠ” ë¬¸ì„œ")
```

### 4. HyDE (Hypothetical Document Embeddings)

```python
from langchain.chains import HypotheticalDocumentEmbedder
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# LLMìœ¼ë¡œ ê°€ìƒ ë¬¸ì„œ ìƒì„± í›„ ê²€ìƒ‰
llm = ChatOpenAI(model="gpt-4o-mini")
base_embeddings = OpenAIEmbeddings()

hyde_embeddings = HypotheticalDocumentEmbedder.from_llm(
    llm=llm,
    base_embeddings=base_embeddings,
    prompt_key="web_search"  # í…œí”Œë¦¿ ì„ íƒ
)

# ë²¡í„° ìŠ¤í† ì–´ì— ì ìš©
vectorstore = FAISS.from_documents(documents, hyde_embeddings)

# "LangChainì˜ ì¥ì "ìœ¼ë¡œ ê²€ìƒ‰ ì‹œ:
# 1. LLMì´ ê°€ìƒì˜ ë‹µë³€ ìƒì„±
# 2. ê°€ìƒ ë‹µë³€ì„ ì„ë² ë”©
# 3. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
results = vectorstore.similarity_search("LangChainì˜ ì¥ì ")
```

### 5. Multi-Vector Retriever (ë‹¤ì¤‘ ë²¡í„°)

```python
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import InMemoryByteStore
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ë¬¸ì„œ ìš”ì•½ ìƒì„± í›„ ê²€ìƒ‰
llm = ChatOpenAI(model="gpt-4o-mini")
vectorstore = Chroma(collection_name="summaries", embedding_function=OpenAIEmbeddings())
byte_store = InMemoryByteStore()

retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    byte_store=byte_store,
    id_key="doc_id"
)

# ìš”ì•½ ìƒì„± ì²´ì¸
summary_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ ë¬¸ì„œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n\n{doc}"
)
summary_chain = summary_prompt | llm | StrOutputParser()

# ìš”ì•½ ìƒì„± ë° ì €ì¥
import uuid
doc_ids = [str(uuid.uuid4()) for _ in documents]

summaries = []
for doc in documents:
    summary = summary_chain.invoke({"doc": doc.page_content})
    summaries.append(Document(page_content=summary, metadata={"doc_id": doc_ids[len(summaries)]}))

# ìš”ì•½ìœ¼ë¡œ ê²€ìƒ‰, ì›ë³¸ ë¬¸ì„œ ë°˜í™˜
retriever.vectorstore.add_documents(summaries)
retriever.docstore.mset(list(zip(doc_ids, documents)))

results = retriever.invoke("LangChain")  # ìš”ì•½ìœ¼ë¡œ ê²€ìƒ‰, ì›ë³¸ ë°˜í™˜
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ ì„ë² ë”©

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# âŒ ë‚˜ìœ ì˜ˆ: ë£¨í”„ë¡œ í•˜ë‚˜ì”©
vectors = []
for text in texts:
    vector = embeddings.embed_query(text)  # 100ë²ˆ API í˜¸ì¶œ
    vectors.append(vector)

# âœ… ì¢‹ì€ ì˜ˆ: ë°°ì¹˜ ì²˜ë¦¬
vectors = embeddings.embed_documents(texts)  # 1ë²ˆ API í˜¸ì¶œ
```

### 2. ì„ë² ë”© ìºì‹±

```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings

# ë¡œì»¬ ìºì‹œ ì €ì¥ì†Œ
store = LocalFileStore("./embedding_cache")

# ìºì‹œ ë˜í¼
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings=OpenAIEmbeddings(),
    document_embedding_cache=store,
    namespace="openai_embeddings"
)

# ë™ì¼ í…ìŠ¤íŠ¸ëŠ” ìºì‹œì—ì„œ ë°˜í™˜ (API í˜¸ì¶œ ì—†ìŒ)
vector1 = cached_embeddings.embed_query("ê°™ì€ í…ìŠ¤íŠ¸")
vector2 = cached_embeddings.embed_query("ê°™ì€ í…ìŠ¤íŠ¸")  # ìºì‹œ hit
```

### 3. ì²­í¬ í¬ê¸° ìµœì í™”

```python
# í”„ë¡œë•ì…˜ ê¶Œì¥ ì„¤ì •
from langchain.text_splitter import RecursiveCharacterTextSplitter

# GPT-4o-mini (128K context)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # ì ë‹¹í•œ í¬ê¸°
    chunk_overlap=200,    # 20% ì˜¤ë²„ë©
    length_function=len,
)

# ë” ì •ë°€í•˜ê²Œ: í† í° ìˆ˜ ê¸°ì¤€
from langchain.text_splitter import TokenTextSplitter

token_splitter = TokenTextSplitter(
    encoding_name="cl100k_base",  # GPT-4 ì¸ì½”ë”©
    chunk_size=500,               # 500 í† í°
    chunk_overlap=50
)
```

### 4. ì¸ë±ìŠ¤ ìµœì í™”

```python
# FAISS ì¸ë±ìŠ¤ íƒ€ì… ì„ íƒ
from langchain_community.vectorstores import FAISS
import faiss

# âœ… ì†Œê·œëª¨ (<100K ë²¡í„°): Flat (ì •í™•ë„ 100%)
index = faiss.IndexFlatL2(1536)

# âœ… ì¤‘ê·œëª¨ (100K-1M): IVFFlat (ë¹ ë¦„, ì •í™•ë„ 95%+)
quantizer = faiss.IndexFlatL2(1536)
index = faiss.IndexIVFFlat(quantizer, 1536, 100)  # 100ê°œ í´ëŸ¬ìŠ¤í„°

# âœ… ëŒ€ê·œëª¨ (1M+): HNSW (ë§¤ìš° ë¹ ë¦„, ì •í™•ë„ 99%+)
index = faiss.IndexHNSWFlat(1536, 32)  # M=32

# LangChainì—ì„œ ì‚¬ìš©
vectorstore = FAISS.from_documents(
    documents,
    embeddings,
    index=index
)
```

### 5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)

# LCEL ì²´ì¸
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
for chunk in chain.stream("LangChainì´ë€?"):
    print(chunk.content, end="", flush=True)
```

### 6. LLM ëª¨ë¸ ì„ íƒ

```python
# ë¹„ìš© vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„

# âœ… ë¹ ë¥´ê³  ì €ë ´ (ì¼ë°˜ RAG)
llm = ChatOpenAI(model="gpt-4o-mini")

# âœ… ê· í˜• (ë³µì¡í•œ RAG)
llm = ChatOpenAI(model="gpt-4o")

# âœ… ë¬´ë£Œ (ë¡œì»¬)
from langchain_community.llms import Ollama
llm = Ollama(model="llama3.1:8b")

# âœ… í•œêµ­ì–´ ìµœì í™”
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-5-sonnet-20241022")
```

---

## ğŸŒ í”„ë¡œë•ì…˜ ë°°í¬

### 1. FastAPI ì„œë²„ êµ¬ì¶•

```python
# app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
import uvicorn

app = FastAPI(title="LangChain RAG API")

# ì „ì—­ ë³€ìˆ˜ (ì„œë²„ ì‹œì‘ ì‹œ ë¡œë“œ)
vectorstore = None
qa_chain = None

class QueryRequest(BaseModel):
    question: str
    k: int = 3

class QueryResponse(BaseModel):
    answer: str
    sources: list[dict]

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
    global vectorstore, qa_chain

    print("Loading vector store...")
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.load_local(
        "faiss_index",
        embeddings,
        allow_dangerous_deserialization=True
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("Server ready!")

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """RAG ì§ˆì˜ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        result = qa_chain.invoke({"query": request.question})

        return QueryResponse(
            answer=result['result'],
            sources=[
                {
                    "content": doc.page_content[:200],
                    "metadata": doc.metadata
                }
                for doc in result['source_documents']
            ]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**ì‹¤í–‰:**
```bash
pip install fastapi uvicorn
python app.py
```

**í…ŒìŠ¤íŠ¸:**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "LangChainì´ë€?"}'
```

### 2. Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì•± ë³µì‚¬
COPY . .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì‹¤í–‰
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

**ë¹Œë“œ ë° ì‹¤í–‰:**
```bash
docker build -t langchain-rag .
docker run -p 8000:8000 -e OPENAI_API_KEY=sk-... langchain-rag
```

### 3. LangSmith ëª¨ë‹ˆí„°ë§

```python
# .envì— ì¶”ê°€
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-key
LANGCHAIN_PROJECT=production-rag

# ì½”ë“œì—ì„œ ìë™ ì¶”ì 
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
# ëª¨ë“  í˜¸ì¶œì´ LangSmithì— ìë™ ê¸°ë¡ë¨

# https://smith.langchain.com ì—ì„œ í™•ì¸:
# - ì²´ì¸ ì‹¤í–‰ ì¶”ì 
# - í† í° ì‚¬ìš©ëŸ‰
# - ì§€ì—° ì‹œê°„
# - ì˜¤ë¥˜ ë¡œê·¸
```

### 4. ì—ëŸ¬ í•¸ë“¤ë§

```python
from langchain.callbacks import get_openai_callback
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def query_with_retry(question: str):
    """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ì¿¼ë¦¬"""
    try:
        with get_openai_callback() as cb:
            result = qa_chain.invoke({"query": question})

            print(f"í† í° ì‚¬ìš©: {cb.total_tokens}")
            print(f"ë¹„ìš©: ${cb.total_cost:.4f}")

            return result
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

# ì‚¬ìš©
result = query_with_retry("LangChainì´ë€?")
```

---

## ğŸ’¼ ì‹¤ë¬´ ê°€ì´ë“œ

### 1. PDF ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (ì™„ì „ ì˜ˆì œ)

```python
# production_rag.py
import os
from pathlib import Path
from dotenv import load_dotenv

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.callbacks import get_openai_callback

load_dotenv()

class ProductionRAG:
    """í”„ë¡œë•ì…˜ê¸‰ RAG ì‹œìŠ¤í…œ"""

    def __init__(self, docs_path: str, index_path: str = "faiss_index"):
        self.docs_path = docs_path
        self.index_path = index_path
        self.vectorstore = None
        self.qa_chain = None

        # LLM ì„¤ì •
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            max_tokens=1000
        )

        # ì„ë² ë”© ì„¤ì •
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"
        )

    def load_documents(self):
        """PDF ë¬¸ì„œ ë¡œë”©"""
        print(f"ğŸ“‚ Loading documents from {self.docs_path}...")

        loader = DirectoryLoader(
            self.docs_path,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True
        )
        documents = loader.load()

        print(f"âœ… Loaded {len(documents)} pages")
        return documents

    def split_documents(self, documents):
        """ë¬¸ì„œ ì²­í‚¹"""
        print("âœ‚ï¸  Splitting documents...")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        splits = text_splitter.split_documents(documents)

        print(f"âœ… Created {len(splits)} chunks")
        return splits

    def create_vectorstore(self, splits):
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        print("ğŸ”„ Creating vector store...")

        self.vectorstore = FAISS.from_documents(
            splits,
            self.embeddings
        )

        # ì €ì¥
        self.vectorstore.save_local(self.index_path)
        print(f"âœ… Vector store saved to {self.index_path}")

    def load_vectorstore(self):
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        print(f"ğŸ“¥ Loading vector store from {self.index_path}...")

        self.vectorstore = FAISS.load_local(
            self.index_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        print("âœ… Vector store loaded")

    def setup_chain(self):
        """RAG ì²´ì¸ ì„¤ì •"""
        print("â›“ï¸  Setting up QA chain...")

        # ë©”ëª¨ë¦¬ (ìµœê·¼ 5ê°œ ëŒ€í™” ê¸°ì–µ)
        memory = ConversationBufferWindowMemory(
            k=5,
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

        # ì²´ì¸ ìƒì„±
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={"k": 5, "fetch_k": 20}
            ),
            memory=memory,
            return_source_documents=True,
            verbose=False
        )
        print("âœ… QA chain ready")

    def initialize(self, force_rebuild=False):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if force_rebuild or not Path(self.index_path).exists():
            # ìƒˆë¡œ ë¹Œë“œ
            documents = self.load_documents()
            splits = self.split_documents(documents)
            self.create_vectorstore(splits)
        else:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
            self.load_vectorstore()

        self.setup_chain()
        print("\nğŸ‰ RAG system initialized!\n")

    def query(self, question: str) -> dict:
        """ì§ˆë¬¸ ì²˜ë¦¬"""
        with get_openai_callback() as cb:
            result = self.qa_chain.invoke({"question": question})

            return {
                "answer": result["answer"],
                "sources": result["source_documents"],
                "tokens": cb.total_tokens,
                "cost": cb.total_cost
            }

    def chat(self):
        """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
        print("=" * 70)
        print("  LangChain RAG ì‹œìŠ¤í…œ")
        print("=" * 70)
        print("ì¢…ë£Œ: 'quit', 'exit', 'ì¢…ë£Œ'")
        print("-" * 70 + "\n")

        while True:
            question = input("â“ ì§ˆë¬¸: ").strip()

            if question.lower() in ["quit", "exit", "ì¢…ë£Œ", ""]:
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            try:
                result = self.query(question)

                print(f"\nğŸ’¡ ë‹µë³€:\n{result['answer']}\n")
                print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(result['sources'])}ê°œ")
                print(f"ğŸª™ í† í°: {result['tokens']} | ë¹„ìš©: ${result['cost']:.4f}\n")
                print("-" * 70 + "\n")

            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜: {e}\n")

# ì‹¤í–‰
if __name__ == "__main__":
    # 1. RAG ì‹œìŠ¤í…œ ìƒì„±
    rag = ProductionRAG(
        docs_path="./documents",
        index_path="./faiss_index"
    )

    # 2. ì´ˆê¸°í™” (force_rebuild=Trueë¡œ ì¸ë±ìŠ¤ ì¬ìƒì„±)
    rag.initialize(force_rebuild=False)

    # 3. ëŒ€í™” ì‹œì‘
    rag.chat()
```

**ë””ë ‰í† ë¦¬ êµ¬ì¡°:**
```
project/
â”œâ”€â”€ production_rag.py
â”œâ”€â”€ documents/           # PDF ë¬¸ì„œ ì €ì¥
â”‚   â”œâ”€â”€ guide1.pdf
â”‚   â”œâ”€â”€ guide2.pdf
â”‚   â””â”€â”€ manual.pdf
â”œâ”€â”€ faiss_index/         # ìƒì„±ë  ë²¡í„° ì¸ë±ìŠ¤
â”œâ”€â”€ .env
â””â”€â”€ requirements.txt
```

### 2. ì›¹ í¬ë¡¤ë§ + RAG

```python
# web_rag.py
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§
urls = [
    "https://python.langchain.com/docs/introduction/",
    "https://python.langchain.com/docs/get_started/quickstart/",
    "https://python.langchain.com/docs/concepts/",
]

loader = WebBaseLoader(urls)
docs = loader.load()

# ì²­í‚¹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(docs)

# ë²¡í„° ìŠ¤í† ì–´
vectorstore = Chroma.from_documents(
    splits,
    OpenAIEmbeddings(),
    persist_directory="./chroma_web"
)

# RAG Chain
template = """ë‹¤ìŒ LangChain ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

prompt = ChatPromptTemplate.from_template(template)
llm = ChatOpenAI(model="gpt-4o-mini")

rag_chain = (
    {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ì§ˆë¬¸
response = rag_chain.invoke("LangChainì˜ ì£¼ìš” ì»¨ì…‰ì€?")
print(response)
```

### 3. ë‹¤êµ­ì–´ RAG (í•œêµ­ì–´ ìµœì í™”)

```python
# multilingual_rag.py
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
embeddings = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ ìµœì í™”
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# í•œêµ­ì–´ ë¬¸ì„œ
korean_docs = [
    Document(page_content="ë­ì²´ì¸ì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
    Document(page_content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤."),
]

# ë²¡í„° ìŠ¤í† ì–´
vectorstore = FAISS.from_documents(korean_docs, embeddings)

# LLM (ClaudeëŠ” í•œêµ­ì–´ ìš°ìˆ˜)
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# RAG Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# í•œêµ­ì–´ ì§ˆë¬¸
result = qa_chain.invoke({"query": "ë­ì²´ì¸ì´ ë­ì•¼?"})
print(result['result'])
```

### 4. SQL ë°ì´í„°ë² ì´ìŠ¤ + RAG

```python
# sql_rag.py
from langchain_community.utilities import SQLDatabase
from langchain.chains import create_sql_query_chain
from langchain_openai import ChatOpenAI
from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool
from operator import itemgetter
from langchain_core.runnables import RunnablePassthrough

# DB ì—°ê²°
db = SQLDatabase.from_uri("sqlite:///company.db")

llm = ChatOpenAI(model="gpt-4o-mini")

# SQL ì¿¼ë¦¬ ìƒì„± ì²´ì¸
query_chain = create_sql_query_chain(llm, db)

# SQL ì‹¤í–‰ ë„êµ¬
execute_query = QuerySQLDataBaseTool(db=db)

# Chain êµ¬ì„±
chain = (
    RunnablePassthrough.assign(query=query_chain)
    | {
        "query": itemgetter("query"),
        "result": itemgetter("query") | execute_query
    }
)

# ìì—°ì–´ ì§ˆë¬¸
response = chain.invoke({"question": "2024ë…„ ë§¤ì¶œ ìƒìœ„ 5ê°œ ì œí’ˆì€?"})
print(f"SQL: {response['query']}")
print(f"ê²°ê³¼: {response['result']}")
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. "Module not found" ì—ëŸ¬

```bash
# íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip uninstall langchain langchain-community langchain-openai
pip install langchain langchain-community langchain-openai

# ê°€ìƒ í™˜ê²½ í™•ì¸
which python  # ê°€ìƒ í™˜ê²½ ê²½ë¡œ í™•ì¸
```

### 2. OpenAI API í‚¤ ì˜¤ë¥˜

```python
# .env íŒŒì¼ í™•ì¸
OPENAI_API_KEY=sk-your-actual-key  # ë”°ì˜´í‘œ ì—†ìŒ

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ í™•ì¸
from dotenv import load_dotenv
import os

load_dotenv()
print(os.getenv("OPENAI_API_KEY"))  # Noneì´ë©´ .env ê²½ë¡œ ë¬¸ì œ
```

### 3. FAISS "allow_dangerous_deserialization" ì—ëŸ¬

```python
# FAISS ë¡œë“œ ì‹œ í•„ìˆ˜ íŒŒë¼ë¯¸í„°
vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True  # ë¡œì»¬ íŒŒì¼ ì‹ ë¢°
)
```

### 4. ë©”ëª¨ë¦¬ ë¶€ì¡± (ëŒ€ìš©ëŸ‰ ë¬¸ì„œ)

```python
# ë°°ì¹˜ ì²˜ë¦¬
from langchain_community.vectorstores import FAISS

batch_size = 100
for i in range(0, len(splits), batch_size):
    batch = splits[i:i+batch_size]

    if i == 0:
        vectorstore = FAISS.from_documents(batch, embeddings)
    else:
        batch_vs = FAISS.from_documents(batch, embeddings)
        vectorstore.merge_from(batch_vs)

vectorstore.save_local("faiss_index")
```

### 5. í† í° ì œí•œ ì´ˆê³¼

```python
# ì²­í¬ í¬ê¸° ì¤„ì´ê¸°
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # ê¸°ë³¸ 1000ì—ì„œ ì¤„ì„
    chunk_overlap=50
)

# ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¤„ì´ê¸°
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 2}  # ê¸°ë³¸ 4ì—ì„œ ì¤„ì„
)
```

### 6. ëŠë¦° ê²€ìƒ‰ ì†ë„

```python
# FAISS ì¸ë±ìŠ¤ ìµœì í™”
import faiss

# HNSW ì¸ë±ìŠ¤ ì‚¬ìš©
index = faiss.IndexHNSWFlat(1536, 32)
vectorstore = FAISS.from_documents(
    documents,
    embeddings,
    index=index
)

# GPU ì‚¬ìš© (CUDA ì„¤ì¹˜ í•„ìš”)
# pip install faiss-gpu
```

---

## â“ FAQ

### Q1: LangChain vs LlamaIndex, ì–´ë–¤ ê²ƒì„ ì„ íƒí•´ì•¼ í•˜ë‚˜ìš”?

**A:**
- **LangChain**: ë²”ìš© LLM ì• í”Œë¦¬ì¼€ì´ì…˜ (ì±—ë´‡, ì—ì´ì „íŠ¸, ì›Œí¬í”Œë¡œìš°)
- **LlamaIndex**: RAG íŠ¹í™” (ë¬¸ì„œ ì¸ë±ì‹±, ê²€ìƒ‰ ìµœì í™”)

RAGë§Œ í•„ìš”í•˜ë©´ LlamaIndex, ë‹¤ì–‘í•œ ê¸°ëŠ¥ì´ í•„ìš”í•˜ë©´ LangChain ì¶”ì²œ.

### Q2: ì–´ë–¤ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì„ íƒí•´ì•¼ í•˜ë‚˜ìš”?

**A:**
- **ê°œë°œ/í…ŒìŠ¤íŠ¸**: FAISS (ë¡œì»¬, ë¬´ë£Œ)
- **ì†Œê·œëª¨ í”„ë¡œë•ì…˜**: Chroma (ê´€ë¦¬ í¸í•¨)
- **ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜**: Pinecone, Weaviate (ì™„ì „ ê´€ë¦¬í˜•)
- **ê¸°ì¡´ DB í™œìš©**: MongoDB Atlas, Supabase

### Q3: ì„ë² ë”© ëª¨ë¸ ì„ íƒ ê¸°ì¤€ì€?

**A:**
- **ì˜ì–´ ì¤‘ì‹¬**: OpenAI text-embedding-3-small
- **ë‹¤êµ­ì–´ (í•œêµ­ì–´)**: Cohere embed-multilingual-v3.0
- **ë¹„ìš© ì ˆê°**: HuggingFace (ë¡œì»¬), Ollama
- **ê³ ì •ë°€**: OpenAI text-embedding-3-large

### Q4: ì²­í¬ í¬ê¸°ëŠ” ì–¼ë§ˆê°€ ì ë‹¹í•œê°€ìš”?

**A:**
- **ì¼ë°˜ ë¬¸ì„œ**: 1000-1500ì, ì˜¤ë²„ë© 200-300ì
- **ì½”ë“œ**: 500-800ì, ì˜¤ë²„ë© 50-100ì
- **ëŒ€í™”í˜•**: 500ì, ì˜¤ë²„ë© 100ì

ì‹¤í—˜ì„ í†µí•´ ë„ë©”ì¸ë³„ë¡œ ìµœì í™”í•˜ì„¸ìš”.

### Q5: LangChain í•™ìŠµ ë¦¬ì†ŒìŠ¤ëŠ”?

**A:**
- ê³µì‹ ë¬¸ì„œ: https://python.langchain.com
- LangChain ë¸”ë¡œê·¸: https://blog.langchain.dev
- YouTube: LangChain ê³µì‹ ì±„ë„
- í•œêµ­ ì»¤ë®¤ë‹ˆí‹°: LangChain Korea (ë””ìŠ¤ì½”ë“œ)

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/)
- [LangChain API Reference](https://api.python.langchain.com/)
- [LangSmith](https://smith.langchain.com/)
- [LangChain GitHub](https://github.com/langchain-ai/langchain)

### íŠœí† ë¦¬ì–¼
- [LangChain Quickstart](https://python.langchain.com/docs/get_started/quickstart/)
- [RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)
- [LangChain Academy (ë¬´ë£Œ ì½”ìŠ¤)](https://academy.langchain.com/)

### ì»¤ë®¤ë‹ˆí‹°
- [LangChain Discord](https://discord.gg/langchain)
- [LangChain Twitter](https://twitter.com/langchainai)

### ê´€ë ¨ í”„ë¡œì íŠ¸
- [LangGraph](https://github.com/langchain-ai/langgraph): ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°
- [LangServe](https://github.com/langchain-ai/langserve): LangChain ì²´ì¸ ë°°í¬
- [LangChain Templates](https://github.com/langchain-ai/langchain/tree/master/templates): ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿

### í•œêµ­ì–´ ìë£Œ
- [LangChain í•œêµ­ ì‚¬ìš©ì ëª¨ì„](https://www.facebook.com/groups/langchainkr)
- [ë­ì²´ì¸ ê³µì‹ ë¬¸ì„œ í•œê¸€ ë²ˆì—­](https://wikidocs.net/book/14314)

---

## ğŸ“ ë¼ì´ì„ ìŠ¤

LangChain í”„ë ˆì„ì›Œí¬ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

**ğŸ‰ ì´ì œ LangChainìœ¼ë¡œ ê°•ë ¥í•œ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!**
