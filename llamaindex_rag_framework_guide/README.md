# LlamaIndexë¥¼ í™œìš©í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ

> í•œêµ­ì–´ ê°œë°œìë¥¼ ìœ„í•œ LlamaIndex ê¸°ë°˜ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ ì™„ë²½ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ì†Œê°œ](#-í”„ë¡œì íŠ¸-ì†Œê°œ)
- [ì™œ LlamaIndexì¸ê°€?](#-ì™œ-llamaindexì¸ê°€)
- [ì£¼ìš” ì‚¬ì–‘](#-ì£¼ìš”-ì‚¬ì–‘-specs)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [í™˜ê²½ êµ¬ì¶•](#-í™˜ê²½-êµ¬ì¶•)
- [ë¹ ë¥¸ ì‹œì‘](#-ë¹ ë¥¸-ì‹œì‘)
- [ìƒì„¸ ê°€ì´ë“œ](#-ìƒì„¸-ê°€ì´ë“œ)
- [ë²¡í„° ìŠ¤í† ì–´ í†µí•©](#-ë²¡í„°-ìŠ¤í† ì–´-í†µí•©)
- [ê³ ê¸‰ RAG íŒ¨í„´](#-ê³ ê¸‰-rag-íŒ¨í„´)
- [ì„±ëŠ¥ ìµœì í™”](#-ì„±ëŠ¥-ìµœì í™”)
- [í”„ë¡œë•ì…˜ ë°°í¬](#-í”„ë¡œë•ì…˜-ë°°í¬)
- [ì‹¤ë¬´ ê°€ì´ë“œ](#-ì‹¤ë¬´-ê°€ì´ë“œ)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
- [FAQ](#-faq)
- [ì°¸ê³  ìë£Œ](#-ì°¸ê³ -ìë£Œ)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ì†Œê°œ

ì´ í”„ë¡œì íŠ¸ëŠ” **LlamaIndex í”„ë ˆì„ì›Œí¬**ë¥¼ í™œìš©í•˜ì—¬ í”„ë¡œë•ì…˜ê¸‰ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.

### LlamaIndexë€?

**LlamaIndex**(êµ¬ GPT Index)ëŠ” LLMê³¼ ì™¸ë¶€ ë°ì´í„°ë¥¼ ì—°ê²°í•˜ëŠ” ë°ì´í„° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. RAG ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- **ê°œë°œì‚¬**: LlamaIndex (Jerry Liu, CEO)
- **ì¶œì‹œ**: 2022ë…„ 11ì›” (GPT Indexë¡œ ì‹œì‘)
- **GitHub Stars**: 35,000+ (2024ë…„ 12ì›” ê¸°ì¤€)
- **ì§€ì› ì–¸ì–´**: Python, TypeScript
- **ë¼ì´ì„ ìŠ¤**: MIT License

### ì£¼ìš” íŠ¹ì§•

- âœ… **RAG íŠ¹í™”**: ë¬¸ì„œ ì¸ë±ì‹±, ê²€ìƒ‰, ìƒì„±ì— ìµœì í™”ëœ ì„¤ê³„
- âœ… **ë°ì´í„° ì»¤ë„¥í„°**: 100+ ë°ì´í„° ì†ŒìŠ¤ (PDF, API, DB, SaaS ë“±) ê¸°ë³¸ ì§€ì›
- âœ… **ì¸ë±ìŠ¤ êµ¬ì¡°**: VectorStoreIndex, TreeIndex, KeywordTableIndex ë“± ë‹¤ì–‘í•œ ì¸ë±ìŠ¤
- âœ… **ì¿¼ë¦¬ ì—”ì§„**: ê³ ê¸‰ ì¿¼ë¦¬ ì²˜ë¦¬ ë° ë©€í‹°ìŠ¤í… ì¶”ë¡ 
- âœ… **ì—ì´ì „íŠ¸**: ë„êµ¬ ì‚¬ìš© ë° ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ì§€ì›
- âœ… **í‰ê°€ ë„êµ¬**: RAG ì„±ëŠ¥ ì¸¡ì • ë° ê°œì„  ë„êµ¬ ë‚´ì¥
- âœ… **LlamaHub**: ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ ë°ì´í„° ë¡œë” ë° ë„êµ¬ ì €ì¥ì†Œ

### í•™ìŠµ ëª©í‘œ

ì´ ê°€ì´ë“œë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. LlamaIndex í•µì‹¬ ê°œë… ë° ì¸ë±ìŠ¤ êµ¬ì¡° ì´í•´
2. ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œ ë¡œë”© ë° ì¸ë±ì‹±
3. VectorStoreIndex, SummaryIndex, TreeIndex í™œìš©
4. ê³ ê¸‰ ì¿¼ë¦¬ ì—”ì§„ ë° Retriever êµ¬í˜„
5. Sub-Question Query, Citation Query, Recursive Retrieval ë“± ê³ ê¸‰ íŒ¨í„´ êµ¬í˜„
6. LlamaIndex ì—ì´ì „íŠ¸ë¡œ ë³µì¡í•œ RAG ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
7. í‰ê°€ ë° ìµœì í™” ë„êµ¬ë¡œ RAG ì„±ëŠ¥ ê°œì„ 
8. í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

### ì‹¤ì œ í™œìš© ì‚¬ë¡€

LlamaIndex ê¸°ë°˜ RAGëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤:

- **ğŸ“š ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ**: ê¸°ìˆ  ë¬¸ì„œ, ë§¤ë‰´ì–¼ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
- **ğŸ” ì—”í„°í”„ë¼ì´ì¦ˆ ê²€ìƒ‰**: ì‚¬ë‚´ ì§€ì‹ë² ì´ìŠ¤ í†µí•© ê²€ìƒ‰
- **ğŸ’¼ ê³ ê° ì§€ì›**: FAQ, í‹°ì¼“ íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ìë™ ì‘ë‹µ
- **ğŸ“Š ë°ì´í„° ë¶„ì„**: êµ¬ì¡°í™”/ë¹„êµ¬ì¡°í™” ë°ì´í„° í†µí•© ë¶„ì„
- **ğŸ›ï¸ ë¦¬ì„œì¹˜ ë„êµ¬**: ë…¼ë¬¸, ë³´ê³ ì„œ ê²€ìƒ‰ ë° ìš”ì•½
- **ğŸ¤– ëŒ€í™”í˜• AI**: ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì±—ë´‡
- **ğŸ“ˆ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤**: ë³´ê³ ì„œ, ë©”íŠ¸ë¦­ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±

---

## ğŸ¤” ì™œ LlamaIndexì¸ê°€?

### LangChain vs LlamaIndex

| ê¸°ëŠ¥ | LangChain | LlamaIndex |
|------|-----------|------------|
| **ì£¼ìš” ëª©ì ** | ë²”ìš© LLM ì• í”Œë¦¬ì¼€ì´ì…˜ | **RAG íŠ¹í™”** |
| **ê°•ì ** | ì—ì´ì „íŠ¸, ì²´ì¸, ë‹¤ì–‘í•œ ìœ ìŠ¤ì¼€ì´ìŠ¤ | **ë¬¸ì„œ ì¸ë±ì‹± ë° ê²€ìƒ‰** |
| **ì¸ë±ìŠ¤ êµ¬ì¡°** | ê¸°ë³¸ì  | **ë‹¤ì–‘í•˜ê³  ê³ ê¸‰ (Tree, List, Keyword ë“±)** |
| **ë°ì´í„° ì»¤ë„¥í„°** | 100+ | **100+ (LlamaHub í¬í•¨)** |
| **ì¿¼ë¦¬ ì—”ì§„** | ê¸°ë³¸ì  | **ê³ ê¸‰ (Sub-Question, Citation, Router ë“±)** |
| **RAG í‰ê°€** | ë³„ë„ ë„êµ¬ í•„ìš” | **ë‚´ì¥ í‰ê°€ ë„êµ¬** |
| **í•™ìŠµ ê³¡ì„ ** | ë³´í†µ | **ì‰¬ì›€ (RAG ì¤‘ì‹¬)** |
| **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤** | ì±—ë´‡, ì—ì´ì „íŠ¸, ì›Œí¬í”Œë¡œìš° | **ë¬¸ì„œ ê²€ìƒ‰, Q&A, ì§€ì‹ë² ì´ìŠ¤** |

### RAGì— ìµœì ì¸ ì´ìœ 

1. **RAG ì „ìš© ì„¤ê³„**: ë¬¸ì„œ ì¸ë±ì‹±ê³¼ ê²€ìƒ‰ì— ìµœì í™”ëœ ì•„í‚¤í…ì²˜
2. **ë‹¤ì–‘í•œ ì¸ë±ìŠ¤**: VectorStore, Tree, List, Keyword ë“± ìƒí™©ë³„ ìµœì  ì¸ë±ìŠ¤
3. **ê³ ê¸‰ ì¿¼ë¦¬**: Sub-Question, Multi-Document, Recursive Retrieval ê¸°ë³¸ ì œê³µ
4. **ë°ì´í„° ì»¤ë„¥í„°**: 100+ ë°ì´í„° ì†ŒìŠ¤ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
5. **í‰ê°€ ë„êµ¬**: Faithfulness, Relevancy, Correctness ìë™ ì¸¡ì •
6. **ê°„ê²°í•œ API**: ëª‡ ì¤„ë¡œ í”„ë¡œë•ì…˜ê¸‰ RAG êµ¬ì¶• ê°€ëŠ¥
7. **LlamaHub**: ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ ë¡œë” ë° ë„êµ¬ í™œìš©

---

## ğŸ“‹ ì£¼ìš” ì‚¬ì–‘ (Specs)

### ì§€ì›í•˜ëŠ” LLM

| ì œê³µì‚¬ | ëª¨ë¸ ì˜ˆì‹œ | LlamaIndex í´ë˜ìŠ¤ |
|--------|----------|-------------------|
| **OpenAI** | GPT-4o, GPT-4o-mini, GPT-3.5-turbo | `OpenAI` |
| **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus/Haiku | `Anthropic` |
| **Google** | Gemini 2.5 Flash, Gemini 2.5 Pro | `Gemini` |
| **AWS Bedrock** | Claude, Llama 3.1, Titan | `Bedrock` |
| **Azure OpenAI** | GPT-4o (Azure) | `AzureOpenAI` |
| **Ollama** | Llama 3.1, Mistral, Qwen | `Ollama` |
| **Cohere** | Command R+, Command Light | `Cohere` |
| **Hugging Face** | ëª¨ë“  text-generation ëª¨ë¸ | `HuggingFaceLLM` |

### ì§€ì›í•˜ëŠ” ì„ë² ë”© ëª¨ë¸

| ì œê³µì‚¬ | ëª¨ë¸ | ì°¨ì› | LlamaIndex í´ë˜ìŠ¤ |
|--------|------|------|-------------------|
| **OpenAI** | text-embedding-3-small | 1536 | `OpenAIEmbedding` |
| **OpenAI** | text-embedding-3-large | 3072 | `OpenAIEmbedding` |
| **Cohere** | embed-multilingual-v3 | 1024 | `CohereEmbedding` |
| **Google** | text-embedding-004 | 768 | `GeminiEmbedding` |
| **AWS Bedrock** | amazon.titan-embed-text-v1 | 1024 | `BedrockEmbedding` |
| **Hugging Face** | BAAI/bge-large-en-v1.5 | 1024 | `HuggingFaceEmbedding` |
| **Ollama** | nomic-embed-text | 768 | `OllamaEmbedding` |
| **Voyage AI** | voyage-2 | 1024 | `VoyageEmbedding` |

### ì§€ì›í•˜ëŠ” ë²¡í„° ìŠ¤í† ì–´

| ë²¡í„° ìŠ¤í† ì–´ | íƒ€ì… | íŠ¹ì§• | LlamaIndex í´ë˜ìŠ¤ |
|------------|------|------|-------------------|
| **SimpleVectorStore** | ë¡œì»¬ (JSON) | ê°„ë‹¨, í”„ë¡œí† íƒ€ì…ìš© | `SimpleVectorStore` |
| **FAISS** | ë¡œì»¬ | ë¹ ë¥¸ ê²€ìƒ‰, ë©”ëª¨ë¦¬ ê¸°ë°˜ | `FaissVectorStore` |
| **Chroma** | ë¡œì»¬/í´ë¼ìš°ë“œ | ê²½ëŸ‰, ì„ë² ë”© ê´€ë¦¬ | `ChromaVectorStore` |
| **Pinecone** | í´ë¼ìš°ë“œ | ì™„ì „ ê´€ë¦¬í˜•, ê³ ì„±ëŠ¥ | `PineconeVectorStore` |
| **Weaviate** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | GraphQL, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ | `WeaviateVectorStore` |
| **Qdrant** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | Rust ê¸°ë°˜, ê³ ì„±ëŠ¥ | `QdrantVectorStore` |
| **Milvus** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ | `MilvusVectorStore` |
| **MongoDB Atlas** | í´ë¼ìš°ë“œ | í†µí•© DB, ë©”íƒ€ë°ì´í„° í•„í„° | `MongoDBAtlasVectorSearch` |
| **Supabase** | í´ë¼ìš°ë“œ | PostgreSQL + pgvector | `SupabaseVectorStore` |
| **Redis** | í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ… | ì¸ë©”ëª¨ë¦¬, ì´ˆê³ ì† | `RedisVectorStore` |

### ì§€ì›í•˜ëŠ” ì¸ë±ìŠ¤ íƒ€ì…

| ì¸ë±ìŠ¤ íƒ€ì… | ì„¤ëª… | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|------------|------|--------------|
| **VectorStoreIndex** | ë²¡í„° ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ | **ì¼ë°˜ì ì¸ RAG (ê°€ì¥ ë§ì´ ì‚¬ìš©)** |
| **SummaryIndex** | ëª¨ë“  ë…¸ë“œ ìˆœì°¨ ì²˜ë¦¬ | ë¬¸ì„œ ìš”ì•½, ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í•„ìš” ì‹œ |
| **TreeIndex** | ê³„ì¸µì  íŠ¸ë¦¬ êµ¬ì¡° | ëŒ€ìš©ëŸ‰ ë¬¸ì„œ, ë‹¨ê³„ë³„ ìš”ì•½ |
| **KeywordTableIndex** | í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ | ì •í™•í•œ ìš©ì–´ ë§¤ì¹­ í•„ìš” ì‹œ |
| **KnowledgeGraphIndex** | ê·¸ë˜í”„ êµ¬ì¡° | ì—”í‹°í‹° ê´€ê³„ íŒŒì•… |
| **DocumentSummaryIndex** | ë¬¸ì„œë³„ ìš”ì•½ | ë©€í‹° ë¬¸ì„œ ë¹„êµ |

### ì§€ì›í•˜ëŠ” ë°ì´í„° ë¡œë” (LlamaHub)

| ì¹´í…Œê³ ë¦¬ | ë¡œë” ì˜ˆì‹œ | ìš©ë„ |
|---------|----------|------|
| **íŒŒì¼** | PDFReader, DocxReader, CSVReader | ë¡œì»¬ ë¬¸ì„œ ë¡œë”© |
| **ì›¹** | SimpleWebPageReader, BeautifulSoupWebReader | ì›¹ í¬ë¡¤ë§ |
| **ë°ì´í„°ë² ì´ìŠ¤** | DatabaseReader, SQLDatabase | SQL/NoSQL DB |
| **API** | NotionReader, SlackReader, GoogleDocsReader | SaaS ë„êµ¬ |
| **í´ë¼ìš°ë“œ** | S3Reader, GCSReader, AzureBlobStorageReader | í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ |
| **ì½”ë“œ** | GithubRepositoryReader | GitHub ë ˆí¬ |

### ì˜ˆìƒ ë¹„ìš© (2025ë…„ ê¸°ì¤€)

#### ì†Œê·œëª¨ í”„ë¡œì íŠ¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸)
```
âœ… LlamaIndex í”„ë ˆì„ì›Œí¬
- LlamaIndex Core: ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
- LlamaIndex Cloud: ë¬´ë£Œ í‹°ì–´ (1,000 ì¿¼ë¦¬/ì›”)

âœ… ë²¡í„° ìŠ¤í† ì–´
- SimpleVectorStore (ë¡œì»¬): $0/ì›”
- Chroma (ë¡œì»¬): $0/ì›”
- Pinecone Starter: $0/ì›” (100,000 ë²¡í„°)

âœ… LLM API
- OpenAI GPT-4o-mini: ~$5-10/ì›” (ì›” 1,000 ì¿¼ë¦¬)
- Ollama (ë¡œì»¬): $0/ì›”

âœ… ì„ë² ë”© API
- OpenAI text-embedding-3-small: ~$0.20/ì›”
- Ollama (ë¡œì»¬): $0/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $0-10/ì›”
```

#### ì¤‘ê·œëª¨ í”„ë¡œì íŠ¸ (í”„ë¡œë•ì…˜)
```
âœ… LlamaIndex Cloud
- Pro Plan: $49/ì›” (50,000 ì¿¼ë¦¬)

âœ… ë²¡í„° ìŠ¤í† ì–´
- Pinecone Standard: $70/ì›” (1M ë²¡í„°)
- Qdrant Cloud: $50/ì›”
- Chroma Cloud: $29/ì›”

âœ… LLM API
- OpenAI GPT-4o: ~$100-200/ì›” (ì›” 10,000 ì¿¼ë¦¬)
- Anthropic Claude 3.5 Sonnet: ~$150/ì›”

âœ… ì„ë² ë”© API
- OpenAI embeddings: ~$2-5/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $200-400/ì›”
```

#### ëŒ€ê·œëª¨ ì—”í„°í”„ë¼ì´ì¦ˆ
```
âœ… LlamaIndex Cloud
- Enterprise: Custom pricing (1M+ ì¿¼ë¦¬)

âœ… ë²¡í„° ìŠ¤í† ì–´
- Pinecone Enterprise: $500-2,000/ì›”
- Weaviate Enterprise: Custom
- Milvus Enterprise: Custom

âœ… LLM API
- OpenAI GPT-4o: $1,000-5,000/ì›” (ëŒ€ëŸ‰ íŠ¸ë˜í”½)
- Azure OpenAI: Custom pricing

âœ… ì¸í”„ë¼
- AWS/GCP/Azure: $500-2,000/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $2,000-10,000+/ì›”
```

**ë¹„ìš© ìµœì í™” íŒ:**
- **SimpleVectorStore í™œìš©**: ì†Œê·œëª¨ í”„ë¡œì íŠ¸ëŠ” ë¡œì»¬ ì €ì¥ì†Œ ì‚¬ìš©
- **Ollama ì‚¬ìš©**: ë¡œì»¬ LLMìœ¼ë¡œ API ë¹„ìš© ì œë¡œí™”
- **ë°°ì¹˜ ì„ë² ë”©**: ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ë¡œ ë¹„ìš© ì ˆê°
- **ìºì‹±**: ë™ì¼ ì¿¼ë¦¬ ì¬ì‚¬ìš©ìœ¼ë¡œ API í˜¸ì¶œ ê°ì†Œ
- **ë¬´ë£Œ í‹°ì–´**: Pinecone, Chroma ë¬´ë£Œ í”Œëœ í™œìš©

---

## ğŸ— ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### LlamaIndex RAG ì›Œí¬í”Œë¡œìš°

```mermaid
graph TD
    User[ì‚¬ìš©ì ì§ˆë¬¸] --> QueryEngine[Query Engine]

    QueryEngine -->|1. ì¿¼ë¦¬ ë¶„ì„| Retriever[Retriever]
    Retriever -->|2. ê²€ìƒ‰| VectorStore[(Vector Store<br/>Pinecone/Chroma)]
    VectorStore -->|ê´€ë ¨ ë…¸ë“œ| Retriever

    Retriever -->|3. ë…¸ë“œ ë°˜í™˜| ResponseSynthesizer[Response Synthesizer]
    ResponseSynthesizer -->|4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±| LLM[LLM<br/>GPT-4o/Claude]
    LLM -->|5. ë‹µë³€ ìƒì„±| ResponseSynthesizer

    ResponseSynthesizer -->|ìµœì¢… ë‹µë³€| QueryEngine
    QueryEngine --> User

    style QueryEngine fill:#4A90E2,stroke:#357ABD,stroke-width:3px,color:#fff
    style VectorStore fill:#50C878,stroke:#3A9B5C,stroke-width:2px
```

### ë¬¸ì„œ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    Docs[ì›ë³¸ ë¬¸ì„œ<br/>PDF/DOCX/API] -->|1. Load| Reader[Data Reader<br/>PDFReader, etc.]

    Reader -->|Document ê°ì²´| Parser[Node Parser<br/>SentenceSplitter]

    Parser -->|Node ë¶„í• | Nodes[Text Nodes<br/>ì²­í¬ ë‹¨ìœ„]

    Nodes -->|2. Embed| Embedding[Embedding Model<br/>OpenAI/Cohere]

    Embedding -->|ë²¡í„° ìƒì„±| Index[3. Index<br/>VectorStoreIndex]

    Index -->|ì €ì¥ ì™„ë£Œ| Ready[ì¿¼ë¦¬ ì¤€ë¹„ ì™„ë£Œ]
```

### LlamaIndex í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì•„í‚¤í…ì²˜

```
                    LlamaIndex RAG ì‹œìŠ¤í…œ
                            |
        +-------------------+-------------------+
        |                   |                   |
    Documents            Indexes            Query Engine
        |                   |                   |
   +----+----+         +----+----+         +----+----+
   |         |         |         |         |         |
 Readers  Parsers   Vector   Summary   Retriever  Synthesizer
                    Store
        |                   |                   |
        +-------------------+-------------------+
                            |
              LlamaIndex Core Framework
                            |
        +-------------------+-------------------+
        |                   |                   |
     LLMs              Embeddings          Evaluators
   (OpenAI,          (OpenAI,              (Faithfulness,
  Anthropic)          Cohere)               Relevancy)
```

### ìƒì„¸ ì¿¼ë¦¬ ì²˜ë¦¬ íë¦„

```python
# LlamaIndex ì¿¼ë¦¬ ì²˜ë¦¬ êµ¬ì¡°

# 1. ì¸ë±ìŠ¤ ìƒì„±
index = VectorStoreIndex.from_documents(documents)

# 2. ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
query_engine = index.as_query_engine(
    similarity_top_k=5,           # ìƒìœ„ 5ê°œ ë…¸ë“œ ê²€ìƒ‰
    response_mode="compact"       # ì‘ë‹µ í•©ì„± ëª¨ë“œ
)

# 3. ì¿¼ë¦¬ ì‹¤í–‰
response = query_engine.query("ì§ˆë¬¸")

# ë‚´ë¶€ í”„ë¡œì„¸ìŠ¤:
# - ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
# - ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
# - ê´€ë ¨ ë…¸ë“œ ì¶”ì¶œ
# - í”„ë¡¬í”„íŠ¸ êµ¬ì„±
# - LLM í˜¸ì¶œ
# - ë‹µë³€ í•©ì„±
```

### í”„ë¡œë•ì…˜ ì•„í‚¤í…ì²˜

```
[í´ë¼ì´ì–¸íŠ¸ ê³„ì¸µ]
ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ (React/Vue) | ëª¨ë°”ì¼ ì•± | API í´ë¼ì´ì–¸íŠ¸
                    |
              HTTPS/REST API
                    |
                    v
[API ê³„ì¸µ]
FastAPI/Flask Server
â”œâ”€ ì¸ì¦/ì¸ê°€ (JWT)
â”œâ”€ Rate Limiting
â”œâ”€ Request Validation
â””â”€ LlamaIndex Query Engine
                    |
                    v
[LlamaIndex ê³„ì¸µ]
â”œâ”€ VectorStoreIndex
â”œâ”€ Query Engine (Retriever + Synthesizer)
â”œâ”€ Chat Engine (Memory)
â””â”€ Agent (Tools)
                    |
        +-----------+-----------+
        |           |           |
        v           v           v
   [Vector DB]   [LLM API]  [Embeddings]
   - Pinecone    - OpenAI    - OpenAI
   - Chroma      - Anthropic - Cohere
   - Qdrant      - Gemini    - Voyage AI
                    |
                    v
[ëª¨ë‹ˆí„°ë§ ê³„ì¸µ]
â”œâ”€ LlamaIndex Cloud (ì¿¼ë¦¬ ì¶”ì )
â”œâ”€ Prometheus/Grafana (ë©”íŠ¸ë¦­)
â”œâ”€ LangFuse (LLM ì˜µì €ë²„ë¹Œë¦¬í‹°)
â””â”€ Sentry (ì—ëŸ¬ ì¶”ì )
```

---

## ğŸš€ í™˜ê²½ êµ¬ì¶•

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **Python**: 3.8 ì´ìƒ (3.10+ ê¶Œì¥)
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 4GB RAM (8GB+ ê¶Œì¥)
- **ìš´ì˜ì²´ì œ**: Windows 10+, macOS 11+, Ubuntu 20.04+
- **íŒ¨í‚¤ì§€ ê´€ë¦¬ì**: pip ë˜ëŠ” poetry

### 1. LlamaIndex ì„¤ì¹˜

#### í•µì‹¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ê¸°ë³¸ LlamaIndex ì„¤ì¹˜
pip install llama-index

# ë˜ëŠ” ê°œë³„ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install llama-index-core           # í•µì‹¬ ê¸°ëŠ¥
pip install llama-index-llms-openai    # OpenAI LLM
pip install llama-index-embeddings-openai  # OpenAI Embeddings
```

#### LLM í†µí•© íŒ¨í‚¤ì§€

```bash
# OpenAI
pip install llama-index-llms-openai

# Anthropic (Claude)
pip install llama-index-llms-anthropic

# Google (Gemini)
pip install llama-index-llms-gemini

# AWS Bedrock
pip install llama-index-llms-bedrock

# Ollama (ë¡œì»¬)
pip install llama-index-llms-ollama

# Cohere
pip install llama-index-llms-cohere
```

#### ë²¡í„° ìŠ¤í† ì–´ íŒ¨í‚¤ì§€

```bash
# Chroma
pip install llama-index-vector-stores-chroma chromadb

# Pinecone
pip install llama-index-vector-stores-pinecone pinecone-client

# Qdrant
pip install llama-index-vector-stores-qdrant qdrant-client

# Weaviate
pip install llama-index-vector-stores-weaviate weaviate-client

# MongoDB Atlas
pip install llama-index-vector-stores-mongodb pymongo

# Supabase
pip install llama-index-vector-stores-supabase supabase
```

#### ë°ì´í„° ë¦¬ë” íŒ¨í‚¤ì§€

```bash
# PDF ë¦¬ë”
pip install llama-index-readers-file pypdf

# ì›¹ ë¦¬ë”
pip install llama-index-readers-web beautifulsoup4

# ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ë”
pip install llama-index-readers-database sqlalchemy

# Notion ë¦¬ë”
pip install llama-index-readers-notion

# GitHub ë¦¬ë”
pip install llama-index-readers-github
```

#### ì „ì²´ requirements.txt

```txt
# LlamaIndex í•µì‹¬
llama-index==0.10.0
llama-index-core==0.10.0

# LLM í†µí•©
llama-index-llms-openai==0.1.5
llama-index-llms-anthropic==0.1.4
llama-index-llms-gemini==0.1.3

# ì„ë² ë”©
llama-index-embeddings-openai==0.1.5
llama-index-embeddings-cohere==0.1.2

# ë²¡í„° ìŠ¤í† ì–´
llama-index-vector-stores-chroma==0.1.4
llama-index-vector-stores-pinecone==0.1.3
chromadb==0.4.22
pinecone-client==3.0.0

# ë¦¬ë”
llama-index-readers-file==0.1.5
llama-index-readers-web==0.1.4
pypdf==3.17.4
beautifulsoup4==4.12.2

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.0
tiktoken==0.5.2
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ ìƒì„±:

```env
# LLM API í‚¤
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
GOOGLE_API_KEY=your-google-api-key
COHERE_API_KEY=your-cohere-key

# ë²¡í„° ìŠ¤í† ì–´
PINECONE_API_KEY=your-pinecone-key
PINECONE_ENVIRONMENT=us-east-1-aws

# Qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your-qdrant-key

# LlamaIndex Cloud (ì„ íƒ)
LLAMA_CLOUD_API_KEY=your-llamacloud-key

# Ollama (ë¡œì»¬ LLM, ì„ íƒ)
OLLAMA_BASE_URL=http://localhost:11434
```

### 3. ì„¤ì¹˜ í™•ì¸

```python
# test_setup.py
import os
from dotenv import load_dotenv

load_dotenv()

def test_imports():
    """íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸"""
    print("1ï¸âƒ£ íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸...\n")

    try:
        import llama_index
        print(f"âœ… LlamaIndex ë²„ì „: {llama_index.__version__}")

        from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
        print("âœ… LlamaIndex Core")

        from llama_index.llms.openai import OpenAI
        print("âœ… OpenAI LLM")

        from llama_index.embeddings.openai import OpenAIEmbedding
        print("âœ… OpenAI Embeddings")

        from llama_index.vector_stores.chroma import ChromaVectorStore
        print("âœ… Chroma Vector Store")

        return True
    except ImportError as e:
        print(f"âŒ Import ì‹¤íŒ¨: {e}")
        return False

def test_openai_connection():
    """OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\n2ï¸âƒ£ OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸...\n")

    try:
        from llama_index.llms.openai import OpenAI
        from llama_index.embeddings.openai import OpenAIEmbedding

        # LLM í…ŒìŠ¤íŠ¸
        llm = OpenAI(model="gpt-4o-mini", temperature=0)
        response = llm.complete("ì•ˆë…•í•˜ì„¸ìš”!")
        print(f"âœ… LLM ì‘ë‹µ: {response.text[:50]}...")

        # ì„ë² ë”© í…ŒìŠ¤íŠ¸
        embed_model = OpenAIEmbedding(model="text-embedding-3-small")
        embedding = embed_model.get_text_embedding("í…ŒìŠ¤íŠ¸")
        print(f"âœ… ì„ë² ë”© ì°¨ì›: {len(embedding)}")

        return True
    except Exception as e:
        print(f"âŒ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def test_index_creation():
    """ì¸ë±ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n3ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸...\n")

    try:
        from llama_index.core import VectorStoreIndex, Document
        from llama_index.embeddings.openai import OpenAIEmbedding

        # ìƒ˜í”Œ ë¬¸ì„œ
        documents = [
            Document(text="LlamaIndexëŠ” RAG ì „ìš© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
            Document(text="VectorStoreIndexëŠ” ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤."),
        ]

        # ì¸ë±ìŠ¤ ìƒì„±
        index = VectorStoreIndex.from_documents(
            documents,
            embed_model=OpenAIEmbedding(model="text-embedding-3-small")
        )
        print("âœ… VectorStoreIndex ìƒì„± ì„±ê³µ")

        # ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        query_engine = index.as_query_engine()
        response = query_engine.query("LlamaIndexë€?")
        print(f"âœ… ì¿¼ë¦¬ ì‘ë‹µ: {response.response[:100]}...")

        return True
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def main():
    print("=" * 60)
    print("  LlamaIndex RAG í™˜ê²½ ì„¤ì • í™•ì¸")
    print("=" * 60 + "\n")

    imports_ok = test_imports()
    openai_ok = test_openai_connection()
    index_ok = test_index_creation()

    print("\n" + "=" * 60)
    if imports_ok and openai_ok and index_ok:
        print("ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("   ì´ì œ LlamaIndex RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸  ì¼ë¶€ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ìœ„ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.")
    print("=" * 60)

if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
python test_setup.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
============================================================
  LlamaIndex RAG í™˜ê²½ ì„¤ì • í™•ì¸
============================================================

1ï¸âƒ£ íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸...

âœ… LlamaIndex ë²„ì „: 0.10.0
âœ… LlamaIndex Core
âœ… OpenAI LLM
âœ… OpenAI Embeddings
âœ… Chroma Vector Store

2ï¸âƒ£ OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸...

âœ… LLM ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?...
âœ… ì„ë² ë”© ì°¨ì›: 1536

3ï¸âƒ£ ì¸ë±ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸...

âœ… VectorStoreIndex ìƒì„± ì„±ê³µ
âœ… ì¿¼ë¦¬ ì‘ë‹µ: LlamaIndexëŠ” LLMê³¼ ì™¸ë¶€ ë°ì´í„°ë¥¼ ì—°ê²°í•˜ëŠ” RAG ì „ìš© í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤...

============================================================
ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
   ì´ì œ LlamaIndex RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.
============================================================
```

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ RAG ì‹œìŠ¤í…œ (3ë¶„ ë§Œì— êµ¬í˜„)

```python
# quick_start.py
import os
from dotenv import load_dotenv

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 1. ì „ì—­ ì„¤ì • (í•œ ë²ˆë§Œ)
Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# 2. ë¬¸ì„œ ë¡œë”©
print("ğŸ“‚ ë¬¸ì„œ ë¡œë”© ì¤‘...")
documents = SimpleDirectoryReader("./data").load_data()
print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n")

# 3. ì¸ë±ìŠ¤ ìƒì„± (ìë™ìœ¼ë¡œ ì²­í‚¹, ì„ë² ë”©, ì €ì¥)
print("ğŸ”„ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
index = VectorStoreIndex.from_documents(documents)
print("âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\n")

# 4. ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
query_engine = index.as_query_engine(
    similarity_top_k=3,      # ìƒìœ„ 3ê°œ ë…¸ë“œ ê²€ìƒ‰
    response_mode="compact"  # ê°„ê²°í•œ ì‘ë‹µ
)

# 5. ì§ˆë¬¸í•˜ê¸°
print("=" * 70)
print("  LlamaIndex RAG ë¹ ë¥¸ ì‹œì‘")
print("=" * 70 + "\n")

question = "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
print(f"â“ ì§ˆë¬¸: {question}\n")

response = query_engine.query(question)

print(f"ğŸ’¡ ë‹µë³€:\n{response.response}\n")
print(f"ğŸ“š ì°¸ê³ í•œ ë…¸ë“œ: {len(response.source_nodes)}ê°œ\n")

# ì¶œì²˜ í‘œì‹œ
for i, node in enumerate(response.source_nodes, 1):
    print(f"  {i}. {node.metadata.get('file_name', 'Unknown')}")
    print(f"     ìœ ì‚¬ë„: {node.score:.4f}")
    print(f"     ë‚´ìš©: {node.text[:100]}...\n")
```

**ë””ë ‰í† ë¦¬ êµ¬ì¡°:**
```
project/
â”œâ”€â”€ quick_start.py
â”œâ”€â”€ data/                # ë¬¸ì„œ ì €ì¥
â”‚   â”œâ”€â”€ guide.txt
â”‚   â”œâ”€â”€ manual.pdf
â”‚   â””â”€â”€ faq.md
â””â”€â”€ .env
```

**ì‹¤í–‰:**
```bash
python quick_start.py
```

### ë©”ëª¨ë¦¬ ì €ì¥ ë° ì¬ì‚¬ìš©

```python
# quick_start_persist.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# ì„¤ì •
Settings.llm = OpenAI(model="gpt-4o-mini")
Settings.embed_model = OpenAIEmbedding()

PERSIST_DIR = "./storage"

# ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ë¡œë“œ
import os
if not os.path.exists(PERSIST_DIR):
    # ì²˜ìŒ ì‹¤í–‰: ì¸ë±ìŠ¤ ìƒì„±
    documents = SimpleDirectoryReader("./data").load_data()
    index = VectorStoreIndex.from_documents(documents)

    # ì €ì¥
    index.storage_context.persist(persist_dir=PERSIST_DIR)
    print("âœ… ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
else:
    # ì´ë¯¸ ì¡´ì¬: ì¸ë±ìŠ¤ ë¡œë“œ (ë¹ ë¦„)
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context)
    print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")

# ì¿¼ë¦¬
query_engine = index.as_query_engine()
response = query_engine.query("LlamaIndexë€?")
print(f"\në‹µë³€: {response.response}")
```

### Chat Engine (ëŒ€í™”í˜• RAG)

```python
# quick_start_chat.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai import OpenAI

Settings.llm = OpenAI(model="gpt-4o-mini")

# ì¸ë±ìŠ¤ ìƒì„±
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)

# Chat Engine ìƒì„± (ëŒ€í™” ê¸°ë¡ ìœ ì§€)
chat_engine = index.as_chat_engine(
    chat_mode="condense_question",  # ëŒ€í™” ë§¥ë½ ìœ ì§€
    verbose=True
)

# ëŒ€í™”
print("ğŸ’¬ Chat Engine (ì¢…ë£Œ: 'quit')\n")

while True:
    user_input = input("You: ").strip()
    if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
        break

    response = chat_engine.chat(user_input)
    print(f"Bot: {response.response}\n")
```

---

## ğŸ“š ìƒì„¸ ê°€ì´ë“œ

### 1. Document Readers: ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ë¡œë”©

#### 1.1 ë¡œì»¬ íŒŒì¼ ë¡œë”©

```python
from llama_index.core import SimpleDirectoryReader

# ê¸°ë³¸ ì‚¬ìš©
reader = SimpleDirectoryReader("./documents")
documents = reader.load_data()
print(f"ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ")

# íŠ¹ì • í™•ì¥ìë§Œ
reader = SimpleDirectoryReader(
    input_dir="./documents",
    required_exts=[".pdf", ".docx"]
)
documents = reader.load_data()

# ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨
reader = SimpleDirectoryReader(
    input_dir="./documents",
    recursive=True
)
documents = reader.load_data()

# íŒŒì¼ë³„ ë©”íƒ€ë°ì´í„° í™•ì¸
for doc in documents:
    print(f"íŒŒì¼: {doc.metadata['file_name']}")
    print(f"í¬ê¸°: {doc.metadata['file_size']} bytes")
    print(f"ë‚´ìš© ê¸¸ì´: {len(doc.text)} ê¸€ì\n")
```

#### 1.2 PDF ë¬¸ì„œ ë¡œë”©

```python
from llama_index.readers.file import PDFReader

# PDF ë¦¬ë”
reader = PDFReader()
documents = reader.load_data(file="./document.pdf")

# í˜ì´ì§€ë³„ ì²˜ë¦¬
for i, doc in enumerate(documents):
    print(f"í˜ì´ì§€ {i+1}: {doc.text[:100]}...")
```

#### 1.3 ì›¹ í˜ì´ì§€ ë¡œë”©

```python
from llama_index.readers.web import SimpleWebPageReader

# ë‹¨ì¼ URL
reader = SimpleWebPageReader()
documents = reader.load_data(urls=["https://example.com/docs"])

# ì—¬ëŸ¬ URL
urls = [
    "https://docs.llamaindex.ai/en/stable/",
    "https://docs.llamaindex.ai/en/stable/getting_started/",
]
documents = reader.load_data(urls=urls)
```

#### 1.4 ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©

```python
from llama_index.readers.database import DatabaseReader

# SQL ë°ì´í„°ë² ì´ìŠ¤
reader = DatabaseReader(
    sql_database="sqlite:///company.db"
)

# SQL ì¿¼ë¦¬ë¡œ ë¡œë”©
documents = reader.load_data(
    query="SELECT * FROM products WHERE category='AI'"
)
```

#### 1.5 Notion í˜ì´ì§€ ë¡œë”©

```python
from llama_index.readers.notion import NotionPageReader

# Notion API í‚¤ í•„ìš”
reader = NotionPageReader(integration_token="your-notion-token")

# íŠ¹ì • í˜ì´ì§€ ID
page_ids = ["page-id-1", "page-id-2"]
documents = reader.load_data(page_ids=page_ids)
```

#### 1.6 GitHub ë ˆí¬ì§€í† ë¦¬ ë¡œë”©

```python
from llama_index.readers.github import GithubRepositoryReader, GithubClient

# GitHub í´ë¼ì´ì–¸íŠ¸
github_client = GithubClient(github_token="your-github-token")

# ë ˆí¬ì§€í† ë¦¬ ë¡œë”©
reader = GithubRepositoryReader(
    github_client=github_client,
    owner="owner-name",
    repo="repo-name",
    use_parser=True,
    verbose=True
)

documents = reader.load_data(branch="main")
```

### 2. Node Parsing: ë¬¸ì„œ ì²­í‚¹

#### 2.1 SentenceSplitter (ê¸°ë³¸ ê¶Œì¥)

```python
from llama_index.core.node_parser import SentenceSplitter

# ê¸°ë³¸ ì„¤ì •
parser = SentenceSplitter(
    chunk_size=1024,        # ì²­í¬ í¬ê¸° (ê¸€ì)
    chunk_overlap=200,      # ì˜¤ë²„ë©
    paragraph_separator="\n\n",
    secondary_chunking_regex="[^,.;ã€‚]+[,.;ã€‚]?"
)

# ë¬¸ì„œë¥¼ ë…¸ë“œë¡œ ë¶„í• 
nodes = parser.get_nodes_from_documents(documents)
print(f"ìƒì„±ëœ ë…¸ë“œ: {len(nodes)}ê°œ")

# ë…¸ë“œ ì •ë³´ í™•ì¸
for node in nodes[:3]:
    print(f"í…ìŠ¤íŠ¸: {node.text[:100]}...")
    print(f"ë©”íƒ€ë°ì´í„°: {node.metadata}\n")
```

#### 2.2 TokenTextSplitter (í† í° ê¸°ë°˜)

```python
from llama_index.core.node_parser import TokenTextSplitter

# OpenAI í† í° ê¸°ì¤€
parser = TokenTextSplitter(
    chunk_size=512,         # 512 í† í°
    chunk_overlap=50,
    separator=" "
)

nodes = parser.get_nodes_from_documents(documents)
```

#### 2.3 SemanticSplitter (ì˜ë¯¸ ê¸°ë°˜)

```python
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding

# ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„í• 
embed_model = OpenAIEmbedding()
parser = SemanticSplitterNodeParser(
    buffer_size=1,
    breakpoint_percentile_threshold=95,
    embed_model=embed_model
)

nodes = parser.get_nodes_from_documents(documents)
```

#### 2.4 HierarchicalNodeParser (ê³„ì¸µì )

```python
from llama_index.core.node_parser import HierarchicalNodeParser

# ì—¬ëŸ¬ í¬ê¸°ì˜ ì²­í¬ ìƒì„±
parser = HierarchicalNodeParser.from_defaults(
    chunk_sizes=[2048, 512, 128]  # í° ì²­í¬ â†’ ì‘ì€ ì²­í¬
)

nodes = parser.get_nodes_from_documents(documents)
```

### 3. Indexes: ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ íƒ€ì…

#### 3.1 VectorStoreIndex (ê°€ì¥ ì¼ë°˜ì )

```python
from llama_index.core import VectorStoreIndex, Document

# ë¬¸ì„œ ì¤€ë¹„
documents = [
    Document(text="LlamaIndexëŠ” RAG í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
    Document(text="VectorStoreIndexëŠ” ë²¡í„° ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤."),
]

# ì¸ë±ìŠ¤ ìƒì„±
index = VectorStoreIndex.from_documents(documents)

# ì¿¼ë¦¬
query_engine = index.as_query_engine(similarity_top_k=2)
response = query_engine.query("LlamaIndexë€?")
print(response.response)
```

#### 3.2 SummaryIndex (ìˆœì°¨ ì²˜ë¦¬)

```python
from llama_index.core import SummaryIndex

# ëª¨ë“  ë…¸ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ LLMì— ì „ë‹¬
index = SummaryIndex.from_documents(documents)

# ìš”ì•½ ì¿¼ë¦¬
query_engine = index.as_query_engine(response_mode="tree_summarize")
response = query_engine.query("ë¬¸ì„œ ì „ì²´ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”")
print(response.response)
```

#### 3.3 TreeIndex (ê³„ì¸µì  ìš”ì•½)

```python
from llama_index.core import TreeIndex

# íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ê³„ì¸µì  ìš”ì•½
index = TreeIndex.from_documents(documents)

# ì¿¼ë¦¬
query_engine = index.as_query_engine()
response = query_engine.query("ì£¼ìš” ë‚´ìš©ì€?")
print(response.response)
```

#### 3.4 KeywordTableIndex (í‚¤ì›Œë“œ ê¸°ë°˜)

```python
from llama_index.core import KeywordTableIndex

# í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¸ë±ì‹±
index = KeywordTableIndex.from_documents(documents)

# í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
query_engine = index.as_query_engine()
response = query_engine.query("RAG í”„ë ˆì„ì›Œí¬")
print(response.response)
```

#### 3.5 KnowledgeGraphIndex (ê·¸ë˜í”„)

```python
from llama_index.core import KnowledgeGraphIndex
from llama_index.core.graph_stores import SimpleGraphStore

# ì§€ì‹ ê·¸ë˜í”„ ìƒì„±
graph_store = SimpleGraphStore()
index = KnowledgeGraphIndex.from_documents(
    documents,
    storage_context=StorageContext.from_defaults(graph_store=graph_store),
    max_triplets_per_chunk=2
)

# ê·¸ë˜í”„ ì¿¼ë¦¬
query_engine = index.as_query_engine()
response = query_engine.query("LlamaIndexì™€ ê´€ë ¨ëœ ì—”í‹°í‹°ëŠ”?")
print(response.response)
```

### 4. Query Engines: ê³ ê¸‰ ì¿¼ë¦¬ ì²˜ë¦¬

#### 4.1 ê¸°ë³¸ Query Engine

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(documents)

# ê¸°ë³¸ ì¿¼ë¦¬ ì—”ì§„
query_engine = index.as_query_engine(
    similarity_top_k=5,             # ìƒìœ„ 5ê°œ ë…¸ë“œ
    response_mode="compact"         # ì‘ë‹µ ëª¨ë“œ
)

response = query_engine.query("ì§ˆë¬¸")
```

**Response Modes:**
```python
# compact: í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ í•œ ë²ˆì— LLM í˜¸ì¶œ (ê¸°ë³¸)
# refine: ë°˜ë³µì ìœ¼ë¡œ ë‹µë³€ ê°œì„ 
# tree_summarize: íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ìš”ì•½
# simple_summarize: ê°„ë‹¨í•œ ìš”ì•½
# no_text: í…ìŠ¤íŠ¸ ì—†ì´ ë©”íƒ€ë°ì´í„°ë§Œ
# accumulate: ê° ì²­í¬ë³„ë¡œ ë³„ë„ ë‹µë³€ ìƒì„±
```

#### 4.2 Sub Question Query Engine (ë³µì¡í•œ ì§ˆë¬¸ ë¶„í•´)

```python
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.tools import QueryEngineTool, ToolMetadata

# ì—¬ëŸ¬ ì¸ë±ìŠ¤ (ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤)
index1 = VectorStoreIndex.from_documents(docs_sales)
index2 = VectorStoreIndex.from_documents(docs_marketing)

# ë„êµ¬ ì •ì˜
query_engine_tools = [
    QueryEngineTool(
        query_engine=index1.as_query_engine(),
        metadata=ToolMetadata(
            name="sales_docs",
            description="ì˜ì—… ê´€ë ¨ ë¬¸ì„œ (ë§¤ì¶œ, ê³„ì•½ ë“±)"
        )
    ),
    QueryEngineTool(
        query_engine=index2.as_query_engine(),
        metadata=ToolMetadata(
            name="marketing_docs",
            description="ë§ˆì¼€íŒ… ê´€ë ¨ ë¬¸ì„œ (ìº í˜ì¸, ê´‘ê³  ë“±)"
        )
    )
]

# Sub Question Query Engine
query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools
)

# ë³µì¡í•œ ì§ˆë¬¸ â†’ ìë™ìœ¼ë¡œ í•˜ìœ„ ì§ˆë¬¸ ìƒì„±
response = query_engine.query(
    "2024ë…„ ì˜ì—… ì‹¤ì ê³¼ ë§ˆì¼€íŒ… ìº í˜ì¸ íš¨ê³¼ë¥¼ ë¹„êµí•´ì£¼ì„¸ìš”"
)
print(response.response)
```

#### 4.3 Router Query Engine (ì¿¼ë¦¬ ë¼ìš°íŒ…)

```python
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector

# ì—¬ëŸ¬ ì¿¼ë¦¬ ì—”ì§„
summary_query_engine = summary_index.as_query_engine(response_mode="tree_summarize")
vector_query_engine = vector_index.as_query_engine()

# ë¼ìš°í„°
query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[
        QueryEngineTool(
            query_engine=summary_query_engine,
            metadata=ToolMetadata(
                name="summary",
                description="ì „ì²´ ë¬¸ì„œ ìš”ì•½ì— ì‚¬ìš©"
            )
        ),
        QueryEngineTool(
            query_engine=vector_query_engine,
            metadata=ToolMetadata(
                name="vector_search",
                description="íŠ¹ì • ì •ë³´ ê²€ìƒ‰ì— ì‚¬ìš©"
            )
        )
    ]
)

# ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìë™ ë¼ìš°íŒ…
response = query_engine.query("ë¬¸ì„œ ì „ì²´ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”")
```

#### 4.4 RetrieverQueryEngine (ì»¤ìŠ¤í…€ Retriever)

```python
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever

# Retriever ìƒì„±
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=10,
    vector_store_query_mode="default"
)

# Query Engine
query_engine = RetrieverQueryEngine.from_args(
    retriever=retriever,
    response_mode="compact"
)

response = query_engine.query("ì§ˆë¬¸")
```

### 5. Retrievers: ê³ ê¸‰ ê²€ìƒ‰ ì „ëµ

#### 5.1 Vector Retriever

```python
from llama_index.core.retrievers import VectorIndexRetriever

retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=5
)

# ê²€ìƒ‰
nodes = retriever.retrieve("LlamaIndex")
for node in nodes:
    print(f"ì ìˆ˜: {node.score:.4f}")
    print(f"í…ìŠ¤íŠ¸: {node.text[:100]}...\n")
```

#### 5.2 Keyword Table Retriever

```python
from llama_index.core.retrievers import KeywordTableSimpleRetriever

retriever = KeywordTableSimpleRetriever(
    index=keyword_index
)

nodes = retriever.retrieve("RAG í”„ë ˆì„ì›Œí¬")
```

#### 5.3 BM25 Retriever (í‚¤ì›Œë“œ ê²€ìƒ‰)

```python
from llama_index.core.retrievers import BM25Retriever

# ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ í•„ìš”
retriever = BM25Retriever.from_defaults(
    nodes=nodes,
    similarity_top_k=5
)

results = retriever.retrieve("LlamaIndex")
```

#### 5.4 Hybrid Retriever (ë²¡í„° + í‚¤ì›Œë“œ)

```python
from llama_index.core.retrievers import QueryFusionRetriever

# ì—¬ëŸ¬ retriever ê²°í•©
vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5)
bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=5)

# Fusion Retriever
retriever = QueryFusionRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    similarity_top_k=5,
    num_queries=1,  # ì¿¼ë¦¬ ë³€í˜• ìˆ˜
    mode="reciprocal_rerank"  # ì¬ìˆœìœ„í™” ë°©ì‹
)

results = retriever.retrieve("LlamaIndex RAG")
```

#### 5.5 Auto Merging Retriever (ìë™ ë³‘í•©)

```python
from llama_index.core.retrievers import AutoMergingRetriever
from llama_index.core.node_parser import HierarchicalNodeParser

# ê³„ì¸µì  ë…¸ë“œ íŒŒì„œ
node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[2048, 512, 128])
nodes = node_parser.get_nodes_from_documents(documents)

# ì¸ë±ìŠ¤
index = VectorStoreIndex(nodes)

# Auto Merging Retriever
retriever = AutoMergingRetriever(
    vector_retriever=index.as_retriever(similarity_top_k=12),
    storage_context=index.storage_context,
    simple_ratio_thresh=0.5
)

results = retriever.retrieve("ì§ˆë¬¸")
```

---

## ğŸ”— ë²¡í„° ìŠ¤í† ì–´ í†µí•©

### SimpleVectorStore (ë¡œì»¬, í”„ë¡œí† íƒ€ì…)

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.core.vector_stores import SimpleVectorStore

# ë¬¸ì„œ ë¡œë”©
documents = SimpleDirectoryReader("./data").load_data()

# SimpleVectorStore (JSON íŒŒì¼ë¡œ ì €ì¥)
vector_store = SimpleVectorStore()
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# ì¸ë±ìŠ¤ ìƒì„±
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)

# ì €ì¥
index.storage_context.persist(persist_dir="./storage")

# ë¡œë“œ
from llama_index.core import load_index_from_storage
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

### Chroma (ë¡œì»¬/í´ë¼ìš°ë“œ)

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# Chroma í´ë¼ì´ì–¸íŠ¸
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("my_collection")

# Vector Store
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# ì¸ë±ìŠ¤
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)

# ì¿¼ë¦¬
query_engine = index.as_query_engine()
response = query_engine.query("LlamaIndexë€?")
print(response.response)

# ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
index = VectorStoreIndex.from_vector_store(vector_store)
```

### Pinecone (í´ë¼ìš°ë“œ, í”„ë¡œë•ì…˜)

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
import os

# Pinecone ì´ˆê¸°í™”
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

# ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒ í•œ ë²ˆ)
index_name = "llamaindex-demo"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # OpenAI embedding ì°¨ì›
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

# Pinecone ì¸ë±ìŠ¤
pinecone_index = pc.Index(index_name)

# Vector Store
vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# LlamaIndex ì¸ë±ìŠ¤
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)

# ì¿¼ë¦¬
query_engine = index.as_query_engine(similarity_top_k=5)
response = query_engine.query("LlamaIndexì˜ ì£¼ìš” ê¸°ëŠ¥ì€?")
print(response.response)
```

### Qdrant (í´ë¼ìš°ë“œ/ìì²´í˜¸ìŠ¤íŒ…)

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
import qdrant_client

# Qdrant í´ë¼ì´ì–¸íŠ¸
client = qdrant_client.QdrantClient(
    url="http://localhost:6333",
    # api_key="your-api-key"  # Qdrant Cloud ì‚¬ìš© ì‹œ
)

# Vector Store
vector_store = QdrantVectorStore(
    client=client,
    collection_name="my_documents"
)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# ì¸ë±ìŠ¤
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)

# ì¿¼ë¦¬
query_engine = index.as_query_engine()
response = query_engine.query("ë¬¸ì„œ ë‚´ìš© ìš”ì•½")
print(response.response)
```

---

## ğŸš€ ê³ ê¸‰ RAG íŒ¨í„´

### 1. Citation Query Engine (ì¶œì²˜ ëª…ì‹œ)

```python
from llama_index.core.query_engine import CitationQueryEngine

# Citation Query Engine
query_engine = CitationQueryEngine.from_args(
    index=index,
    similarity_top_k=3,
    citation_chunk_size=512
)

response = query_engine.query("LlamaIndexì˜ ì£¼ìš” ê¸°ëŠ¥ì€?")

# ë‹µë³€ê³¼ ì¶œì²˜
print(f"ë‹µë³€: {response.response}\n")
print("ì¶œì²˜:")
for i, node in enumerate(response.source_nodes, 1):
    print(f"  [{i}] {node.text[:100]}...")
```

### 2. Multi-Document Agents (ì—¬ëŸ¬ ë¬¸ì„œ ì²˜ë¦¬)

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool

# ë¬¸ì„œë³„ ì¸ë±ìŠ¤
sales_index = VectorStoreIndex.from_documents(sales_docs)
marketing_index = VectorStoreIndex.from_documents(marketing_docs)

# ë„êµ¬ ì •ì˜
query_engine_tools = [
    QueryEngineTool(
        query_engine=sales_index.as_query_engine(),
        metadata=ToolMetadata(
            name="sales_data",
            description="2024ë…„ ì˜ì—… ë°ì´í„° ë° ì‹¤ì "
        )
    ),
    QueryEngineTool(
        query_engine=marketing_index.as_query_engine(),
        metadata=ToolMetadata(
            name="marketing_data",
            description="2024ë…„ ë§ˆì¼€íŒ… ìº í˜ì¸ ë°ì´í„°"
        )
    )
]

# ReAct Agent
agent = ReActAgent.from_tools(
    query_engine_tools,
    verbose=True
)

# ë³µì¡í•œ ì§ˆë¬¸
response = agent.chat(
    "2024ë…„ Q3 ì˜ì—… ì‹¤ì ê³¼ ë§ˆì¼€íŒ… ìº í˜ì¸ ROIë¥¼ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”"
)
print(response.response)
```

### 3. Recursive Retrieval (ì¬ê·€ ê²€ìƒ‰)

```python
from llama_index.core.retrievers import RecursiveRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.node_parser import SentenceSplitter

# ë¬¸ì„œ ìš”ì•½ ë…¸ë“œ ìƒì„±
from llama_index.core.schema import IndexNode

# ê° ë¬¸ì„œë¥¼ ìš”ì•½
doc_summaries = []
for doc in documents:
    summary = llm.complete(f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ì„¸ìš”:\n\n{doc.text}")

    # ìš”ì•½ ë…¸ë“œ ìƒì„±
    summary_node = IndexNode(
        text=summary.text,
        index_id=doc.doc_id
    )
    doc_summaries.append(summary_node)

# ìš”ì•½ ì¸ë±ìŠ¤
summary_index = VectorStoreIndex(doc_summaries)

# ì›ë³¸ ë¬¸ì„œ ì¸ë±ìŠ¤
doc_index = VectorStoreIndex.from_documents(documents)

# Recursive Retriever
retriever = RecursiveRetriever(
    root_id="root",
    retriever_dict={
        "root": summary_index.as_retriever(similarity_top_k=3),
        **{doc.doc_id: doc_index.as_retriever() for doc in documents}
    }
)

query_engine = RetrieverQueryEngine.from_args(retriever)
response = query_engine.query("ì£¼ìš” ë‚´ìš©ì€?")
```

### 4. Sentence Window Retrieval (ë¬¸ì¥ ìœˆë„ìš°)

```python
from llama_index.core.node_parser import SentenceWindowNodeParser
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# Sentence Window Parser
node_parser = SentenceWindowNodeParser.from_defaults(
    window_size=3,          # ì•ë’¤ 3ë¬¸ì¥ í¬í•¨
    window_metadata_key="window",
    original_text_metadata_key="original_text"
)

# ë…¸ë“œ ìƒì„±
nodes = node_parser.get_nodes_from_documents(documents)

# ì¸ë±ìŠ¤
index = VectorStoreIndex(nodes)

# Post-processor (ê²€ìƒ‰ í›„ ìœˆë„ìš° í™•ì¥)
postprocessor = MetadataReplacementPostProcessor(
    target_metadata_key="window"
)

# ì¿¼ë¦¬ ì—”ì§„
query_engine = index.as_query_engine(
    similarity_top_k=2,
    node_postprocessors=[postprocessor]
)

response = query_engine.query("LlamaIndexë€?")
```

### 5. Query Transformations (ì¿¼ë¦¬ ë³€í™˜)

```python
from llama_index.core.indices.query.query_transform import HyDEQueryTransform
from llama_index.core.query_engine import TransformQueryEngine

# HyDE (Hypothetical Document Embeddings)
hyde = HyDEQueryTransform(include_original=True)

# ê¸°ë³¸ ì¿¼ë¦¬ ì—”ì§„
base_query_engine = index.as_query_engine()

# Transform Query Engine
query_engine = TransformQueryEngine(
    base_query_engine,
    query_transform=hyde
)

# ì¿¼ë¦¬ ì‹œ ê°€ìƒ ë‹µë³€ ìƒì„± í›„ ê²€ìƒ‰
response = query_engine.query("LlamaIndexì˜ ì¥ì ì€?")
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ ì„ë² ë”©

```python
from llama_index.embeddings.openai import OpenAIEmbedding

# ë°°ì¹˜ í¬ê¸° ì„¤ì •
embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    embed_batch_size=100  # 100ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
)

# Settingsì— ì ìš©
from llama_index.core import Settings
Settings.embed_model = embed_model
```

### 2. ì²­í¬ í¬ê¸° ìµœì í™”

```python
from llama_index.core.node_parser import SentenceSplitter

# ì‹¤í—˜ì„ í†µí•´ ìµœì  í¬ê¸° ì°¾ê¸°
chunk_sizes = [256, 512, 1024, 2048]

for chunk_size in chunk_sizes:
    parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=50)
    nodes = parser.get_nodes_from_documents(documents)

    index = VectorStoreIndex(nodes)
    query_engine = index.as_query_engine()

    # í‰ê°€ (í›„ìˆ )
    # ...
```

### 3. ì¸ë±ìŠ¤ ìºì‹±

```python
# ì¸ë±ìŠ¤ ì €ì¥
index.storage_context.persist(persist_dir="./storage")

# ë¹ ë¥¸ ë¡œë“œ
from llama_index.core import load_index_from_storage, StorageContext

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

### 4. Retriever ê°œìˆ˜ ì¡°ì •

```python
# top_k ìµœì í™”
query_engine = index.as_query_engine(
    similarity_top_k=3  # 5 â†’ 3ìœ¼ë¡œ ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ
)

# Response Mode ìµœì í™”
query_engine = index.as_query_engine(
    response_mode="compact"  # refineë³´ë‹¤ ë¹ ë¦„
)
```

### 5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine(streaming=True)

# ìŠ¤íŠ¸ë¦¬ë°
streaming_response = query_engine.query("LlamaIndexë€?")
for text in streaming_response.response_gen:
    print(text, end="", flush=True)
```

---

## ğŸŒ í”„ë¡œë•ì…˜ ë°°í¬

### 1. FastAPI ì„œë²„

```python
# app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

app = FastAPI(title="LlamaIndex RAG API")

# ì „ì—­ ë³€ìˆ˜
index = None

class QueryRequest(BaseModel):
    question: str
    top_k: int = 3

class QueryResponse(BaseModel):
    answer: str
    sources: list[dict]

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì¸ë±ìŠ¤ ë¡œë“œ"""
    global index

    print("Loading index...")

    # ì„¤ì •
    Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0)
    Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

    # ì¸ë±ìŠ¤ ë¡œë“œ
    storage_context = StorageContext.from_defaults(persist_dir="./storage")
    index = load_index_from_storage(storage_context)

    print("Server ready!")

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """RAG ì¿¼ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        query_engine = index.as_query_engine(similarity_top_k=request.top_k)
        response = query_engine.query(request.question)

        return QueryResponse(
            answer=response.response,
            sources=[
                {
                    "text": node.text[:200],
                    "score": node.score,
                    "metadata": node.metadata
                }
                for node in response.source_nodes
            ]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "index_loaded": index is not None}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**ì‹¤í–‰:**
```bash
pip install fastapi uvicorn
python app.py
```

**í…ŒìŠ¤íŠ¸:**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "LlamaIndexë€?"}'
```

### 2. Docker ë°°í¬

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì˜ì¡´ì„±
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì•± ë° ë°ì´í„°
COPY . .

EXPOSE 8000

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
docker build -t llamaindex-rag .
docker run -p 8000:8000 \
  -e OPENAI_API_KEY=sk-... \
  -v $(pwd)/storage:/app/storage \
  llamaindex-rag
```

### 3. í‰ê°€ ë° ëª¨ë‹ˆí„°ë§

```python
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator
)
from llama_index.llms.openai import OpenAI

# LLM (í‰ê°€ìš©)
llm = OpenAI(model="gpt-4o-mini")

# í‰ê°€ì
faithfulness_evaluator = FaithfulnessEvaluator(llm=llm)
relevancy_evaluator = RelevancyEvaluator(llm=llm)
correctness_evaluator = CorrectnessEvaluator(llm=llm)

# ì¿¼ë¦¬
query = "LlamaIndexì˜ ì£¼ìš” ê¸°ëŠ¥ì€?"
response = query_engine.query(query)

# í‰ê°€
faithfulness_result = faithfulness_evaluator.evaluate_response(response=response)
print(f"Faithfulness: {faithfulness_result.passing}")

relevancy_result = relevancy_evaluator.evaluate_response(
    query=query,
    response=response
)
print(f"Relevancy: {relevancy_result.passing}")

# ì •ë‹µê³¼ ë¹„êµ
reference_answer = "LlamaIndexëŠ” ë°ì´í„° ì»¤ë„¥í„°, ì¸ë±ìŠ¤, ì¿¼ë¦¬ ì—”ì§„ì„ ì œê³µí•©ë‹ˆë‹¤."
correctness_result = correctness_evaluator.evaluate(
    query=query,
    response=response.response,
    reference=reference_answer
)
print(f"Correctness Score: {correctness_result.score}")
```

---

## ğŸ’¼ ì‹¤ë¬´ ê°€ì´ë“œ

### 1. PDF ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ (ì™„ì „ ì˜ˆì œ)

```python
# production_pdf_qa.py
import os
from pathlib import Path
from dotenv import load_dotenv

from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter

load_dotenv()

class ProductionPDFQA:
    """í”„ë¡œë•ì…˜ê¸‰ PDF Q&A ì‹œìŠ¤í…œ"""

    def __init__(self, docs_path: str = "./documents", storage_path: str = "./storage"):
        self.docs_path = docs_path
        self.storage_path = storage_path
        self.index = None

        # ì„¤ì •
        Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0)
        Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
        Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)

    def build_index(self):
        """ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print(f"ğŸ“‚ Loading documents from {self.docs_path}...")

        documents = SimpleDirectoryReader(
            self.docs_path,
            required_exts=[".pdf"]
        ).load_data()

        print(f"âœ… Loaded {len(documents)} documents")
        print("ğŸ”„ Building index...")

        self.index = VectorStoreIndex.from_documents(documents)

        # ì €ì¥
        self.index.storage_context.persist(persist_dir=self.storage_path)
        print(f"âœ… Index saved to {self.storage_path}")

    def load_index(self):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        print(f"ğŸ“¥ Loading index from {self.storage_path}...")

        storage_context = StorageContext.from_defaults(persist_dir=self.storage_path)
        self.index = load_index_from_storage(storage_context)

        print("âœ… Index loaded")

    def initialize(self, rebuild=False):
        """ì´ˆê¸°í™”"""
        if rebuild or not Path(self.storage_path).exists():
            self.build_index()
        else:
            self.load_index()

    def query(self, question: str, top_k: int = 3):
        """ì§ˆë¬¸ ì²˜ë¦¬"""
        query_engine = self.index.as_query_engine(
            similarity_top_k=top_k,
            response_mode="compact"
        )

        response = query_engine.query(question)

        return {
            "answer": response.response,
            "sources": [
                {
                    "text": node.text[:200],
                    "score": node.score,
                    "page": node.metadata.get("page_label", "N/A"),
                    "file": node.metadata.get("file_name", "Unknown")
                }
                for node in response.source_nodes
            ]
        }

    def chat(self):
        """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
        print("=" * 70)
        print("  LlamaIndex PDF Q&A System")
        print("=" * 70)
        print("ì¢…ë£Œ: 'quit', 'exit', 'ì¢…ë£Œ'\n")

        chat_engine = self.index.as_chat_engine(
            chat_mode="condense_question",
            verbose=False
        )

        while True:
            question = input("â“ ì§ˆë¬¸: ").strip()

            if question.lower() in ["quit", "exit", "ì¢…ë£Œ", ""]:
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            try:
                response = chat_engine.chat(question)
                print(f"\nğŸ’¡ ë‹µë³€:\n{response.response}\n")
                print("-" * 70 + "\n")
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜: {e}\n")

# ì‹¤í–‰
if __name__ == "__main__":
    qa_system = ProductionPDFQA(
        docs_path="./documents",
        storage_path="./storage"
    )

    qa_system.initialize(rebuild=False)
    qa_system.chat()
```

### 2. ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ + RAG

```python
# web_rag.py
from llama_index.core import VectorStoreIndex, Settings
from llama_index.readers.web import SimpleWebPageReader
from llama_index.llms.openai import OpenAI

Settings.llm = OpenAI(model="gpt-4o-mini")

# ì›¹ í¬ë¡¤ë§
urls = [
    "https://docs.llamaindex.ai/en/stable/",
    "https://docs.llamaindex.ai/en/stable/getting_started/",
]

documents = SimpleWebPageReader(html_to_text=True).load_data(urls)

# ì¸ë±ìŠ¤
index = VectorStoreIndex.from_documents(documents)

# ì¿¼ë¦¬
query_engine = index.as_query_engine()
response = query_engine.query("LlamaIndexì˜ ì£¼ìš” ê°œë…ì€?")
print(response.response)
```

### 3. SQL ë°ì´í„°ë² ì´ìŠ¤ + RAG

```python
# sql_rag.py
from llama_index.core import SQLDatabase, VectorStoreIndex
from llama_index.core.query_engine import NLSQLTableQueryEngine
from sqlalchemy import create_engine

# DB ì—°ê²°
engine = create_engine("sqlite:///company.db")
sql_database = SQLDatabase(engine, include_tables=["products", "sales"])

# SQL Query Engine
query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    tables=["products", "sales"]
)

# ìì—°ì–´ ì§ˆë¬¸ â†’ SQL ì¿¼ë¦¬ ìë™ ìƒì„± ë° ì‹¤í–‰
response = query_engine.query("2024ë…„ ë§¤ì¶œ ìƒìœ„ 5ê°œ ì œí’ˆì€?")
print(f"ë‹µë³€: {response.response}")
print(f"SQL: {response.metadata['sql_query']}")
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. "Module not found" ì—ëŸ¬

```bash
# LlamaIndex 0.10+ ë²„ì „ì€ íŒ¨í‚¤ì§€ ë¶„ë¦¬ë¨
pip install llama-index-core
pip install llama-index-llms-openai
pip install llama-index-embeddings-openai

# ë˜ëŠ” ì „ì²´ ì„¤ì¹˜
pip install llama-index
```

### 2. OpenAI API í‚¤ ì˜¤ë¥˜

```python
# Settingsì— ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI

Settings.llm = OpenAI(api_key="sk-your-key")
```

### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding(embed_batch_size=10)  # ê¸°ë³¸ 100
Settings.embed_model = embed_model
```

### 4. í† í° ì œí•œ ì´ˆê³¼

```python
# ì²­í¬ í¬ê¸° ì¤„ì´ê¸°
from llama_index.core.node_parser import SentenceSplitter

parser = SentenceSplitter(chunk_size=512)  # ê¸°ë³¸ 1024
Settings.node_parser = parser

# top_k ì¤„ì´ê¸°
query_engine = index.as_query_engine(similarity_top_k=2)  # ê¸°ë³¸ 5
```

---

## â“ FAQ

### Q1: LlamaIndex vs LangChain, ì–´ë–¤ ê²ƒì„ ì„ íƒí•´ì•¼ í•˜ë‚˜ìš”?

**A:**
- **LlamaIndex**: RAG, ë¬¸ì„œ Q&A, ê²€ìƒ‰ ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜
- **LangChain**: ì—ì´ì „íŠ¸, ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°, ë²”ìš© LLM ì•±

RAGê°€ ì£¼ ëª©ì ì´ë©´ LlamaIndex ì¶”ì²œ.

### Q2: ì–´ë–¤ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?

**A:**
- **VectorStoreIndex**: ì¼ë°˜ì ì¸ RAG (90% ì¼€ì´ìŠ¤)
- **SummaryIndex**: ë¬¸ì„œ ì „ì²´ ìš”ì•½
- **TreeIndex**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ, ê³„ì¸µì  ìš”ì•½
- **KeywordTableIndex**: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­

### Q3: ë²¡í„° ìŠ¤í† ì–´ ì„ íƒ ê¸°ì¤€ì€?

**A:**
- **í”„ë¡œí† íƒ€ì…**: SimpleVectorStore (ë¡œì»¬ JSON)
- **ê°œë°œ/í…ŒìŠ¤íŠ¸**: Chroma (ë¡œì»¬)
- **í”„ë¡œë•ì…˜ (ì†Œê·œëª¨)**: Chroma Cloud, Qdrant
- **í”„ë¡œë•ì…˜ (ëŒ€ê·œëª¨)**: Pinecone, Weaviate

### Q4: ì²­í¬ í¬ê¸° ê¶Œì¥ê°’ì€?

**A:**
- **ì¼ë°˜ ë¬¸ì„œ**: 1024ì, ì˜¤ë²„ë© 200ì
- **ê¸°ìˆ  ë¬¸ì„œ**: 512-768ì
- **ëŒ€í™”í˜•**: 256-512ì

ë„ë©”ì¸ë³„ë¡œ ì‹¤í—˜ í•„ìš”.

### Q5: LlamaIndex í•™ìŠµ ë¦¬ì†ŒìŠ¤ëŠ”?

**A:**
- ê³µì‹ ë¬¸ì„œ: https://docs.llamaindex.ai
- LlamaHub: https://llamahub.ai
- Discord: https://discord.gg/llamaindex
- YouTube: LlamaIndex ê³µì‹ ì±„ë„

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LlamaIndex ê³µì‹ ë¬¸ì„œ](https://docs.llamaindex.ai/)
- [LlamaIndex API Reference](https://docs.llamaindex.ai/en/stable/api_reference/)
- [LlamaHub (Data Loaders)](https://llamahub.ai/)
- [LlamaIndex GitHub](https://github.com/run-llama/llama_index)

### íŠœí† ë¦¬ì–¼
- [Getting Started](https://docs.llamaindex.ai/en/stable/getting_started/)
- [Understanding LlamaIndex](https://docs.llamaindex.ai/en/stable/understanding/)
- [Community Guides](https://docs.llamaindex.ai/en/stable/community/)

### ì»¤ë®¤ë‹ˆí‹°
- [LlamaIndex Discord](https://discord.gg/llamaindex)
- [LlamaIndex Twitter](https://twitter.com/llama_index)

### ê´€ë ¨ í”„ë¡œì íŠ¸
- [LlamaCloud](https://www.llamaindex.ai/): ê´€ë¦¬í˜• ì¸ë±ì‹± ë° ê²€ìƒ‰
- [LlamaParse](https://github.com/run-llama/llama_parse): ê³ ê¸‰ ë¬¸ì„œ íŒŒì‹±

### í•œêµ­ì–´ ìë£Œ
- LlamaIndex í•œêµ­ ì‚¬ìš©ì ì»¤ë®¤ë‹ˆí‹° (Discord)

---

## ğŸ“ ë¼ì´ì„ ìŠ¤

LlamaIndex í”„ë ˆì„ì›Œí¬ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

**ğŸ‰ ì´ì œ LlamaIndexë¡œ ê°•ë ¥í•œ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!**
