# LangGraphë¥¼ í™œìš©í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ

> í•œêµ­ì–´ ê°œë°œìë¥¼ ìœ„í•œ LangGraph ê¸°ë°˜ ê³ ê¸‰ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ ì™„ë²½ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ì†Œê°œ](#-í”„ë¡œì íŠ¸-ì†Œê°œ)
- [ì™œ LangGraphì¸ê°€?](#-ì™œ-langgraphì¸ê°€)
- [ì£¼ìš” ì‚¬ì–‘](#-ì£¼ìš”-ì‚¬ì–‘-specs)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [í™˜ê²½ êµ¬ì¶•](#-í™˜ê²½-êµ¬ì¶•)
- [ë¹ ë¥¸ ì‹œì‘](#-ë¹ ë¥¸-ì‹œì‘)
- [ìƒì„¸ ê°€ì´ë“œ](#-ìƒì„¸-ê°€ì´ë“œ)
- [ê³ ê¸‰ RAG íŒ¨í„´](#-ê³ ê¸‰-rag-íŒ¨í„´)
- [ì—ì´ì „íŠ¸ RAG ì‹œìŠ¤í…œ](#-ì—ì´ì „íŠ¸-rag-ì‹œìŠ¤í…œ)
- [ì„±ëŠ¥ ìµœì í™”](#-ì„±ëŠ¥-ìµœì í™”)
- [í”„ë¡œë•ì…˜ ë°°í¬](#-í”„ë¡œë•ì…˜-ë°°í¬)
- [ì‹¤ë¬´ ê°€ì´ë“œ](#-ì‹¤ë¬´-ê°€ì´ë“œ)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
- [FAQ](#-faq)
- [ì°¸ê³  ìë£Œ](#-ì°¸ê³ -ìë£Œ)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ì†Œê°œ

ì´ í”„ë¡œì íŠ¸ëŠ” **LangGraph í”„ë ˆì„ì›Œí¬**ë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ê°€ì§„ ê³ ê¸‰ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.

### LangGraphë€?

**LangGraph**ëŠ” LangChain íŒ€ì´ ê°œë°œí•œ ìƒíƒœ ê¸°ë°˜(stateful) ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬ì¶• í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìˆœí™˜ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë³µì¡í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **ê°œë°œì‚¬**: LangChain Inc.
- **ì¶œì‹œ**: 2024ë…„ 1ì›”
- **GitHub Stars**: 10,000+ (2024ë…„ 12ì›” ê¸°ì¤€)
- **ì§€ì› ì–¸ì–´**: Python, JavaScript
- **ë¼ì´ì„ ìŠ¤**: MIT License

### ì£¼ìš” íŠ¹ì§•

- âœ… **ìƒíƒœ ê´€ë¦¬**: ê·¸ë˜í”„ ì „ì²´ì—ì„œ ìƒíƒœë¥¼ ìœ ì§€ ë° ì—…ë°ì´íŠ¸
- âœ… **ìˆœí™˜ êµ¬ì¡°**: DAG(ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„)ê°€ ì•„ë‹Œ ìˆœí™˜ ê·¸ë˜í”„ ì§€ì›
- âœ… **ì¡°ê±´ë¶€ ë¼ìš°íŒ…**: ë™ì ìœ¼ë¡œ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
- âœ… **Human-in-the-Loop**: ì‚¬ëŒì˜ ê°œì… ë° ìŠ¹ì¸ ì§€ì  ì„¤ì •
- âœ… **ì²´í¬í¬ì¸íŠ¸**: ìƒíƒœ ì €ì¥ ë° ë³µì› (ì˜¤ë¥˜ ë³µêµ¬, ì¤‘ë‹¨-ì¬ê°œ)
- âœ… **ë³‘ë ¬ ì‹¤í–‰**: ì—¬ëŸ¬ ë…¸ë“œ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥
- âœ… **ìŠ¤íŠ¸ë¦¬ë°**: ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì¶œë ¥
- âœ… **ì‹œê°í™”**: ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™” ë„êµ¬ ì œê³µ

### í•™ìŠµ ëª©í‘œ

ì´ ê°€ì´ë“œë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. LangGraph í•µì‹¬ ê°œë… (StateGraph, Node, Edge) ì´í•´
2. ìƒíƒœ ê´€ë¦¬ ë° Reducer í™œìš©
3. ì¡°ê±´ë¶€ ì—£ì§€ë¡œ ë™ì  ì›Œí¬í”Œë¡œìš° êµ¬í˜„
4. Adaptive RAG, Corrective RAG, Self-RAG ë“± ê³ ê¸‰ íŒ¨í„´ êµ¬í˜„
5. ë©€í‹° ì—ì´ì „íŠ¸ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
6. Human-in-the-Loop RAG êµ¬í˜„
7. ì²´í¬í¬ì¸íŠ¸ì™€ ë©”ëª¨ë¦¬ë¡œ ë³µì› ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ êµ¬ì¶•
8. LangGraph Studioë¡œ ë””ë²„ê¹… ë° ì‹œê°í™”

### ì‹¤ì œ í™œìš© ì‚¬ë¡€

LangGraph ê¸°ë°˜ RAGëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤:

- **ğŸ”„ Adaptive RAG**: ì¿¼ë¦¬ í’ˆì§ˆì— ë”°ë¼ ê²€ìƒ‰ ì „ëµ ë™ì  ë³€ê²½
- **âœ… Corrective RAG**: ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° ì¬ê²€ìƒ‰
- **ğŸ¤– Multi-Agent RAG**: ì—¬ëŸ¬ ì „ë¬¸ ì—ì´ì „íŠ¸ê°€ í˜‘ì—…í•˜ëŠ” RAG
- **ğŸ‘¤ Human-in-the-Loop**: ì¤‘ìš” ê²°ì •ì— ì‚¬ëŒì˜ ìŠ¹ì¸ í•„ìš”
- **ğŸ” Research Agent**: ë°˜ë³µì ìœ¼ë¡œ ê²€ìƒ‰í•˜ë©° ë‹µë³€ ìƒì„±
- **ğŸ“Š ë¶„ì„ ì›Œí¬í”Œë¡œìš°**: ë°ì´í„° ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ ë¦¬í¬íŠ¸ ìƒì„±
- **ğŸ› ï¸ ë„êµ¬ ì‚¬ìš© RAG**: ê²€ìƒ‰ ì™¸ ì™¸ë¶€ API, DB ì¿¼ë¦¬ ë“± ë„êµ¬ í™œìš©

---

## ğŸ¤” ì™œ LangGraphì¸ê°€?

### LangChain vs LangGraph

| ê¸°ëŠ¥ | LangChain | LangGraph |
|------|-----------|-----------|
| **ì£¼ìš” ëª©ì ** | ê°„ë‹¨í•œ ì²´ì¸, ê¸°ë³¸ RAG | **ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°, ë©€í‹° ì—ì´ì „íŠ¸** |
| **ìƒíƒœ ê´€ë¦¬** | ì œí•œì  (ë©”ëª¨ë¦¬) | **ì „ì—­ ìƒíƒœ ê·¸ë˜í”„** |
| **ìˆœí™˜ êµ¬ì¡°** | âŒ ì§€ì› ì•ˆ í•¨ | **âœ… ìˆœí™˜ ê·¸ë˜í”„ ì§€ì›** |
| **ì¡°ê±´ë¶€ ë¶„ê¸°** | ì œí•œì  | **âœ… ë™ì  ë¼ìš°íŒ…** |
| **Human-in-the-Loop** | êµ¬í˜„ ë³µì¡ | **âœ… ê¸°ë³¸ ì§€ì›** |
| **ì²´í¬í¬ì¸íŠ¸** | âŒ ì—†ìŒ | **âœ… ìƒíƒœ ì €ì¥/ë³µì›** |
| **ì‹œê°í™”** | ì œí•œì  | **âœ… LangGraph Studio** |
| **ì‚¬ìš© ë‚œì´ë„** | ì‰¬ì›€ | **ì¤‘ê¸‰-ê³ ê¸‰** |

### ê³ ê¸‰ RAGì— ìµœì ì¸ ì´ìœ 

1. **ë³µì¡í•œ ê²€ìƒ‰ ì „ëµ**: ì¿¼ë¦¬ ë¶„ì„ â†’ ê²€ìƒ‰ â†’ í‰ê°€ â†’ ì¬ê²€ìƒ‰ ë£¨í”„
2. **ë©€í‹° ìŠ¤í… ì¶”ë¡ **: ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ ë‹µë³€ ìƒì„±
3. **ì¡°ê±´ë¶€ ë¡œì§**: ê²€ìƒ‰ í’ˆì§ˆì— ë”°ë¼ ë‹¤ë¥¸ ê²½ë¡œ ì„ íƒ
4. **ì—ì´ì „íŠ¸ í˜‘ì—…**: ì—¬ëŸ¬ ì „ë¬¸ ì—ì´ì „íŠ¸ê°€ RAG ì‘ì—… ë¶„ë‹´
5. **Human-in-the-Loop**: ì¤‘ìš” ë‹µë³€ì€ ì‚¬ëŒì´ ê²€í† 
6. **ì˜¤ë¥˜ ë³µêµ¬**: ì²´í¬í¬ì¸íŠ¸ë¡œ ì‹¤íŒ¨ ì§€ì ë¶€í„° ì¬ì‹œì‘
7. **í™•ì¥ì„±**: ìƒˆë¡œìš´ ë…¸ë“œ ì¶”ê°€ë¡œ ê¸°ëŠ¥ í™•ì¥ ìš©ì´

---

## ğŸ“‹ ì£¼ìš” ì‚¬ì–‘ (Specs)

### ì§€ì›í•˜ëŠ” LLM

LangGraphëŠ” LangChainì˜ LLM í†µí•©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

| ì œê³µì‚¬ | ëª¨ë¸ ì˜ˆì‹œ | í´ë˜ìŠ¤ |
|--------|----------|--------|
| **OpenAI** | GPT-4o, GPT-4o-mini | `ChatOpenAI` |
| **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus | `ChatAnthropic` |
| **Google** | Gemini 2.5 Flash, Gemini 2.5 Pro | `ChatGoogleGenerativeAI` |
| **AWS Bedrock** | Claude, Llama 3.1 | `BedrockChat` |
| **Ollama** | Llama 3.1, Mistral | `ChatOllama` |

### LangGraph í•µì‹¬ ì»´í¬ë„ŒíŠ¸

| ì»´í¬ë„ŒíŠ¸ | ì„¤ëª… | ìš©ë„ |
|---------|------|------|
| **StateGraph** | ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ | RAG ì›Œí¬í”Œë¡œìš° ì •ì˜ |
| **State** | ê·¸ë˜í”„ ì „ì—­ ìƒíƒœ | ì¿¼ë¦¬, ë¬¸ì„œ, ë‹µë³€ ë“± ì €ì¥ |
| **Node** | ì‹¤í–‰ í•¨ìˆ˜ | ê²€ìƒ‰, LLM í˜¸ì¶œ, í‰ê°€ ë“± |
| **Edge** | ë…¸ë“œ ê°„ ì—°ê²° | ì›Œí¬í”Œë¡œìš° íë¦„ ì •ì˜ |
| **Conditional Edge** | ì¡°ê±´ë¶€ ë¶„ê¸° | ë™ì  ë¼ìš°íŒ… |
| **Checkpointer** | ìƒíƒœ ì €ì¥ì†Œ | ë©”ëª¨ë¦¬, SQLite, Postgres ë“± |
| **ToolNode** | ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ | ê²€ìƒ‰, API í˜¸ì¶œ ë“± |

### RAG ì›Œí¬í”Œë¡œìš° íŒ¨í„´

| íŒ¨í„´ | ì„¤ëª… | ë³µì¡ë„ |
|------|------|-------|
| **Basic RAG** | ê²€ìƒ‰ â†’ LLM | ë‚®ìŒ |
| **Adaptive RAG** | ì¿¼ë¦¬ ë¶„ì„ â†’ ê²½ë¡œ ì„ íƒ | ì¤‘ê°„ |
| **Corrective RAG** | ê²€ìƒ‰ â†’ í‰ê°€ â†’ ì¬ê²€ìƒ‰ | ì¤‘ê°„ |
| **Self-RAG** | ë°˜ë³µì  ìê¸° í‰ê°€ ë° ê°œì„  | ë†’ìŒ |
| **Agentic RAG** | ì—ì´ì „íŠ¸ê°€ ë„êµ¬ ì„ íƒ | ë†’ìŒ |
| **Multi-Agent RAG** | ì—¬ëŸ¬ ì—ì´ì „íŠ¸ í˜‘ì—… | ë§¤ìš° ë†’ìŒ |

### ì˜ˆìƒ ë¹„ìš© (2025ë…„ ê¸°ì¤€)

#### ì†Œê·œëª¨ í”„ë¡œì íŠ¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸)
```
âœ… LangGraph í”„ë ˆì„ì›Œí¬
- LangGraph: ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
- LangGraph Platform: ë¬´ë£Œ í‹°ì–´ (1,000 ì‹¤í–‰/ì›”)

âœ… LLM API
- OpenAI GPT-4o-mini: ~$5-10/ì›” (ì›” 1,000 ì¿¼ë¦¬)
- Ollama (ë¡œì»¬): $0/ì›”

âœ… ì²´í¬í¬ì¸í„°
- MemorySaver: $0/ì›”
- SQLite: $0/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $0-10/ì›”
```

#### ì¤‘ê·œëª¨ í”„ë¡œì íŠ¸ (í”„ë¡œë•ì…˜)
```
âœ… LangGraph Platform
- Plus Plan: $49/ì›” (50,000 ì‹¤í–‰)

âœ… LLM API
- OpenAI GPT-4o: ~$100-200/ì›” (ì›” 10,000 ì¿¼ë¦¬)
- Anthropic Claude 3.5 Sonnet: ~$150/ì›”

âœ… ì²´í¬í¬ì¸í„°
- PostgreSQL (Supabase Pro): $25/ì›”
- Redis: $30/ì›”

âœ… ë²¡í„° ìŠ¤í† ì–´
- Pinecone Standard: $70/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $300-500/ì›”
```

#### ëŒ€ê·œëª¨ ì—”í„°í”„ë¼ì´ì¦ˆ
```
âœ… LangGraph Platform
- Enterprise: Custom pricing

âœ… LLM API
- OpenAI GPT-4o: $1,000-5,000/ì›”
- Azure OpenAI: Custom

âœ… ì¸í”„ë¼
- PostgreSQL (RDS): $200-500/ì›”
- Redis (ElastiCache): $150-300/ì›”
- ë²¡í„° DB: $500-2,000/ì›”

ğŸ“Š ì´ ì˜ˆìƒ ë¹„ìš©: $2,000-10,000+/ì›”
```

**ë¹„ìš© ìµœì í™” íŒ:**
- **MemorySaver**: ê°œë°œ í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ì‚¬ìš©
- **Ollama**: ë¡œì»¬ LLMìœ¼ë¡œ API ë¹„ìš© ì œë¡œ
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ë¬¶ì–´ì„œ ì²˜ë¦¬
- **ìºì‹±**: ë™ì¼ ì¿¼ë¦¬ ê²°ê³¼ ì¬ì‚¬ìš©
- **ì¡°ê±´ë¶€ ì‹¤í–‰**: í•„ìš”í•œ ê²½ìš°ì—ë§Œ LLM í˜¸ì¶œ

---

## ğŸ— ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### LangGraph RAG ì›Œí¬í”Œë¡œìš°

```mermaid
graph TD
    Start([ì‹œì‘]) --> Retrieve[ê²€ìƒ‰ ë…¸ë“œ]
    Retrieve --> Grade{ë¬¸ì„œ í’ˆì§ˆ<br/>í‰ê°€}

    Grade -->|ê´€ë ¨ì„± ë†’ìŒ| Generate[ë‹µë³€ ìƒì„±]
    Grade -->|ê´€ë ¨ì„± ë‚®ìŒ| WebSearch[ì›¹ ê²€ìƒ‰]

    WebSearch --> Generate

    Generate --> CheckHallucination{í™˜ê° ê²€ì‚¬}

    CheckHallucination -->|ì •ìƒ| CheckAnswer{ë‹µë³€ í’ˆì§ˆ}
    CheckHallucination -->|í™˜ê° ë°œê²¬| Generate

    CheckAnswer -->|ìœ ìš©í•¨| End([ì¢…ë£Œ])
    CheckAnswer -->|ë¶ˆì¶©ë¶„| Retrieve

    style Grade fill:#FFB347,stroke:#FF8C00,stroke-width:2px
    style CheckHallucination fill:#FFB347,stroke:#FF8C00,stroke-width:2px
    style CheckAnswer fill:#FFB347,stroke:#FF8C00,stroke-width:2px
```

### Adaptive RAG ì•„í‚¤í…ì²˜

```mermaid
graph TD
    Start([ì‚¬ìš©ì ì§ˆë¬¸]) --> Analyze[ì¿¼ë¦¬ ë¶„ì„ ë…¸ë“œ]

    Analyze --> Route{ë¼ìš°íŒ…}

    Route -->|ë²¡í„° ê²€ìƒ‰| VectorSearch[ë²¡í„° ê²€ìƒ‰ ë…¸ë“œ]
    Route -->|ì›¹ ê²€ìƒ‰| WebSearch[ì›¹ ê²€ìƒ‰ ë…¸ë“œ]
    Route -->|ì§ì ‘ ë‹µë³€| DirectAnswer[LLM ì§ì ‘ ë‹µë³€]

    VectorSearch --> Generate[ë‹µë³€ ìƒì„± ë…¸ë“œ]
    WebSearch --> Generate
    DirectAnswer --> End([ì¢…ë£Œ])

    Generate --> End

    style Route fill:#4A90E2,stroke:#357ABD,stroke-width:3px
```

### LangGraph í•µì‹¬ ê°œë…

```
[LangGraph RAG ì‹œìŠ¤í…œ]
         |
         v
    StateGraph (ìƒíƒœ ê·¸ë˜í”„)
         |
    +----+----+----+----+
    |    |    |    |    |
    v    v    v    v    v
  Node Node Node Node Node
(ê²€ìƒ‰)(í‰ê°€)(ìƒì„±)(ë„êµ¬)(ê²€ì‚¬)
    |
    +--- State (ì „ì—­ ìƒíƒœ) ---+
    |                         |
messages: list[Message]   documents: list[Document]
query: str                generation: str
steps: int                quality_score: float
```

### ìƒíƒœ(State) êµ¬ì¡°

```python
# RAG State ì˜ˆì‹œ
class RAGState(TypedDict):
    """RAG ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    # ì…ë ¥
    question: str                    # ì‚¬ìš©ì ì§ˆë¬¸

    # ê²€ìƒ‰
    documents: list[Document]        # ê²€ìƒ‰ëœ ë¬¸ì„œ
    web_results: Optional[str]       # ì›¹ ê²€ìƒ‰ ê²°ê³¼

    # í‰ê°€
    relevance_score: float           # ê´€ë ¨ì„± ì ìˆ˜
    hallucination_detected: bool     # í™˜ê° ì—¬ë¶€

    # ìƒì„±
    generation: str                  # ìƒì„±ëœ ë‹µë³€

    # ë©”íƒ€
    steps: int                       # ì‹¤í–‰ ë‹¨ê³„ ìˆ˜
    should_continue: bool            # ê³„ì† ì—¬ë¶€
```

### ë…¸ë“œ(Node) ì˜ˆì‹œ

```python
def retrieve(state: RAGState) -> RAGState:
    """ê²€ìƒ‰ ë…¸ë“œ"""
    question = state["question"]

    # ë²¡í„° ê²€ìƒ‰
    docs = vectorstore.similarity_search(question, k=3)

    return {
        "documents": docs,
        "steps": state.get("steps", 0) + 1
    }

def grade_documents(state: RAGState) -> RAGState:
    """ë¬¸ì„œ í‰ê°€ ë…¸ë“œ"""
    question = state["question"]
    documents = state["documents"]

    # LLMìœ¼ë¡œ ê´€ë ¨ì„± í‰ê°€
    score = llm.evaluate_relevance(question, documents)

    return {
        "relevance_score": score,
        "steps": state["steps"] + 1
    }
```

### í”„ë¡œë•ì…˜ ì•„í‚¤í…ì²˜

```
[í´ë¼ì´ì–¸íŠ¸ ê³„ì¸µ]
ì›¹/ëª¨ë°”ì¼ ì•±
     |
     v
[API ê³„ì¸µ]
FastAPI Server
â”œâ”€ ì¸ì¦/ì¸ê°€
â”œâ”€ Rate Limiting
â””â”€ LangGraph ì‹¤í–‰
     |
     v
[LangGraph ê³„ì¸µ]
StateGraph
â”œâ”€ ê²€ìƒ‰ ë…¸ë“œ (Retrieval)
â”œâ”€ í‰ê°€ ë…¸ë“œ (Grading)
â”œâ”€ ìƒì„± ë…¸ë“œ (Generation)
â”œâ”€ ë„êµ¬ ë…¸ë“œ (Tools)
â””â”€ ì¡°ê±´ë¶€ ì—£ì§€ (Routing)
     |
     +-------------+-------------+
     |             |             |
     v             v             v
[Vector DB]   [LLM API]   [Checkpointer]
Pinecone      OpenAI      PostgreSQL
Chroma        Claude      SQLite
     |
     v
[ëª¨ë‹ˆí„°ë§]
â”œâ”€ LangSmith (ì¶”ì )
â”œâ”€ Prometheus (ë©”íŠ¸ë¦­)
â””â”€ Sentry (ì—ëŸ¬)
```

---

## ğŸš€ í™˜ê²½ êµ¬ì¶•

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **Python**: 3.9 ì´ìƒ (3.11+ ê¶Œì¥)
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 4GB RAM (8GB+ ê¶Œì¥)
- **ìš´ì˜ì²´ì œ**: Windows 10+, macOS 11+, Ubuntu 20.04+

### 1. LangGraph ì„¤ì¹˜

```bash
# LangGraph ì„¤ì¹˜
pip install langgraph

# LangChain í†µí•© (í•„ìˆ˜)
pip install langchain langchain-community langchain-openai

# ë²¡í„° ìŠ¤í† ì–´ (ì„ íƒ)
pip install faiss-cpu chromadb

# ì²´í¬í¬ì¸í„° (ì„ íƒ)
pip install aiosqlite  # SQLite ì²´í¬í¬ì¸í„°
```

#### ì „ì²´ requirements.txt

```txt
# LangGraph í•µì‹¬
langgraph==0.2.0
langchain==0.1.0
langchain-community==0.0.13
langchain-core==0.1.10

# LLM í†µí•©
langchain-openai==0.0.2
langchain-anthropic==0.0.1

# ë²¡í„° ìŠ¤í† ì–´
faiss-cpu==1.7.4
chromadb==0.4.22

# ì²´í¬í¬ì¸í„°
aiosqlite==0.19.0

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.0
tiktoken==0.5.2
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ ìƒì„±:

```env
# LLM API í‚¤
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key

# LangSmith (ì„ íƒ, ë””ë²„ê¹…ìš©)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-key
LANGCHAIN_PROJECT=langgraph-rag

# Tavily (ì›¹ ê²€ìƒ‰, ì„ íƒ)
TAVILY_API_KEY=tvly-your-key
```

### 3. ì„¤ì¹˜ í™•ì¸

```python
# test_setup.py
import os
from dotenv import load_dotenv

load_dotenv()

def test_imports():
    """íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸"""
    print("1ï¸âƒ£ íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸...\n")

    try:
        import langgraph
        print(f"âœ… LangGraph ë²„ì „: {langgraph.__version__}")

        from langgraph.graph import StateGraph, END
        print("âœ… LangGraph Graph")

        from langgraph.checkpoint.memory import MemorySaver
        print("âœ… LangGraph Checkpointer")

        from langchain_openai import ChatOpenAI
        print("âœ… LangChain OpenAI")

        return True
    except ImportError as e:
        print(f"âŒ Import ì‹¤íŒ¨: {e}")
        return False

def test_simple_graph():
    """ê°„ë‹¨í•œ ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸"""
    print("\n2ï¸âƒ£ ê·¸ë˜í”„ ìƒì„± í…ŒìŠ¤íŠ¸...\n")

    try:
        from langgraph.graph import StateGraph, END
        from typing import TypedDict

        # ìƒíƒœ ì •ì˜
        class State(TypedDict):
            message: str

        # ë…¸ë“œ í•¨ìˆ˜
        def node_1(state: State) -> State:
            return {"message": state["message"] + " -> Node1"}

        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(State)
        workflow.add_node("node_1", node_1)
        workflow.set_entry_point("node_1")
        workflow.add_edge("node_1", END)

        graph = workflow.compile()
        print("âœ… StateGraph ìƒì„± ì„±ê³µ")

        # ì‹¤í–‰
        result = graph.invoke({"message": "Start"})
        print(f"âœ… ì‹¤í–‰ ê²°ê³¼: {result['message']}")

        return True
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_llm():
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\n3ï¸âƒ£ LLM ì—°ê²° í…ŒìŠ¤íŠ¸...\n")

    try:
        from langchain_openai import ChatOpenAI

        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        response = llm.invoke("ì•ˆë…•í•˜ì„¸ìš”!")

        print(f"âœ… LLM ì‘ë‹µ: {response.content[:50]}...")
        return True
    except Exception as e:
        print(f"âŒ LLM ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def main():
    print("=" * 60)
    print("  LangGraph RAG í™˜ê²½ ì„¤ì • í™•ì¸")
    print("=" * 60 + "\n")

    imports_ok = test_imports()
    graph_ok = test_simple_graph()
    llm_ok = test_llm()

    print("\n" + "=" * 60)
    if imports_ok and graph_ok and llm_ok:
        print("ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("   ì´ì œ LangGraph RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸  ì¼ë¶€ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ìœ„ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.")
    print("=" * 60)

if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
python test_setup.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
============================================================
  LangGraph RAG í™˜ê²½ ì„¤ì • í™•ì¸
============================================================

1ï¸âƒ£ íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸...

âœ… LangGraph ë²„ì „: 0.2.0
âœ… LangGraph Graph
âœ… LangGraph Checkpointer
âœ… LangChain OpenAI

2ï¸âƒ£ ê·¸ë˜í”„ ìƒì„± í…ŒìŠ¤íŠ¸...

âœ… StateGraph ìƒì„± ì„±ê³µ
âœ… ì‹¤í–‰ ê²°ê³¼: Start -> Node1

3ï¸âƒ£ LLM ì—°ê²° í…ŒìŠ¤íŠ¸...

âœ… LLM ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?...

============================================================
ğŸ‰ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
   ì´ì œ LangGraph RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.
============================================================
```

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ RAG ê·¸ë˜í”„ (10ë¶„ ë§Œì— êµ¬í˜„)

```python
# quick_start.py
import os
from typing import TypedDict, Annotated
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

load_dotenv()

# 1. ìƒíƒœ ì •ì˜
class RAGState(TypedDict):
    """RAG ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    question: str                          # ì‚¬ìš©ì ì§ˆë¬¸
    documents: list[Document]              # ê²€ìƒ‰ëœ ë¬¸ì„œ
    generation: str                        # ìƒì„±ëœ ë‹µë³€

# 2. ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„
print("ğŸ“‚ ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì¤‘...")
docs = [
    Document(page_content="LangGraphëŠ” LangChain íŒ€ì´ ë§Œë“  ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
    Document(page_content="StateGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
    Document(page_content="ì¡°ê±´ë¶€ ì—£ì§€ë¡œ ë™ì  ë¼ìš°íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."),
]

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = FAISS.from_documents(docs, embeddings)
print("âœ… ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ\n")

# 3. LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 4. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def retrieve(state: RAGState) -> RAGState:
    """ê²€ìƒ‰ ë…¸ë“œ"""
    print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {state['question']}")

    # ë²¡í„° ê²€ìƒ‰
    documents = vectorstore.similarity_search(state["question"], k=2)

    return {"documents": documents}

def generate(state: RAGState) -> RAGState:
    """ë‹µë³€ ìƒì„± ë…¸ë“œ"""
    print("ğŸ’¡ ë‹µë³€ ìƒì„± ì¤‘...")

    question = state["question"]
    documents = state["documents"]

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\n\n".join([doc.page_content for doc in documents])

    # í”„ë¡¬í”„íŠ¸
    prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

    # LLM í˜¸ì¶œ
    response = llm.invoke(prompt)

    return {"generation": response.content}

# 5. ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(RAGState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)

# ì—£ì§€ ì •ì˜
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

# ì»´íŒŒì¼
graph = workflow.compile()

# 6. ì‹¤í–‰
print("=" * 70)
print("  LangGraph RAG ë¹ ë¥¸ ì‹œì‘")
print("=" * 70 + "\n")

question = "LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?"
print(f"â“ ì§ˆë¬¸: {question}\n")

result = graph.invoke({"question": question})

print(f"\nğŸ’¬ ë‹µë³€:\n{result['generation']}\n")
print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(result['documents'])}ê°œ")
```

**ì‹¤í–‰:**
```bash
python quick_start.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
============================================================
  LangGraph RAG ë¹ ë¥¸ ì‹œì‘
============================================================

â“ ì§ˆë¬¸: LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸ” ê²€ìƒ‰ ì¤‘: LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?
ğŸ’¡ ë‹µë³€ ìƒì„± ì¤‘...

ğŸ’¬ ë‹µë³€:
LangGraphëŠ” LangChain íŒ€ì´ ë§Œë“  ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
StateGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìœ¼ë©°, ì¡°ê±´ë¶€
ì—£ì§€ë¡œ ë™ì  ë¼ìš°íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ğŸ“š ì°¸ê³  ë¬¸ì„œ: 2ê°œ
```

### ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì˜ˆì œ

```python
# conditional_routing.py
from typing import Literal
from langgraph.graph import StateGraph, END

# ìƒíƒœ
class RouterState(TypedDict):
    question: str
    route: str
    answer: str

# ë¼ìš°íŒ… í•¨ìˆ˜
def route_question(state: RouterState) -> Literal["vectorstore", "websearch"]:
    """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë¼ìš°íŒ…"""
    question = state["question"].lower()

    # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¼ìš°íŒ…
    if "langgraph" in question or "langchain" in question:
        return "vectorstore"
    else:
        return "websearch"

# ë…¸ë“œ
def vectorstore_search(state: RouterState) -> RouterState:
    print("ğŸ“š ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰")
    return {"route": "vectorstore", "answer": "ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ë¨"}

def web_search(state: RouterState) -> RouterState:
    print("ğŸŒ ì›¹ ê²€ìƒ‰")
    return {"route": "websearch", "answer": "ì›¹ì—ì„œ ê²€ìƒ‰ë¨"}

# ê·¸ë˜í”„
workflow = StateGraph(RouterState)
workflow.add_node("vectorstore_search", vectorstore_search)
workflow.add_node("web_search", web_search)

# ì¡°ê±´ë¶€ ì—£ì§€
workflow.set_conditional_entry_point(
    route_question,
    {
        "vectorstore": "vectorstore_search",
        "websearch": "web_search"
    }
)

workflow.add_edge("vectorstore_search", END)
workflow.add_edge("web_search", END)

graph = workflow.compile()

# ì‹¤í–‰
print("\ní…ŒìŠ¤íŠ¸ 1: LangGraph ì§ˆë¬¸")
result1 = graph.invoke({"question": "LangGraphë€?"})
print(f"ê²°ê³¼: {result1}\n")

print("í…ŒìŠ¤íŠ¸ 2: ì¼ë°˜ ì§ˆë¬¸")
result2 = graph.invoke({"question": "ë‚ ì”¨ëŠ” ì–´ë•Œ?"})
print(f"ê²°ê³¼: {result2}")
```

---

## ğŸ“š ìƒì„¸ ê°€ì´ë“œ

### 1. StateGraph ê¸°ë³¸

#### 1.1 ìƒíƒœ(State) ì •ì˜

```python
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

# ê¸°ë³¸ ìƒíƒœ
class BasicState(TypedDict):
    question: str
    answer: str

# ë©”ì‹œì§€ í¬í•¨ ìƒíƒœ (ì±„íŒ…ìš©)
class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

# ë³µì¡í•œ RAG ìƒíƒœ
class AdvancedRAGState(TypedDict):
    # ì…ë ¥
    question: str

    # ê²€ìƒ‰
    documents: list[Document]
    web_results: Optional[str]

    # í‰ê°€
    relevance_scores: list[float]
    is_relevant: bool

    # ìƒì„±
    generation: str
    confidence: float

    # ì œì–´
    retry_count: int
    max_retries: int
```

#### 1.2 Reducer í•¨ìˆ˜

```python
from operator import add

# ë¦¬ìŠ¤íŠ¸ ëˆ„ì 
class StateWithList(TypedDict):
    results: Annotated[list[str], add]  # ìë™ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

# ì»¤ìŠ¤í…€ Reducer
def merge_documents(existing: list, new: list) -> list:
    """ì¤‘ë³µ ì œê±°í•˜ë©° ë¬¸ì„œ ë³‘í•©"""
    seen = {doc.page_content for doc in existing}
    merged = existing.copy()

    for doc in new:
        if doc.page_content not in seen:
            merged.append(doc)
            seen.add(doc.page_content)

    return merged

class StateWithMerge(TypedDict):
    documents: Annotated[list[Document], merge_documents]
```

#### 1.3 ë…¸ë“œ(Node) ì •ì˜

```python
def simple_node(state: RAGState) -> RAGState:
    """ê°„ë‹¨í•œ ë…¸ë“œ"""
    return {"answer": "Simple answer"}

def node_with_logic(state: RAGState) -> RAGState:
    """ë¡œì§ì´ ìˆëŠ” ë…¸ë“œ"""
    question = state["question"]

    # ë³µì¡í•œ ì²˜ë¦¬
    if len(question) > 50:
        result = "ê¸´ ì§ˆë¬¸"
    else:
        result = "ì§§ì€ ì§ˆë¬¸"

    return {"answer": result}

async def async_node(state: RAGState) -> RAGState:
    """ë¹„ë™ê¸° ë…¸ë“œ"""
    import asyncio
    await asyncio.sleep(1)
    return {"answer": "Async result"}
```

#### 1.4 ì—£ì§€(Edge) ì •ì˜

```python
from langgraph.graph import StateGraph, END

workflow = StateGraph(RAGState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("node1", node1_func)
workflow.add_node("node2", node2_func)

# ì¼ë°˜ ì—£ì§€ (í•­ìƒ ì‹¤í–‰)
workflow.add_edge("node1", "node2")
workflow.add_edge("node2", END)

# ì§„ì…ì  ì„¤ì •
workflow.set_entry_point("node1")

# ì¡°ê±´ë¶€ ì—£ì§€
def should_continue(state: RAGState) -> Literal["node2", END]:
    if state.get("should_continue", True):
        return "node2"
    return END

workflow.add_conditional_edges(
    "node1",
    should_continue,
    {
        "node2": "node2",
        END: END
    }
)
```

### 2. Checkpointer: ìƒíƒœ ì €ì¥ ë° ë³µì›

#### 2.1 MemorySaver (ë©”ëª¨ë¦¬)

```python
from langgraph.checkpoint.memory import MemorySaver

# ì²´í¬í¬ì¸í„° ìƒì„±
checkpointer = MemorySaver()

# ê·¸ë˜í”„ì— ì ìš©
graph = workflow.compile(checkpointer=checkpointer)

# ì‹¤í–‰ (ìŠ¤ë ˆë“œ IDë¡œ ì„¸ì…˜ ê´€ë¦¬)
config = {"configurable": {"thread_id": "user_123"}}
result = graph.invoke({"question": "ì²« ë²ˆì§¸ ì§ˆë¬¸"}, config)

# ê°™ì€ ìŠ¤ë ˆë“œë¡œ ì´ì–´ì„œ ì‹¤í–‰ (ìƒíƒœ ìœ ì§€)
result2 = graph.invoke({"question": "ë‘ ë²ˆì§¸ ì§ˆë¬¸"}, config)
```

#### 2.2 SqliteSaver (ì˜êµ¬ ì €ì¥)

```python
from langgraph.checkpoint.sqlite import SqliteSaver

# SQLite ì²´í¬í¬ì¸í„°
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

graph = workflow.compile(checkpointer=checkpointer)

# ì‹¤í–‰
config = {"configurable": {"thread_id": "session_001"}}
result = graph.invoke({"question": "ì§ˆë¬¸"}, config)

# ë‚˜ì¤‘ì— ê°™ì€ ìŠ¤ë ˆë“œë¡œ ì¬ê°œ ê°€ëŠ¥
```

#### 2.3 ìƒíƒœ ê¸°ë¡ ì¡°íšŒ

```python
# íŠ¹ì • ìŠ¤ë ˆë“œì˜ ìƒíƒœ ê¸°ë¡
config = {"configurable": {"thread_id": "user_123"}}
state_history = graph.get_state_history(config)

for state in state_history:
    print(f"ì²´í¬í¬ì¸íŠ¸ ID: {state.config['configurable']['checkpoint_id']}")
    print(f"ìƒíƒœ: {state.values}")
    print()
```

### 3. ìŠ¤íŠ¸ë¦¬ë°

#### 3.1 ë…¸ë“œë³„ ìŠ¤íŠ¸ë¦¬ë°

```python
# ê° ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°
for event in graph.stream({"question": "LangGraphë€?"}):
    print(event)

# ì¶œë ¥:
# {'retrieve': {'documents': [...]}}
# {'generate': {'generation': '...'}}
```

#### 3.2 ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ìŠ¤íŠ¸ë¦¬ë°

```python
# ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ê¸°
for chunk in graph.stream(
    {"question": "ì§ˆë¬¸"},
    stream_mode="updates"
):
    print(chunk)
```

#### 3.3 ê°’ ìŠ¤íŠ¸ë¦¬ë°

```python
# ìµœì¢… ìƒíƒœë§Œ ìŠ¤íŠ¸ë¦¬ë°
for value in graph.stream(
    {"question": "ì§ˆë¬¸"},
    stream_mode="values"
):
    print(value)
```

### 4. Human-in-the-Loop

#### 4.1 ì¤‘ë‹¨ì  ì„¤ì •

```python
from langgraph.graph import StateGraph, END

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(RAGState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)
workflow.set_entry_point("retrieve")

# ì¤‘ë‹¨ì  ì„¤ì • (generate ì „ì— ë©ˆì¶¤)
graph = workflow.compile(
    checkpointer=checkpointer,
    interrupt_before=["generate"]  # generate ë…¸ë“œ ì „ì— ì¤‘ë‹¨
)

# ì‹¤í–‰
config = {"configurable": {"thread_id": "review_001"}}
result = graph.invoke({"question": "ì§ˆë¬¸"}, config)

print("ì¤‘ë‹¨ë¨. ë¬¸ì„œ ê²€í† :")
print(result["documents"])

# ì‚¬ìš©ì í™•ì¸ í›„ ì¬ê°œ
approval = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
if approval.lower() == 'y':
    result = graph.invoke(None, config)  # Noneìœ¼ë¡œ ì¬ê°œ
    print(f"ìµœì¢… ë‹µë³€: {result['generation']}")
```

#### 4.2 ìƒíƒœ ì—…ë°ì´íŠ¸ í›„ ì¬ê°œ

```python
# ì¤‘ë‹¨ëœ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
state = graph.get_state(config)

# ìƒíƒœ ìˆ˜ì •
state.values["documents"] = filtered_documents  # ì‚¬ìš©ìê°€ ë¬¸ì„œ ìˆ˜ì •

# ìˆ˜ì •ëœ ìƒíƒœë¡œ ì¬ê°œ
graph.update_state(config, state.values)
result = graph.invoke(None, config)
```

### 5. ë„êµ¬ ì‚¬ìš©

#### 5.1 ToolNode

```python
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

# ë„êµ¬ ì •ì˜
@tool
def search_wikipedia(query: str) -> str:
    """Wikipedia ê²€ìƒ‰"""
    # ì‹¤ì œ êµ¬í˜„
    return f"Wikipedia ê²°ê³¼: {query}"

@tool
def calculator(expression: str) -> float:
    """ê³„ì‚°ê¸°"""
    return eval(expression)

tools = [search_wikipedia, calculator]

# ToolNode ìƒì„±
tool_node = ToolNode(tools)

# ê·¸ë˜í”„ì— ì¶”ê°€
workflow.add_node("tools", tool_node)
```

#### 5.2 ì—ì´ì „íŠ¸ íŒ¨í„´

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# ë„êµ¬ ë°”ì¸ë”©
llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools)

def agent(state: AgentState):
    """ì—ì´ì „íŠ¸ ë…¸ë“œ"""
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["tools", END]:
    """ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ íŒë‹¨"""
    last_message = state["messages"][-1]

    if last_message.tool_calls:
        return "tools"
    return END

# ê·¸ë˜í”„
workflow = StateGraph(AgentState)
workflow.add_node("agent", agent)
workflow.add_node("tools", tool_node)
workflow.set_entry_point("agent")
workflow.add_conditional_edges("agent", should_continue)
workflow.add_edge("tools", "agent")  # ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì—ì´ì „íŠ¸
```

---

## ğŸš€ ê³ ê¸‰ RAG íŒ¨í„´

### 1. Adaptive RAG (ì ì‘í˜• RAG)

```python
# adaptive_rag.py
from typing import Literal
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

class AdaptiveRAGState(TypedDict):
    question: str
    documents: list[Document]
    generation: str

llm = ChatOpenAI(model="gpt-4o-mini")

# ì¿¼ë¦¬ ë¶„ì„ ë…¸ë“œ
def analyze_query(state: AdaptiveRAGState) -> AdaptiveRAGState:
    """ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„"""
    question = state["question"]

    prompt = ChatPromptTemplate.from_template(
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}

ë‹¤ìŒ ì¤‘ ì„ íƒ:
- simple: ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸
- complex: ë³µì¡í•œ ì¶”ë¡  í•„ìš”
- web: ìµœì‹  ì •ë³´ í•„ìš”

ë‹µë³€ (ë‹¨ì–´ í•˜ë‚˜):"""
    )

    response = llm.invoke(prompt.format(question=question))
    complexity = response.content.strip().lower()

    return {"complexity": complexity}

# ë¼ìš°íŒ… í•¨ìˆ˜
def route_query(state: AdaptiveRAGState) -> Literal["vectorstore", "websearch", "direct"]:
    """ë³µì¡ë„ì— ë”°ë¼ ë¼ìš°íŒ…"""
    complexity = state.get("complexity", "simple")

    if complexity == "web":
        return "websearch"
    elif complexity == "complex":
        return "vectorstore"
    else:
        return "direct"

# ë…¸ë“œë“¤
def vectorstore_retrieval(state: AdaptiveRAGState) -> AdaptiveRAGState:
    """ë²¡í„° ê²€ìƒ‰"""
    docs = vectorstore.similarity_search(state["question"], k=5)
    return {"documents": docs}

def web_search(state: AdaptiveRAGState) -> AdaptiveRAGState:
    """ì›¹ ê²€ìƒ‰"""
    # Tavily ë˜ëŠ” ë‹¤ë¥¸ ì›¹ ê²€ìƒ‰ API
    results = "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
    return {"web_results": results}

def direct_answer(state: AdaptiveRAGState) -> AdaptiveRAGState:
    """ì§ì ‘ ë‹µë³€"""
    response = llm.invoke(state["question"])
    return {"generation": response.content}

def generate_answer(state: AdaptiveRAGState) -> AdaptiveRAGState:
    """ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ìƒì„±
    # ...
    return {"generation": "ë‹µë³€"}

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(AdaptiveRAGState)

workflow.add_node("analyze", analyze_query)
workflow.add_node("vectorstore", vectorstore_retrieval)
workflow.add_node("websearch", web_search)
workflow.add_node("direct", direct_answer)
workflow.add_node("generate", generate_answer)

workflow.set_entry_point("analyze")

workflow.add_conditional_edges(
    "analyze",
    route_query,
    {
        "vectorstore": "vectorstore",
        "websearch": "websearch",
        "direct": "direct"
    }
)

workflow.add_edge("vectorstore", "generate")
workflow.add_edge("websearch", "generate")
workflow.add_edge("direct", END)
workflow.add_edge("generate", END)

graph = workflow.compile()
```

### 2. Corrective RAG (êµì • RAG)

```python
# corrective_rag.py
class CorrectiveRAGState(TypedDict):
    question: str
    documents: list[Document]
    relevance_score: float
    generation: str
    retry_count: int

# ë¬¸ì„œ í‰ê°€ ë…¸ë“œ
def grade_documents(state: CorrectiveRAGState) -> CorrectiveRAGState:
    """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
    question = state["question"]
    documents = state["documents"]

    prompt = ChatPromptTemplate.from_template(
        """ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}
ë¬¸ì„œ: {document}

ê´€ë ¨ì„± (yes/no):"""
    )

    relevant_docs = []
    for doc in documents:
        response = llm.invoke(
            prompt.format(question=question, document=doc.page_content)
        )
        if "yes" in response.content.lower():
            relevant_docs.append(doc)

    score = len(relevant_docs) / len(documents) if documents else 0

    return {
        "documents": relevant_docs,
        "relevance_score": score
    }

# ì¬ê²€ìƒ‰ ê²°ì •
def decide_research(state: CorrectiveRAGState) -> Literal["generate", "websearch"]:
    """ê´€ë ¨ì„± ì ìˆ˜ì— ë”°ë¼ ê²°ì •"""
    score = state.get("relevance_score", 0)
    retry = state.get("retry_count", 0)

    # ì ìˆ˜ê°€ ë‚®ê³  ì¬ì‹œë„ íšŸìˆ˜ê°€ ì ìœ¼ë©´ ì›¹ ê²€ìƒ‰
    if score < 0.5 and retry < 2:
        return "websearch"
    return "generate"

# ì›¹ ê²€ìƒ‰ ë…¸ë“œ
def web_search_fallback(state: CorrectiveRAGState) -> CorrectiveRAGState:
    """ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„"""
    # Tavily ë˜ëŠ” ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    web_docs = [
        Document(page_content="ì›¹ì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš©")
    ]

    return {
        "documents": state["documents"] + web_docs,
        "retry_count": state.get("retry_count", 0) + 1
    }

# ê·¸ë˜í”„
workflow = StateGraph(CorrectiveRAGState)

workflow.add_node("retrieve", retrieve)
workflow.add_node("grade", grade_documents)
workflow.add_node("websearch", web_search_fallback)
workflow.add_node("generate", generate)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade")

workflow.add_conditional_edges(
    "grade",
    decide_research,
    {
        "websearch": "websearch",
        "generate": "generate"
    }
)

workflow.add_edge("websearch", "grade")  # ì¬í‰ê°€
workflow.add_edge("generate", END)

graph = workflow.compile()
```

### 3. Self-RAG (ìê¸° í‰ê°€ RAG)

```python
# self_rag.py
class SelfRAGState(TypedDict):
    question: str
    documents: list[Document]
    generation: str
    hallucination_check: bool
    answer_useful: bool
    iteration: int

# í™˜ê° ê²€ì‚¬
def check_hallucination(state: SelfRAGState) -> SelfRAGState:
    """ìƒì„±ëœ ë‹µë³€ì´ ë¬¸ì„œì— ê·¼ê±°í–ˆëŠ”ì§€ í™•ì¸"""
    generation = state["generation"]
    documents = state["documents"]

    context = "\n".join([doc.page_content for doc in documents])

    prompt = ChatPromptTemplate.from_template(
        """ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸:
{context}

ë‹µë³€:
{generation}

ê·¼ê±° ìˆìŒ? (yes/no):"""
    )

    response = llm.invoke(
        prompt.format(context=context, generation=generation)
    )

    grounded = "yes" in response.content.lower()

    return {"hallucination_check": grounded}

# ë‹µë³€ ìœ ìš©ì„± ê²€ì‚¬
def check_answer_usefulness(state: SelfRAGState) -> SelfRAGState:
    """ë‹µë³€ì´ ì§ˆë¬¸ì— ìœ ìš©í•œì§€ í™•ì¸"""
    question = state["question"]
    generation = state["generation"]

    prompt = ChatPromptTemplate.from_template(
        """ë‹µë³€ì´ ì§ˆë¬¸ì— ìœ ìš©í•œì§€ í‰ê°€í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}
ë‹µë³€: {generation}

ìœ ìš©í•¨? (yes/no):"""
    )

    response = llm.invoke(
        prompt.format(question=question, generation=generation)
    )

    useful = "yes" in response.content.lower()

    return {"answer_useful": useful}

# ì¬ìƒì„± ê²°ì •
def decide_regenerate(state: SelfRAGState) -> Literal["retrieve", "end"]:
    """í™˜ê° ë˜ëŠ” ë¬´ìš©í•œ ë‹µë³€ ì‹œ ì¬ìƒì„±"""
    grounded = state.get("hallucination_check", True)
    useful = state.get("answer_useful", True)
    iteration = state.get("iteration", 0)

    if (not grounded or not useful) and iteration < 3:
        return "retrieve"
    return "end"

# ê·¸ë˜í”„
workflow = StateGraph(SelfRAGState)

workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)
workflow.add_node("check_hallucination", check_hallucination)
workflow.add_node("check_usefulness", check_answer_usefulness)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "check_hallucination")
workflow.add_edge("check_hallucination", "check_usefulness")

workflow.add_conditional_edges(
    "check_usefulness",
    decide_regenerate,
    {
        "retrieve": "retrieve",
        "end": END
    }
)

graph = workflow.compile()
```

---

## ğŸ¤– ì—ì´ì „íŠ¸ RAG ì‹œìŠ¤í…œ

### 1. ReAct Agent RAG

```python
# react_agent_rag.py
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

# ë„êµ¬ ì •ì˜
@tool
def search_vectorstore(query: str) -> str:
    """ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰"""
    docs = vectorstore.similarity_search(query, k=3)
    return "\n\n".join([doc.page_content for doc in docs])

@tool
def search_web(query: str) -> str:
    """ì›¹ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰"""
    # Tavily API í˜¸ì¶œ
    return "ì›¹ ê²€ìƒ‰ ê²°ê³¼"

@tool
def calculator(expression: str) -> float:
    """ìˆ˜ì‹ ê³„ì‚°"""
    return eval(expression)

tools = [search_vectorstore, search_web, calculator]

# LLM
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# ReAct Agent ìƒì„±
agent_executor = create_react_agent(llm, tools)

# ì‹¤í–‰
result = agent_executor.invoke({
    "messages": [("user", "LangGraphì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ 2024ë…„ ì¶œì‹œì¼ì„ ì•Œë ¤ì£¼ì„¸ìš”")]
})

print(result["messages"][-1].content)
```

### 2. Multi-Agent RAG

```python
# multi_agent_rag.py
from typing import Literal

class MultiAgentState(TypedDict):
    question: str
    research_result: str
    analysis_result: str
    final_answer: str

# ì—°êµ¬ ì—ì´ì „íŠ¸
research_llm = ChatOpenAI(model="gpt-4o-mini")
def research_agent(state: MultiAgentState) -> MultiAgentState:
    """ì •ë³´ ìˆ˜ì§‘ ì—ì´ì „íŠ¸"""
    question = state["question"]

    # ë²¡í„° ê²€ìƒ‰
    docs = vectorstore.similarity_search(question, k=5)
    context = "\n\n".join([doc.page_content for doc in docs])

    prompt = f"ë‹¤ìŒ ì •ë³´ë¥¼ ìš”ì•½í•˜ì„¸ìš”:\n\n{context}"
    response = research_llm.invoke(prompt)

    return {"research_result": response.content}

# ë¶„ì„ ì—ì´ì „íŠ¸
analysis_llm = ChatOpenAI(model="gpt-4o")
def analysis_agent(state: MultiAgentState) -> MultiAgentState:
    """ë¶„ì„ ì—ì´ì „íŠ¸"""
    question = state["question"]
    research = state["research_result"]

    prompt = f"""ì§ˆë¬¸: {question}

ì—°êµ¬ ê²°ê³¼:
{research}

ìœ„ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”:"""

    response = analysis_llm.invoke(prompt)

    return {"analysis_result": response.content}

# ì‘ì„± ì—ì´ì „íŠ¸
writer_llm = ChatOpenAI(model="gpt-4o")
def writer_agent(state: MultiAgentState) -> MultiAgentState:
    """ìµœì¢… ë‹µë³€ ì‘ì„± ì—ì´ì „íŠ¸"""
    question = state["question"]
    analysis = state["analysis_result"]

    prompt = f"""ì§ˆë¬¸: {question}

ë¶„ì„ ê²°ê³¼:
{analysis}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:"""

    response = writer_llm.invoke(prompt)

    return {"final_answer": response.content}

# ê·¸ë˜í”„
workflow = StateGraph(MultiAgentState)

workflow.add_node("researcher", research_agent)
workflow.add_node("analyst", analysis_agent)
workflow.add_node("writer", writer_agent)

workflow.set_entry_point("researcher")
workflow.add_edge("researcher", "analyst")
workflow.add_edge("analyst", "writer")
workflow.add_edge("writer", END)

graph = workflow.compile()

# ì‹¤í–‰
result = graph.invoke({"question": "LangGraphì˜ ì¥ì ì€?"})
print(result["final_answer"])
```

### 3. Supervisor Pattern (ê°ë…ì íŒ¨í„´)

```python
# supervisor_pattern.py
class SupervisorState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    next: str

# ì›Œì»¤ ì—ì´ì „íŠ¸
def researcher(state: SupervisorState):
    """ì—°êµ¬ ì—ì´ì „íŠ¸"""
    return {"messages": [AIMessage(content="ì—°êµ¬ ì™„ë£Œ")]}

def writer(state: SupervisorState):
    """ì‘ì„± ì—ì´ì „íŠ¸"""
    return {"messages": [AIMessage(content="ì‘ì„± ì™„ë£Œ")]}

# ê°ë…ì
supervisor_llm = ChatOpenAI(model="gpt-4o")
def supervisor(state: SupervisorState) -> Literal["researcher", "writer", "FINISH"]:
    """ë‹¤ìŒ ì›Œì»¤ ê²°ì •"""
    messages = state["messages"]

    prompt = ChatPromptTemplate.from_template(
        """í˜„ì¬ ëŒ€í™”ë¥¼ ë³´ê³  ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:

ì˜µì…˜: researcher, writer, FINISH

ëŒ€í™”:
{messages}

ì„ íƒ:"""
    )

    response = supervisor_llm.invoke(
        prompt.format(messages="\n".join([m.content for m in messages]))
    )

    return response.content.strip()

# ê·¸ë˜í”„
workflow = StateGraph(SupervisorState)

workflow.add_node("supervisor", supervisor)
workflow.add_node("researcher", researcher)
workflow.add_node("writer", writer)

workflow.set_entry_point("supervisor")

workflow.add_conditional_edges(
    "supervisor",
    lambda x: x["next"],
    {
        "researcher": "researcher",
        "writer": "writer",
        "FINISH": END
    }
)

workflow.add_edge("researcher", "supervisor")
workflow.add_edge("writer", "supervisor")

graph = workflow.compile()
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. ë³‘ë ¬ ë…¸ë“œ ì‹¤í–‰

```python
# ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ë…¸ë“œ ì •ì˜
def search_docs(state):
    """ë¬¸ì„œ ê²€ìƒ‰"""
    return {"docs": vectorstore.search(state["question"])}

def search_web(state):
    """ì›¹ ê²€ìƒ‰"""
    return {"web": web_search(state["question"])}

# ê·¸ë˜í”„ì— ì¶”ê°€
workflow.add_node("search_docs", search_docs)
workflow.add_node("search_web", search_web)

# ë‘ ë…¸ë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
workflow.add_edge("start", "search_docs")
workflow.add_edge("start", "search_web")

# ë³‘í•© ë…¸ë“œ
def merge_results(state):
    """ê²°ê³¼ ë³‘í•©"""
    return {"all_results": state["docs"] + state["web"]}

workflow.add_node("merge", merge_results)
workflow.add_edge("search_docs", "merge")
workflow.add_edge("search_web", "merge")
```

### 2. ì¡°ê¸° ì¢…ë£Œ

```python
def early_exit_check(state: RAGState) -> Literal["continue", "end"]:
    """ì¶©ë¶„íˆ ì¢‹ì€ ë‹µë³€ì´ë©´ ì¡°ê¸° ì¢…ë£Œ"""
    confidence = state.get("confidence", 0)

    if confidence > 0.9:
        return "end"
    return "continue"

workflow.add_conditional_edges(
    "generate",
    early_exit_check,
    {
        "end": END,
        "continue": "refine"
    }
)
```

### 3. ìºì‹±

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_retrieval(question: str):
    """ê²€ìƒ‰ ê²°ê³¼ ìºì‹±"""
    return vectorstore.similarity_search(question, k=3)

def retrieve_with_cache(state: RAGState) -> RAGState:
    """ìºì‹œ í™œìš© ê²€ìƒ‰"""
    docs = cached_retrieval(state["question"])
    return {"documents": docs}
```

---

## ğŸŒ í”„ë¡œë•ì…˜ ë°°í¬

### 1. FastAPI ì„œë²„

```python
# app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite import SqliteSaver

app = FastAPI(title="LangGraph RAG API")

# ì²´í¬í¬ì¸í„°
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

# ê·¸ë˜í”„ (ì „ì—­)
graph = None

class QueryRequest(BaseModel):
    question: str
    thread_id: str = "default"

class QueryResponse(BaseModel):
    answer: str
    steps: int

@app.on_event("startup")
async def startup():
    """ì„œë²„ ì‹œì‘ ì‹œ ê·¸ë˜í”„ ë¡œë“œ"""
    global graph

    print("Loading graph...")
    # ê·¸ë˜í”„ êµ¬ì„± (ìœ„ì˜ ì˜ˆì œ ì°¸ê³ )
    graph = build_rag_graph()
    print("Graph ready!")

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """RAG ì¿¼ë¦¬"""
    try:
        config = {"configurable": {"thread_id": request.thread_id}}

        result = graph.invoke(
            {"question": request.question},
            config=config
        )

        return QueryResponse(
            answer=result["generation"],
            steps=result.get("steps", 0)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. Docker ë°°í¬

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 3. LangSmith ëª¨ë‹ˆí„°ë§

```python
# .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-key
LANGCHAIN_PROJECT=langgraph-production

# ìë™ìœ¼ë¡œ ëª¨ë“  ì‹¤í–‰ì´ LangSmithì— ê¸°ë¡ë¨
# https://smith.langchain.com ì—ì„œ í™•ì¸:
# - ê·¸ë˜í”„ ì‹¤í–‰ ì¶”ì 
# - ê° ë…¸ë“œë³„ ì†Œìš” ì‹œê°„
# - LLM í˜¸ì¶œ ë° í† í° ì‚¬ìš©ëŸ‰
# - ì—ëŸ¬ ë¡œê·¸
```

---

## ğŸ’¼ ì‹¤ë¬´ ê°€ì´ë“œ

### ì™„ì „í•œ í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ

```python
# production_rag.py
import os
from typing import TypedDict, Annotated, Literal
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite import SqliteSaver

load_dotenv()

# ============== ìƒíƒœ ì •ì˜ ==============
class ProductionRAGState(TypedDict):
    # ì…ë ¥
    question: str

    # ê²€ìƒ‰
    documents: list[Document]
    relevance_score: float

    # ìƒì„±
    generation: str
    confidence: float

    # ì œì–´
    retry_count: int

# ============== LLM ë° ë²¡í„° ìŠ¤í† ì–´ ==============
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ë²¡í„° ìŠ¤í† ì–´ (ì‹¤ì œë¡œëŠ” Pinecone ë“± ì‚¬ìš©)
docs = [
    Document(page_content="LangGraphëŠ” ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
    Document(page_content="ì¡°ê±´ë¶€ ì—£ì§€ë¡œ ë™ì  ë¼ìš°íŒ…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."),
    Document(page_content="ì²´í¬í¬ì¸í„°ë¡œ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ë³µì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
]
vectorstore = FAISS.from_documents(docs, embeddings)

# ============== ë…¸ë“œ í•¨ìˆ˜ ==============
def retrieve(state: ProductionRAGState) -> ProductionRAGState:
    """ë¬¸ì„œ ê²€ìƒ‰"""
    print(f"ğŸ” ê²€ìƒ‰: {state['question']}")

    docs = vectorstore.similarity_search(state["question"], k=3)

    return {
        "documents": docs,
        "retry_count": state.get("retry_count", 0)
    }

def grade_documents(state: ProductionRAGState) -> ProductionRAGState:
    """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
    print("ğŸ“Š ë¬¸ì„œ í‰ê°€ ì¤‘...")

    question = state["question"]
    documents = state["documents"]

    grade_prompt = ChatPromptTemplate.from_template(
        """ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}
ë¬¸ì„œ: {document}

ê´€ë ¨ì„± (yes/no):"""
    )

    relevant_count = 0
    for doc in documents:
        response = llm.invoke(
            grade_prompt.format(
                question=question,
                document=doc.page_content
            )
        )
        if "yes" in response.content.lower():
            relevant_count += 1

    score = relevant_count / len(documents) if documents else 0

    return {"relevance_score": score}

def generate(state: ProductionRAGState) -> ProductionRAGState:
    """ë‹µë³€ ìƒì„±"""
    print("ğŸ’¡ ë‹µë³€ ìƒì„± ì¤‘...")

    question = state["question"]
    documents = state["documents"]

    context = "\n\n".join([doc.page_content for doc in documents])

    gen_prompt = ChatPromptTemplate.from_template(
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
    )

    response = llm.invoke(
        gen_prompt.format(context=context, question=question)
    )

    return {"generation": response.content}

def web_search_fallback(state: ProductionRAGState) -> ProductionRAGState:
    """ì›¹ ê²€ìƒ‰ (ê´€ë ¨ì„± ë‚®ì„ ë•Œ)"""
    print("ğŸŒ ì›¹ ê²€ìƒ‰...")

    # ì‹¤ì œë¡œëŠ” Tavily ë“± ì‚¬ìš©
    web_doc = Document(
        page_content="ì›¹ì—ì„œ ê²€ìƒ‰ëœ ìµœì‹  ì •ë³´ì…ë‹ˆë‹¤."
    )

    return {
        "documents": state["documents"] + [web_doc],
        "retry_count": state["retry_count"] + 1
    }

# ============== ì¡°ê±´ë¶€ í•¨ìˆ˜ ==============
def decide_to_generate(state: ProductionRAGState) -> Literal["generate", "websearch"]:
    """ê´€ë ¨ì„± ì ìˆ˜ì— ë”°ë¼ ê²°ì •"""
    score = state.get("relevance_score", 0)
    retry = state.get("retry_count", 0)

    if score < 0.5 and retry < 2:
        return "websearch"
    return "generate"

# ============== ê·¸ë˜í”„ êµ¬ì„± ==============
def build_graph():
    """í”„ë¡œë•ì…˜ RAG ê·¸ë˜í”„ êµ¬ì„±"""
    workflow = StateGraph(ProductionRAGState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("grade", grade_documents)
    workflow.add_node("generate", generate)
    workflow.add_node("websearch", web_search_fallback)

    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "grade")

    workflow.add_conditional_edges(
        "grade",
        decide_to_generate,
        {
            "websearch": "websearch",
            "generate": "generate"
        }
    )

    workflow.add_edge("websearch", "grade")
    workflow.add_edge("generate", END)

    # ì²´í¬í¬ì¸í„°
    checkpointer = SqliteSaver.from_conn_string("production.db")

    return workflow.compile(checkpointer=checkpointer)

# ============== ì‹¤í–‰ ==============
def main():
    print("=" * 70)
    print("  LangGraph Production RAG System")
    print("=" * 70 + "\n")

    graph = build_graph()

    # ëŒ€í™”í˜•
    thread_id = "user_001"

    while True:
        question = input("\nâ“ ì§ˆë¬¸ (ì¢…ë£Œ: quit): ").strip()

        if question.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not question:
            continue

        try:
            config = {"configurable": {"thread_id": thread_id}}

            result = graph.invoke(
                {"question": question},
                config=config
            )

            print(f"\nğŸ’¬ ë‹µë³€:\n{result['generation']}\n")
            print(f"ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜: {result.get('relevance_score', 0):.2f}")
            print(f"ğŸ“š ë¬¸ì„œ ìˆ˜: {len(result.get('documents', []))}")
            print(f"ğŸ”„ ì¬ì‹œë„: {result.get('retry_count', 0)}")

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. "Module not found" ì—ëŸ¬

```bash
# LangGraph ì¬ì„¤ì¹˜
pip uninstall langgraph
pip install langgraph

# LangChainë„ í•¨ê»˜ ì„¤ì¹˜ í•„ìš”
pip install langchain langchain-core langchain-openai
```

### 2. ìˆœí™˜ ê·¸ë˜í”„ ì˜¤ë¥˜

```python
# ìˆœí™˜ ì‹œ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì„¤ì •
result = graph.invoke(
    {"question": "ì§ˆë¬¸"},
    {"recursion_limit": 10}  # ê¸°ë³¸ 25
)
```

### 3. ì²´í¬í¬ì¸í„° ì˜¤ë¥˜

```python
# SQLite ì²´í¬í¬ì¸í„°ëŠ” ë¹„ë™ê¸° í•„ìš”
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

checkpointer = AsyncSqliteSaver.from_conn_string("db.sqlite")

# ë¹„ë™ê¸° ì‹¤í–‰
import asyncio
result = asyncio.run(graph.ainvoke({"question": "ì§ˆë¬¸"}))
```

---

## â“ FAQ

### Q1: LangChain vs LangGraph, ì–¸ì œ LangGraphë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?

**A:**
- **LangChain**: ê°„ë‹¨í•œ RAG, ê¸°ë³¸ ì²´ì¸
- **LangGraph**: ì¡°ê±´ë¶€ ë¡œì§, ë©€í‹° ì—ì´ì „íŠ¸, Human-in-the-Loop

ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ê°€ í•„ìš”í•˜ë©´ LangGraph.

### Q2: ì²´í¬í¬ì¸í„°ëŠ” ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?

**A:**
- ëŒ€í™” ê¸°ë¡ ìœ ì§€
- ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¬ì‹œì‘
- Human-in-the-Loop (ìŠ¹ì¸ í›„ ì¬ê°œ)

### Q3: LangGraph Studioë€?

**A:**
LangGraphì˜ ì‹œê°í™” ë° ë””ë²„ê¹… ë„êµ¬. ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ê³ , ê° ë…¸ë“œì˜ ìƒíƒœë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangGraph ê³µì‹ ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)
- [LangGraph GitHub](https://github.com/langchain-ai/langgraph)
- [LangChain ë¬¸ì„œ](https://python.langchain.com/)

---

**ğŸ‰ ì´ì œ LangGraphë¡œ ê³ ê¸‰ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!**
